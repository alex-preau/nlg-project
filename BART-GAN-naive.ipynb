{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive GAN training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-7dd2d3f9d1d8>:12: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"rouge\")\n"
     ]
    }
   ],
   "source": [
    "from transformers import BartTokenizer, BartModel,BartForConditionalGeneration\n",
    "from torch.distributions import Categorical\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from undecorated import undecorated\n",
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from transformers import  Seq2SeqTrainingArguments, Seq2SeqTrainer,DataCollatorForSeq2Seq\n",
    "metric = load_metric(\"rouge\")\n",
    "import torch.optim as optim\n",
    "import torchvision.utils as vutils\n",
    "import types\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.distributions import Categorical\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import PoemDataset,encode_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gan_utils import get_end_mask,get_valid_mask,Discriminator,discriminator_train_standard, generate_random_input,reinforce_loss,Generator\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-17 19:41:52.171655: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-17 19:41:52.358909: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-12-17 19:41:52.961289: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2022-12-17 19:41:52.961413: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2022-12-17 19:41:52.961430: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "!python3 gan_utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-base')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data_dir =  '/home/alexander/nlg-project/data/'\n",
    "test_df = pd.read_csv(processed_data_dir + 'test_data.csv')\n",
    "test_model = encode_sentences(tokenizer,test_df)\n",
    "\n",
    "train_df = pd.read_csv(processed_data_dir + 'train_data.csv')\n",
    "train_model = encode_sentences(tokenizer,train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = PoemDataset(train_model)\n",
    "eval_ds = PoemDataset(test_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Training Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = '/home/alexander/nlg-project/GAN_models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    " #reduced to a binary problem \n",
    "bsize = 12\n",
    "generator =  Generator().cuda()\n",
    "\n",
    "#uncomment if starting from checkpoint\n",
    "#generator.load_state_dict(torch.load(PATH + 'generator.pt')['model_state_dict'])\n",
    "discriminator = Discriminator().cuda()\n",
    "#discriminator.load_state_dict(torch.load(PATH + 'discriminator.pt')['model_state_dict'])\n",
    "criterion = nn.BCEWithLogitsLoss(reduction='none')\n",
    "\n",
    "# Create batch of latent vectors that we will use to visualize\n",
    "\n",
    "fixed_noise = generate_random_input(bsize,'cuda')\n",
    "\n",
    "\n",
    "real_label = 1.\n",
    "ewma_reward = 0.0\n",
    "fake_label = 0.\n",
    "lr = .000001\n",
    "beta1 = .9\n",
    "# Setup Adam optimizers for both G and D\n",
    "optimizerD = optim.Adam(discriminator.parameters(), lr=lr*.05, betas=(beta1, 0.99)) \n",
    "optimizerG = optim.Adam(generator.parameters(), lr=lr, betas=(beta1, 0.99))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training Loop...\n",
      "Label Noise: 0.05\n",
      "Disc LogProbs True\n",
      "tensor([[ 0.0143, -0.0546,  0.0058,  0.0117, -0.0504, -0.0379,  0.0013,  0.0016,\n",
      "         -0.0111,  0.0134,  0.0088,  0.0211,  0.0421,  0.0366,  0.0471,  0.0383,\n",
      "          0.0408,  0.0322,  0.0266,  0.0190,  0.0194]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward>)\n",
      "Disc LogProbs False\n",
      "tensor([[0.0063, 0.1021, 0.0927, 0.0983, 0.0985, 0.0747, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000]], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "gen loss\n",
      "tensor([[ 9.6255e-07,  9.8022e-02,  5.6093e-02,  2.2434e-01,  2.0876e-01,\n",
      "         -2.6792e-02,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00]], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cum rewards\n",
      "tensor([[ 0.0032,  0.0663,  0.0644,  0.0696,  0.0702, -0.1134,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000]], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "gen logprobs\n",
      "tensor([[-1.9073e-04, -2.0657e-03, -8.4617e-01, -3.1396e+00, -2.8973e+00,\n",
      "         -2.4029e-01, -1.8033e+01, -1.6073e+01, -1.5758e+01, -1.5827e+01,\n",
      "         -1.5849e+01]], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "['<s>Panel Racial Racial Discrimination</s><pad><pad><pad><pad><pad>']\n",
      "[0/30][0/11403]\tLoss_D: 17.0279\tLoss_G: 0.0159\tD(x): 0.0316\tD(G(z)): 0.0042 / 0.0159\n",
      "['</s><s> Orb Sat Pwr Lie</s><pad><pad><pad>', '</s><s>Requisite watchdog</s><pad><pad><pad><pad>']\n",
      "saving geneartor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/alexander/anaconda3/lib/python3.8/multiprocessing/queues.py\", line 245, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/alexander/anaconda3/lib/python3.8/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/alexander/anaconda3/lib/python3.8/multiprocessing/connection.py\", line 411, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/alexander/anaconda3/lib/python3.8/multiprocessing/queues.py\", line 245, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/alexander/anaconda3/lib/python3.8/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/alexander/anaconda3/lib/python3.8/multiprocessing/connection.py\", line 411, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/alexander/anaconda3/lib/python3.8/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at inline_container.cc:262] . unexpected pos 473610112 vs 473610000",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    363\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m                 \u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(obj, zip_file, pickle_module, pickle_protocol)\u001b[0m\n\u001b[1;32m    482\u001b[0m             \u001b[0mbuf_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 483\u001b[0;31m             \u001b[0mzip_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_record\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-226bba781593>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    163\u001b[0m             \u001b[0;34m'loss'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0.3355\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m             }, PATH + 'generator.pt')\n\u001b[0;32m--> 165\u001b[0;31m             torch.save({\n\u001b[0m\u001b[1;32m    166\u001b[0m             \u001b[0;34m'epoch'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m             \u001b[0;34m'model_state_dict'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    363\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m                 \u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 365\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    366\u001b[0m         \u001b[0m_legacy_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_end_of_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at inline_container.cc:262] . unexpected pos 473610112 vs 473610000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/home/alexander/anaconda3/lib/python3.8/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n"
     ]
    }
   ],
   "source": [
    "# Training Loop\n",
    "device = 'cuda'\n",
    "\n",
    "# Lists to keep track of progress\n",
    "img_list = []\n",
    "G_losses = []\n",
    "D_losses = []\n",
    "iters = 0\n",
    "num_epochs = 30\n",
    "\n",
    "train_dl = DataLoader(train_ds,batch_size=bsize,num_workers=2)\n",
    "print(\"Starting Training Loop...\")\n",
    "# For each epoch\n",
    "for epoch in range(num_epochs):\n",
    "    print(\"Label Noise:\",0.05)\n",
    "    # For each batch in the dataloader\n",
    "    for i, data in enumerate(train_dl, 0):\n",
    "       # print(i)\n",
    "        \n",
    "        if len(data['input_ids']) < bsize:\n",
    "            continue\n",
    "       # print(data)\n",
    "        ############################\n",
    "        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "        ###########################\n",
    "        ## Train with all-real batch\n",
    "        \n",
    "        # Format batch\n",
    "        real_inputs = data['input_ids'].to(device)\n",
    "        real_input_attention = data['attention_mask'].to(device)\n",
    "        real_input_test = {'input_ids':real_inputs,'attention_mask':real_input_attention}\n",
    "        \n",
    "        \n",
    "        real_gpu = data['label_ids'].to(device)\n",
    "        attention_gpu = data['decoder_attention_mask'].to(device)\n",
    "        b_size = real_gpu.size(0)\n",
    "        #\n",
    "        label = torch.full((b_size,), 1.0 , dtype=torch.float, device=device) #.95 instead of real to inject noise\n",
    "        label = torch.bernoulli(label)\n",
    "       # print(label)\n",
    "        # Forward pass real batch through D\n",
    "        real_mask = get_valid_mask(real_gpu)\n",
    "\n",
    "        output_real,loss_real = discriminator_train_standard(discriminator,real_gpu[:,1:],label,criterion,mask=real_mask)\n",
    "\n",
    "        loss_real.backward()\n",
    "\n",
    "        #del label\n",
    "        #output = discriminator(real_gpu,attention_mask=attention_gpu).view(-1)\n",
    "        # Calculate loss on all-real batch\n",
    "        #errD_real = criterion(output, label)\n",
    "        # Calculate gradients for D in backward pass\n",
    "        #errD_real.backward()\n",
    "        D_x = output_real.detach().mean().item()\n",
    "        #del output_real\n",
    "        ## Train with all-fake batch\n",
    "        # Generate batch of latent vectors\n",
    "        noise = generate_random_input(bsize,device)#torch.randn(b_size, nz, 1, 1, device=device)\n",
    "        # Generate fake image batch with G\n",
    "        #print(fake)\n",
    "       # print(noise)\n",
    "        if random.random() > .5:\n",
    "\n",
    "            fake,fake_logprobs = generator(noise)\n",
    "        else:\n",
    "\n",
    "            #sometimes the discirminator learns words in real input, this way some will be based on these same words\n",
    "            fake,fake_logprobs = generator(real_input_test)\n",
    "            #real_input_test\n",
    "  \n",
    "        \n",
    "        fake_mask = get_valid_mask(fake)\n",
    "        end_mask = get_end_mask(fake)\n",
    "  \n",
    "        #generate_sequence(generator,noise)\n",
    "       # print(len(fake_logprobs))\n",
    "        #a = 1/0\n",
    "        #fake = generator.generate_with_grad(noise['input_ids'].cuda(),attention_mask=noise[\"attention_mask\"].cuda()) #,max_length=128\n",
    "       # print(fake)\n",
    "        #print((fake.logits))\n",
    "        #fake = decode(fake.logits)\n",
    "      #  print(fake)\n",
    "        # I am injecting some false positives into the fake labels so the discriminator does a worse job\n",
    "        label.fill_(0.00) \n",
    "        label = torch.bernoulli(label)\n",
    "       # print(label)\n",
    "        # Classify all fake batch with D\n",
    "        output_fake,loss_fake = discriminator_train_standard(discriminator,fake,label,criterion,mask=fake_mask)\n",
    "\n",
    "        loss_fake.backward()\n",
    "       # print('fake')\n",
    "       # print(loss_fake.detach())\n",
    "        #output = discriminator(fake).view(-1)\n",
    "        # Calculate D's loss on the all-fake batch\n",
    "        #errD_fake = criterion(output, label)\n",
    "        # Calculate the gradients for this batch, accumulated (summed) with previous gradients\n",
    "       # errD_fake.backward()\n",
    "        D_G_z1 = output_fake.detach().cpu().mean().item()\n",
    "        # Compute error of D as sum over the fake and the real batches\n",
    "        errD = (torch.mean(loss_real.detach()) + torch.mean(loss_fake.detach())).cpu()\n",
    "       # del output_real\n",
    "        # Update D\n",
    "        optimizerD.step()\n",
    "        \n",
    "\n",
    "        ############################\n",
    "        # (2) Update G network\n",
    "        ###########################\n",
    "        \n",
    "        #generator.zero_grad()\n",
    "\n",
    "        lossG, cumulative_rewards, ewma_reward = reinforce_loss(output_fake,fake_logprobs,.3,.08,epoch,fake_mask,end_mask,ewma_reward)\n",
    "        if i % 512 == 0:\n",
    "            #for debugging\n",
    "            print('Disc LogProbs True')\n",
    "            print(output_real[0:1])\n",
    "            print('Disc LogProbs False')\n",
    "            print(output_fake[0:1])\n",
    "            print('gen loss')\n",
    "            print(lossG[0:1])\n",
    "            print('cum rewards')\n",
    "            print(cumulative_rewards[0:1])\n",
    "            print('gen logprobs')\n",
    "            print(fake_logprobs[0:1])\n",
    "            print(tokenizer.batch_decode(fake[0:1].cpu()))\n",
    "\n",
    "        #print(lossG)\n",
    "        lossG = torch.mean(lossG)\n",
    "\n",
    "        lossG.backward()\n",
    "        D_G_z2 = lossG.detach().cpu().mean().item()\n",
    "        # Update G\n",
    "        \n",
    "        optimizerG.step()\n",
    "        generator.zero_grad()\n",
    "        discriminator.zero_grad()\n",
    "\n",
    "        # Output training stats\n",
    "        if i % 256 == 0:\n",
    "            print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n",
    "                  % (epoch, num_epochs, i, len(train_dl),\n",
    "                     errD.item(), lossG.item(), D_x, D_G_z1, D_G_z2))\n",
    "          #  print('ewma_reward:',ewma_reward)\n",
    "\n",
    "        # Save Losses for plotting later\n",
    "        G_losses.append(lossG.detach().item())\n",
    "        D_losses.append(errD.detach().item())\n",
    "        \n",
    "        del lossG\n",
    "        del errD\n",
    "\n",
    "        # Check how the generator is doing by printing G's output on fixed_noise\n",
    "        if (iters % 500 == 0) or ((epoch == num_epochs-1) and (i == len(dataloader)-1)):\n",
    "            with torch.no_grad():\n",
    "                fake = tokenizer.batch_decode(generator.model.generate(fixed_noise['input_ids'],attention_mask=fixed_noise[\"attention_mask\"],max_length=32,num_beams= 50,early_stopping = True))\n",
    "                print(fake[0:2])\n",
    "        if (iters % 1000 == 0 ):\n",
    "            print('saving geneartor')\n",
    "            torch.save({\n",
    "            'epoch': 15,\n",
    "            'model_state_dict': generator.state_dict(),\n",
    "            'optimizer_state_dict': optimizerG.state_dict(),\n",
    "            'loss': 0.3355,\n",
    "            }, PATH + 'generator.pt')\n",
    "            torch.save({\n",
    "            'epoch': 15,\n",
    "            'model_state_dict': discriminator.state_dict(),\n",
    "            'optimizer_state_dict': optimizerD.state_dict(),\n",
    "            'loss': 0.3355,\n",
    "            }, PATH + 'discriminator.pt')\n",
    "            #img_list.append(vutils.make_grid(fake, padding=2, normalize=True))\n",
    "\n",
    "        iters += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
