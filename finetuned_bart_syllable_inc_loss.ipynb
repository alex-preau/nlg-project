{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-da4a1f766315>:7: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"rouge\")\n"
     ]
    }
   ],
   "source": [
    "from transformers import BartTokenizer, BartModel,BartForConditionalGeneration\n",
    "import pandas as pd\n",
    "from datasets import load_metric\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import  Seq2SeqTrainingArguments, Seq2SeqTrainer,DataCollatorForSeq2Seq\n",
    "metric = load_metric(\"rouge\")\n",
    "from torch import nn\n",
    "from typing import List, Optional, Tuple, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.modeling_outputs import Seq2SeqLMOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import PoemDataset,encode_sentences_wsyl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Model \n",
    "# use checkpoint at 34000 iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'=20.9'                      d_loss_finetuned_half.txt\r\n",
      " BART-GAN-finetuned.ipynb    d_loss_finetuned_onehalf.txt\r\n",
      " BART-GAN.ipynb              d_loss_finetuned_three.txt\r\n",
      " \u001b[0m\u001b[01;34mBART-base\u001b[0m/                  d_loss_finetuned_twohalf.txt\r\n",
      " \u001b[01;34mBART-base-syllable-loss\u001b[0m/    \u001b[01;34mdata\u001b[0m/\r\n",
      " \u001b[01;34mBART-base-syllable-naive\u001b[0m/   data_processing.ipynb\r\n",
      " D_3epochs.txt               \u001b[01;34mdata_raw\u001b[0m/\r\n",
      " D_losses_2.txt              finetuned_bart.ipynb\r\n",
      " D_losses_3.txt              finetuned_bart_syllable_inc_loss.ipynb\r\n",
      " \u001b[01;34mGAN_models\u001b[0m/                 finetuned_bart_syllable_naive.ipynb\r\n",
      " \u001b[01;34mGAN_models_finetuned\u001b[0m/       g_loss_finetuned_four.txt\r\n",
      " G_3epochs.txt               g_loss_finetuned_half.txt\r\n",
      " G_losses_2.txt              g_loss_finetuned_onehalf.txt\r\n",
      " G_losses_3.txt              g_loss_finetuned_three.txt\r\n",
      " __init__.py                 g_loss_finetuned_twohalf.txt\r\n",
      " \u001b[01;34m__pycache__\u001b[0m/                utils.py\r\n",
      " d_loss_finetuned_four.txt\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-base')\n",
    "model = BartForConditionalGeneration.from_pretrained('facebook/bart-base')\n",
    "\n",
    "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "\n",
    "#last_hidden_states = outputs.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    0, 11706,  1886,     2]]), 'attention_mask': tensor([[1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(\"held card\", return_tensors=\"pt\")\n",
    "inputs = inputs \n",
    "\n",
    "#inputs['input_ids'][:,-1]= 0\n",
    "#inputs['input_ids'] = torch.cat([inputs['input_ids'],torch.tensor([50264]).unsqueeze(0)],axis=1)\n",
    "#inputs['attention_mask'] = torch.cat([inputs['attention_mask'],torch.tensor([1]).unsqueeze(0)],axis=1)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BartForConditionalGeneration(\n",
       "  (model): BartModel(\n",
       "    (shared): Embedding(50265, 768, padding_idx=1)\n",
       "    (encoder): BartEncoder(\n",
       "      (embed_tokens): Embedding(50265, 768, padding_idx=1)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n",
       "      (layers): ModuleList(\n",
       "        (0): BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): BartDecoder(\n",
       "      (embed_tokens): Embedding(50265, 768, padding_idx=1)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n",
       "      (layers): ModuleList(\n",
       "        (0): BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50265, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5])"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = torch.argmax(model(**inputs).logits[:, :, :],axis=-1)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 50265])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(inputs['input_ids']).logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-155-ce5e4363f521>:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  output_tokens = torch.cat([output_tokens,torch.tensor(next_word)],axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 0]])\n",
      "tensor([[  1,   0, 939]])\n",
      "tensor([[  1,   0, 939,  33]])\n",
      "tensor([[  1,   0, 939,  33,  10]])\n",
      "tensor([[   1,    0,  939,   33,   10, 1886]])\n",
      "tensor([[    1,     0,   939,    33,    10,  1886, 50118]])\n",
      "tensor([[    1,     0,   939,    33,    10,  1886, 50118, 11706]])\n",
      "tensor([[    1,     0,   939,    33,    10,  1886, 50118, 11706,    11]])\n",
      "tensor([[    1,     0,   939,    33,    10,  1886, 50118, 11706,    11,   127]])\n",
      "tensor([[    1,     0,   939,    33,    10,  1886, 50118, 11706,    11,   127,\n",
      "           865]])\n",
      "tensor([[    1,     0,   939,    33,    10,  1886, 50118, 11706,    11,   127,\n",
      "           865,     8]])\n",
      "tensor([[    1,     0,   939,    33,    10,  1886, 50118, 11706,    11,   127,\n",
      "           865,     8,   939]])\n",
      "tensor([[    1,     0,   939,    33,    10,  1886, 50118, 11706,    11,   127,\n",
      "           865,     8,   939, 33976]])\n",
      "tensor([[    1,     0,   939,    33,    10,  1886, 50118, 11706,    11,   127,\n",
      "           865,     8,   939, 33976, 50118]])\n",
      "tensor([[    1,     0,   939,    33,    10,  1886, 50118, 11706,    11,   127,\n",
      "           865,     8,   939, 33976, 50118, 12963]])\n",
      "tensor([[    1,     0,   939,    33,    10,  1886, 50118, 11706,    11,   127,\n",
      "           865,     8,   939, 33976, 50118, 12963,   216]])\n",
      "tensor([[    1,     0,   939,    33,    10,  1886, 50118, 11706,    11,   127,\n",
      "           865,     8,   939, 33976, 50118, 12963,   216,    99]])\n",
      "tensor([[    1,     0,   939,    33,    10,  1886, 50118, 11706,    11,   127,\n",
      "           865,     8,   939, 33976, 50118, 12963,   216,    99,    24]])\n",
      "tensor([[    1,     0,   939,    33,    10,  1886, 50118, 11706,    11,   127,\n",
      "           865,     8,   939, 33976, 50118, 12963,   216,    99,    24,    16]])\n",
      "tensor([[    1,     0,   939,    33,    10,  1886, 50118, 11706,    11,   127,\n",
      "           865,     8,   939, 33976, 50118, 12963,   216,    99,    24,    16,\n",
      "             2]])\n"
     ]
    }
   ],
   "source": [
    "#this works thank god\n",
    "\n",
    "output_tokens = torch.tensor([1]).type(torch.LongTensor)\n",
    "output_tokens = output_tokens.unsqueeze(0)\n",
    "#print(output_tokens.shape)\n",
    "#output_tokens = torch.long(output_tokens)\n",
    "eos = 2\n",
    "done = False\n",
    "while not done:\n",
    "    encoder_hidden_state = model.model.encoder(inputs['input_ids']).last_hidden_state\n",
    "   # print(encoder_hidden_state)\n",
    "    last_hidden = model.model.decoder(input_ids=output_tokens,\n",
    "            encoder_hidden_states=encoder_hidden_state,\n",
    "            encoder_attention_mask=inputs['attention_mask'])\n",
    "    #logits = model.decoder(inputs['input_ids'])\n",
    "   # logits = model.lm_head(last_hidden)[0][0]\n",
    "    #print(logits[1:])\n",
    "    #print(model.lm_head)\n",
    "    lm_logits = model.lm_head(last_hidden[0])\n",
    "    lm_logits = lm_logits + model.final_logits_bias\n",
    "    \n",
    "    lm_logits = lm_logits.squeeze()\n",
    "    \n",
    "    lm_logits = lm_logits[-1]\n",
    "  #  print(lm_logits)\n",
    "   # logits = logits[1:]\n",
    "    next_word = lm_logits.argmax().unsqueeze(0).unsqueeze(0)\n",
    "  #  print(next_word.shape)\n",
    "    #output_tokens.append(next_word)\n",
    "   # print(output_tokens.shape)\n",
    "    output_tokens = torch.cat([output_tokens,torch.tensor(next_word)],axis=1)\n",
    "   # print(output_tokens.shape)\n",
    "    print(output_tokens)\n",
    "    if next_word == eos: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<pad><s> i have a card\\nheld in my hand and i dont\\neven know what it is</s>']"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(output_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Tensors must have same number of dimensions: got 2 and 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-168-5fefcd648655>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'attention_mask'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'attention_mask'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Tensors must have same number of dimensions: got 2 and 3"
     ]
    }
   ],
   "source": [
    "inputs['input_ids'] = torch.cat([inputs['input_ids'],out.unsqueeze(0)],axis=1)\n",
    "inputs['attention_mask'] = torch.cat([inputs['attention_mask'],torch.tensor([1]).unsqueeze(0)],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0, 11706,  1886, 50264,     2]])"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<pad><s> i have a card\\nheld in my hand and i dont\\neven know what it is</s>']"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(model.eval().cpu().generate(inputs['input_ids'],attention_mask=inputs[\"attention_mask\"],max_length=128, use_cache=False,\n",
    "        decoder_start_token_id = tokenizer.pad_token_id,\n",
    "        num_beams= 1,\n",
    "        early_stopping = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    1,     0,   939,    33,    10,  1886, 50118, 11706,    11,   127,\n",
       "           865,     8,   939, 33976, 50118, 12963,   216,    99,    24,    16,\n",
       "             2]])"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval().cpu().generate(inputs['input_ids'],attention_mask=inputs[\"attention_mask\"],max_length=128, use_cache=False,\n",
    "        decoder_start_token_id = tokenizer.pad_token_id,\n",
    "        num_beams= 1,\n",
    "        early_stopping = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#note the argmx does get\n",
    "#this is greedy\n",
    "def simple_decoding(model,inputs):\n",
    "    logprobs = []\n",
    "    output_tokens = torch.tensor([1]).type(torch.LongTensor)\n",
    "    output_tokens = output_tokens.unsqueeze(0)\n",
    "    eos = 2\n",
    "    done = False\n",
    "    while not done:\n",
    "        encoder_hidden_state = model.model.encoder(inputs['input_ids']).last_hidden_state\n",
    "        last_hidden = model.model.decoder(input_ids=output_tokens,\n",
    "                encoder_hidden_states=encoder_hidden_state,\n",
    "                encoder_attention_mask=inputs['attention_mask'])\n",
    "        lm_logits = model.lm_head(last_hidden[0])\n",
    "        lm_logits = lm_logits + model.final_logits_bias\n",
    "        \n",
    "        lm_logits = lm_logits.squeeze()\n",
    "       # print(lm_logits.shape)\n",
    "        lm_logits = lm_logits[-1]\n",
    "        next_word = lm_logits.argmax().unsqueeze(0).unsqueeze(0)\n",
    "        \n",
    "        try:\n",
    "            logprobs.append(lm_logits[next_word].squeeze())\n",
    "        except:\n",
    "            logprobs = [lm_logits]\n",
    "        output_tokens = torch.cat([output_tokens,torch.tensor(next_word)],axis=1)\n",
    "       # print(output_tokens.shape)\n",
    "       # print(output_tokens)\n",
    "        if next_word == eos: break\n",
    "    return output_tokens.squeeze(),logprobs\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-178-0b7b6bf5747b>:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  output_tokens = torch.cat([output_tokens,torch.tensor(next_word)],axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad> <s>  i  have  a  card \n",
      " held  in  my  hand  and  i  dont \n",
      " even  know  what  it  is </s>\n"
     ]
    }
   ],
   "source": [
    "generated_str, probs = simple_decoding(model,inputs)\n",
    "print(' '.join(tokenizer.batch_decode(generated_str)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_inputs_for_generation(input_ids, past=None, attention_mask=None, use_cache=None, **kwargs):\n",
    "    # if model is used as a decoder in encoder-decoder model, the decoder attention mask is created on the fly\n",
    "    if attention_mask is None:\n",
    "        attention_mask = input_ids.new_ones(input_ids.shape)\n",
    "\n",
    "    if past:\n",
    "        input_ids = input_ids[:, -1:]\n",
    "    # first step, decoder_cached_states are empty\n",
    "    return {\n",
    "        \"input_ids\": input_ids,  # encoder_outputs is defined. input_ids not needed\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"past_key_values\": past,\n",
    "        \"use_cache\": use_cache,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_processor = LogitsProcessorList(\n",
    "      [\n",
    "            MinLengthLogitsProcessor(10, eos_token_id=model.config.eos_token_id),\n",
    "        ]\n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complex_decoder(model,inputs):\n",
    "    device = 'cpu'\n",
    "    input_ids = inputs['input_ids']\n",
    "    scores = () \n",
    "    output_attentions = False\n",
    "    return_dict_in_generate = True\n",
    "    output_hidden_states = False\n",
    "    eos_token_id = 2 #i hope\n",
    "    pad_token_id = 1\n",
    "    scores = ()\n",
    "    output_scores = True\n",
    "    unfinished_sequences = input_ids.new(input_ids.shape[0]).fill_(1)\n",
    "\n",
    "    this_peer_finished = False  # used by synced_gpus only\n",
    "    while True:\n",
    "\n",
    "        print(input_ids)\n",
    "        # prepare model inputs\n",
    "        model_inputs = prepare_inputs_for_generation(input_ids)#, **model_kwargs)\n",
    "\n",
    "        # forward pass to get next token\n",
    "        outputs = model(\n",
    "            **model_inputs,\n",
    "            return_dict=True,\n",
    "            output_attentions=True,\n",
    "            output_hidden_states=True,\n",
    "        )\n",
    "\n",
    "\n",
    "        next_token_logits = outputs.logits[:, -1, :]\n",
    "\n",
    "        # pre-process distribution\n",
    "        next_tokens_scores = logits_processor(input_ids, next_token_logits)\n",
    "\n",
    "        # Store scores, attentions and hidden_states when required\n",
    "        if return_dict_in_generate:\n",
    "            if output_scores:\n",
    "                scores += (next_tokens_scores,)\n",
    "            if output_attentions:\n",
    "                decoder_attentions += (\n",
    "                    (outputs.decoder_attentions,) if self.config.is_encoder_decoder else (outputs.attentions,)\n",
    "                )\n",
    "                if self.config.is_encoder_decoder:\n",
    "                    cross_attentions += (outputs.cross_attentions,)\n",
    "\n",
    "            if output_hidden_states:\n",
    "                decoder_hidden_states += (\n",
    "                    (outputs.decoder_hidden_states,)\n",
    "                    if self.config.is_encoder_decoder\n",
    "                    else (outputs.hidden_states,)\n",
    "                )\n",
    "\n",
    "        # argmax\n",
    "        next_tokens = torch.argmax(next_tokens_scores, dim=-1)\n",
    "\n",
    "        # finished sentences should have their next token be a padding token\n",
    "        if eos_token_id is not None:\n",
    "            if pad_token_id is None:\n",
    "                raise ValueError(\"If `eos_token_id` is defined, make sure that `pad_token_id` is defined.\")\n",
    "            next_tokens = next_tokens * unfinished_sequences + pad_token_id * (1 - unfinished_sequences)\n",
    "\n",
    "        # update generated ids, model inputs, and length for next step\n",
    "        input_ids = torch.cat([input_ids, next_tokens[:, None]], dim=-1)\n",
    "       # model_kwargs = self._update_model_kwargs_for_generation(\n",
    "       #     outputs, model_kwargs, is_encoder_decoder=self.config.is_encoder_decoder\n",
    "       # )\n",
    "\n",
    "        # if eos_token was found in one sentence, set sentence to finished\n",
    "        if eos_token_id is not None:\n",
    "            unfinished_sequences = unfinished_sequences.mul((next_tokens != eos_token_id).long())\n",
    "\n",
    "        # stop when each sentence is finished, or if we exceed the maximum length\n",
    "        if unfinished_sequences.max() == 0:\n",
    "\n",
    "            break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0, 11706,  1886, 50264,     2]])\n",
      "tensor([[    0, 11706,  1886, 50264,     2, 50118]])\n",
      "tensor([[    0, 11706,  1886, 50264,     2, 50118,     1]])\n",
      "tensor([[    0, 11706,  1886, 50264,     2, 50118,     1,     1]])\n",
      "tensor([[    0, 11706,  1886, 50264,     2, 50118,     1,     1,     1]])\n",
      "tensor([[    0, 11706,  1886, 50264,     2, 50118,     1,     1,     1,     1]])\n",
      "tensor([[    0, 11706,  1886, 50264,     2, 50118,     1,     1,     1,     1,\n",
      "             1]])\n",
      "tensor([[    0, 11706,  1886, 50264,     2, 50118,     1,     1,     1,     1,\n",
      "             1,     1]])\n",
      "tensor([[    0, 11706,  1886, 50264,     2, 50118,     1,     1,     1,     1,\n",
      "             1,     1,     1]])\n",
      "tensor([[    0, 11706,  1886, 50264,     2, 50118,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1]])\n",
      "tensor([[    0, 11706,  1886, 50264,     2, 50118,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1]])\n",
      "tensor([[    0, 11706,  1886, 50264,     2, 50118,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1]])\n",
      "tensor([[    0, 11706,  1886, 50264,     2, 50118,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1]])\n",
      "tensor([[    0, 11706,  1886, 50264,     2, 50118,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1]])\n",
      "tensor([[    0, 11706,  1886, 50264,     2, 50118,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1]])\n",
      "tensor([[    0, 11706,  1886, 50264,     2, 50118,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1]])\n",
      "tensor([[    0, 11706,  1886, 50264,     2, 50118,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1]])\n",
      "tensor([[    0, 11706,  1886, 50264,     2, 50118,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1]])\n",
      "tensor([[    0, 11706,  1886, 50264,     2, 50118,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1]])\n",
      "tensor([[    0, 11706,  1886, 50264,     2, 50118,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1]])\n",
      "tensor([[    0, 11706,  1886, 50264,     2, 50118,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1]])\n",
      "tensor([[    0, 11706,  1886, 50264,     2, 50118,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1]])\n",
      "tensor([[    0, 11706,  1886, 50264,     2, 50118,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1]])\n",
      "tensor([[    0, 11706,  1886, 50264,     2, 50118,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1]])\n",
      "tensor([[    0, 11706,  1886, 50264,     2, 50118,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1]])\n",
      "tensor([[    0, 11706,  1886, 50264,     2, 50118,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1]])\n",
      "tensor([[    0, 11706,  1886, 50264,     2, 50118,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1]])\n",
      "tensor([[    0, 11706,  1886, 50264,     2, 50118,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1]])\n",
      "tensor([[    0, 11706,  1886, 50264,     2, 50118,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1]])\n",
      "tensor([[    0, 11706,  1886, 50264,     2, 50118,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1]])\n",
      "tensor([[    0, 11706,  1886, 50264,     2, 50118,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1]])\n",
      "tensor([[    0, 11706,  1886, 50264,     2, 50118,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1]])\n",
      "tensor([[    0, 11706,  1886, 50264,     2, 50118,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1]])\n",
      "tensor([[    0, 11706,  1886, 50264,     2, 50118,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1]])\n",
      "tensor([[    0, 11706,  1886, 50264,     2, 50118,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1]])\n",
      "tensor([[    0, 11706,  1886, 50264,     2, 50118,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1]])\n",
      "tensor([[    0, 11706,  1886, 50264,     2, 50118,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1]])\n",
      "tensor([[    0, 11706,  1886, 50264,     2, 50118,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1]])\n",
      "tensor([[    0, 11706,  1886, 50264,     2, 50118,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1]])\n",
      "tensor([[    0, 11706,  1886, 50264,     2, 50118,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1]])\n",
      "tensor([[    0, 11706,  1886, 50264,     2, 50118,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1]])\n",
      "tensor([[    0, 11706,  1886, 50264,     2, 50118,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1]])\n",
      "tensor([[    0, 11706,  1886, 50264,     2, 50118,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1]])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0, 11706,  1886, 50264,     2, 50118,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1]])\n",
      "tensor([[    0, 11706,  1886, 50264,     2, 50118,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1]])\n",
      "tensor([[    0, 11706,  1886, 50264,     2, 50118,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1]])\n",
      "tensor([[    0, 11706,  1886, 50264,     2, 50118,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1]])\n",
      "tensor([[    0, 11706,  1886, 50264,     2, 50118,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1]])\n",
      "tensor([[    0, 11706,  1886, 50264,     2, 50118,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1]])\n",
      "tensor([[    0, 11706,  1886, 50264,     2, 50118,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1]])\n",
      "tensor([[    0, 11706,  1886, 50264,     2, 50118,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1]])\n",
      "tensor([[    0, 11706,  1886, 50264,     2, 50118,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1]])\n",
      "tensor([[    0, 11706,  1886, 50264,     2, 50118,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1]])\n",
      "tensor([[    0, 11706,  1886, 50264,     2, 50118,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1]])\n",
      "tensor([[    0, 11706,  1886, 50264,     2, 50118,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1]])\n",
      "tensor([[    0, 11706,  1886, 50264,     2, 50118,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1]])\n",
      "tensor([[    0, 11706,  1886, 50264,     2, 50118,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1]])\n",
      "tensor([[    0, 11706,  1886, 50264,     2, 50118,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1]])\n",
      "tensor([[    0, 11706,  1886, 50264,     2, 50118,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1]])\n",
      "tensor([[    0, 11706,  1886, 50264,     2, 50118,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1]])\n",
      "tensor([[    0, 11706,  1886, 50264,     2, 50118,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1]])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-156-69e00ddb93e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcomplex_decoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-155-21598e364a02>\u001b[0m in \u001b[0;36mcomplex_decoder\u001b[0;34m(model, inputs)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m# forward pass to get next token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         outputs = model(\n\u001b[0m\u001b[1;32m     23\u001b[0m             \u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/transformers/models/bart/modeling_bart.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1357\u001b[0m                 )\n\u001b[1;32m   1358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1359\u001b[0;31m         outputs = self.model(\n\u001b[0m\u001b[1;32m   1360\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/transformers/models/bart/modeling_bart.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1245\u001b[0m         \u001b[0;31m# decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1246\u001b[0;31m         decoder_outputs = self.decoder(\n\u001b[0m\u001b[1;32m   1247\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoder_input_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1248\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/transformers/models/bart/modeling_bart.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, head_mask, cross_attn_head_mask, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1102\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1104\u001b[0;31m                 layer_outputs = decoder_layer(\n\u001b[0m\u001b[1;32m   1105\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1106\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/transformers/models/bart/modeling_bart.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, encoder_hidden_states, encoder_attention_mask, layer_head_mask, cross_attn_layer_head_mask, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[1;32m    454\u001b[0m         \u001b[0;31m# Fully Connected\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m         \u001b[0mresidual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 456\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    457\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation_dropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1674\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1675\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1676\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1677\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1678\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "complex_decoder(model.cpu(),inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "You have to specify either input_ids or inputs_embeds",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-c2353770c752>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m       )\n\u001b[1;32m     21\u001b[0m \u001b[0mstopping_criteria\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStoppingCriteriaList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mMaxLengthCriteria\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m outputs = model.greedy_search(\n\u001b[0m\u001b[1;32m     23\u001b[0m          \u001b[0;34m**\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits_processor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogits_processor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstopping_criteria\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstopping_criteria\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         )\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/transformers/generation_utils.py\u001b[0m in \u001b[0;36mgreedy_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   1783\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1784\u001b[0m             \u001b[0;31m# forward pass to get next token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1785\u001b[0;31m             outputs = self(\n\u001b[0m\u001b[1;32m   1786\u001b[0m                 \u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1787\u001b[0m                 \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/transformers/models/bart/modeling_bart.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1357\u001b[0m                 )\n\u001b[1;32m   1358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1359\u001b[0;31m         outputs = self.model(\n\u001b[0m\u001b[1;32m   1360\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/transformers/models/bart/modeling_bart.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1227\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mencoder_outputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1228\u001b[0;31m             encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m   1229\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1230\u001b[0m                 \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/transformers/models/bart/modeling_bart.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    796\u001b[0m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs_embeds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    797\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 798\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You have to specify either input_ids or inputs_embeds\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    799\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    800\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: You have to specify either input_ids or inputs_embeds"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "   LogitsProcessorList,\n",
    "       MinLengthLogitsProcessor,\n",
    "           StoppingCriteriaList,\n",
    "            MaxLengthCriteria,\n",
    "         )\n",
    "\n",
    "tokenizer_g = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model_g = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "model_g.config.pad_token_id = model.config.eos_token_id\n",
    "input_prompt = \"It might be possible to\"\n",
    "input_ids = tokenizer(input_prompt, return_tensors=\"pt\")#.input_ids\n",
    "    \n",
    "logits_processor = LogitsProcessorList(\n",
    "      [\n",
    "            MinLengthLogitsProcessor(10, eos_token_id=model_g.config.eos_token_id),\n",
    "        ]\n",
    "      )\n",
    "stopping_criteria = StoppingCriteriaList([MaxLengthCriteria(max_length=20)])\n",
    "outputs = model.greedy_search(\n",
    "         **input_ids, logits_processor=logits_processor, stopping_criteria=stopping_criteria\n",
    "        )\n",
    "tokenizer.batch_decode(outputs, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1026, 1244,  307, 1744,  284]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retreive data\n",
    "Right now just using kaggle corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/alexander/nlg-project/data'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data_dir = '/home/alexander/nlg-project/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-f10881ed36ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Poem'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'train_df' is not defined"
     ]
    }
   ],
   "source": [
    "train_df['Poem'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Unnamed: 0                                               Poem  \\\n",
      "0             7542  the mockingbird has testified\\nto springs exis...   \n",
      "1           116211  ive actually\\nheld a black card in my hand\\nan...   \n",
      "2            31718  although alas the\\ncoat on his back is a coat\\...   \n",
      "3            97130  these memories dont\\nmean anything to you and\\...   \n",
      "4            97063  a mad old lady\\nwhooped her tits out at me in\\...   \n",
      "...            ...                                                ...   \n",
      "136820      133722  the manager hugged\\nme like u back see i am\\na...   \n",
      "136821       50010  an entire plot\\nbased around klaus tripping an...   \n",
      "136822       96222  why cant paulina\\nhave social media i\\nmiss he...   \n",
      "136823        5797  i feel used he said\\nand old as usual\\ni belie...   \n",
      "136824       51245  tip if youre upset\\ntake a walk or jog it off\\...   \n",
      "\n",
      "                   Title  \n",
      "0       testified almond  \n",
      "1              held card  \n",
      "2        lining although  \n",
      "3          memories mean  \n",
      "4            whooped pub  \n",
      "...                  ...  \n",
      "136820    hugged manager  \n",
      "136821    klaus tripping  \n",
      "136822     paulina maddy  \n",
      "136823      height usual  \n",
      "136824           jog tip  \n",
      "\n",
      "[136825 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "test_df = pd.read_csv(processed_data_dir + 'test_data.csv')\n",
    "train_df = pd.read_csv(processed_data_dir + 'train_data.csv')\n",
    "print(train_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_df = pd.read_csv(processed_data_dir + 'kaggle_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>line_0</th>\n",
       "      <th>line_1</th>\n",
       "      <th>line_2</th>\n",
       "      <th>source</th>\n",
       "      <th>valid</th>\n",
       "      <th>line_0_scount</th>\n",
       "      <th>line_1_scount</th>\n",
       "      <th>line_2_scount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>fishing boats</td>\n",
       "      <td>colors of</td>\n",
       "      <td>the rainbow</td>\n",
       "      <td>tempslibres</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>ash wednesday</td>\n",
       "      <td>trying to remember</td>\n",
       "      <td>my dream</td>\n",
       "      <td>tempslibres</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>snowy morn</td>\n",
       "      <td>pouring another cup</td>\n",
       "      <td>of black coffee</td>\n",
       "      <td>tempslibres</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>shortest day</td>\n",
       "      <td>flames dance</td>\n",
       "      <td>in the oven</td>\n",
       "      <td>tempslibres</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>haze</td>\n",
       "      <td>half the horse hidden</td>\n",
       "      <td>behind the house</td>\n",
       "      <td>tempslibres</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144022</th>\n",
       "      <td>144118</td>\n",
       "      <td>im not asking did</td>\n",
       "      <td>you say it nor clarify</td>\n",
       "      <td>what you said neither</td>\n",
       "      <td>twaiku</td>\n",
       "      <td>True</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144023</th>\n",
       "      <td>144119</td>\n",
       "      <td>you are truly a</td>\n",
       "      <td>moron or a liar im</td>\n",
       "      <td>inclined to think both</td>\n",
       "      <td>twaiku</td>\n",
       "      <td>True</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144024</th>\n",
       "      <td>144120</td>\n",
       "      <td>aint no selfie on</td>\n",
       "      <td>this earth thats gonna make me</td>\n",
       "      <td>like theresa may</td>\n",
       "      <td>twaiku</td>\n",
       "      <td>True</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144025</th>\n",
       "      <td>144121</td>\n",
       "      <td>is doing a great</td>\n",
       "      <td>job turning independents</td>\n",
       "      <td>into democrats</td>\n",
       "      <td>twaiku</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144026</th>\n",
       "      <td>144122</td>\n",
       "      <td>wanted to send a</td>\n",
       "      <td>quick follow up on if the</td>\n",
       "      <td>blood is loud talk soon</td>\n",
       "      <td>twaiku</td>\n",
       "      <td>True</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>144027 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0             line_0                          line_1  \\\n",
       "0                0      fishing boats                       colors of   \n",
       "1                1      ash wednesday              trying to remember   \n",
       "2                2         snowy morn             pouring another cup   \n",
       "3                3       shortest day                    flames dance   \n",
       "4                4               haze           half the horse hidden   \n",
       "...            ...                ...                             ...   \n",
       "144022      144118  im not asking did          you say it nor clarify   \n",
       "144023      144119    you are truly a              moron or a liar im   \n",
       "144024      144120  aint no selfie on  this earth thats gonna make me   \n",
       "144025      144121   is doing a great        job turning independents   \n",
       "144026      144122   wanted to send a       quick follow up on if the   \n",
       "\n",
       "                         line_2       source  valid  line_0_scount  \\\n",
       "0                   the rainbow  tempslibres   True              4   \n",
       "1                      my dream  tempslibres   True              5   \n",
       "2               of black coffee  tempslibres   True              3   \n",
       "3                   in the oven  tempslibres   True              4   \n",
       "4              behind the house  tempslibres   True              2   \n",
       "...                         ...          ...    ...            ...   \n",
       "144022    what you said neither       twaiku   True              7   \n",
       "144023   inclined to think both       twaiku   True              7   \n",
       "144024         like theresa may       twaiku   True              6   \n",
       "144025           into democrats       twaiku   True              5   \n",
       "144026  blood is loud talk soon       twaiku   True              6   \n",
       "\n",
       "        line_1_scount  line_2_scount  \n",
       "0                   3              3  \n",
       "1                   6              2  \n",
       "2                   7              6  \n",
       "3                   5              5  \n",
       "4                   7              6  \n",
       "...               ...            ...  \n",
       "144022             10              8  \n",
       "144023              6              8  \n",
       "144024             11              6  \n",
       "144025              8              5  \n",
       "144026              9              8  \n",
       "\n",
       "[144027 rows x 9 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whole_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def syllable_vec(row):\n",
    "    return [row['line_0_scount'],row['line_1_scount'],row['line_2_scount']]\n",
    "\n",
    "def change_title(row):\n",
    "    #adds syllable info to title\n",
    "    #print(row['Title'])\n",
    "    return str(row['syllables']) + ';' + str(row['Title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_syllables_title(part_df, match_df):\n",
    "    merged = pd.merge(part_df, match_df,how='left', left_on=['Unnamed: 0'], right_index=True)\n",
    "    merged['syllables'] = merged.apply(syllable_vec,axis=1)\n",
    "    merged.drop(columns=['Unnamed: 0_x', 'Unnamed: 0_y','line_0','line_1','line_2','source','valid','line_0_scount',\n",
    "                        'line_1_scount','line_2_scount'],inplace=True)\n",
    "    merged['Title'] = merged.apply(change_title,axis=1)\n",
    "    \n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = add_syllables_title(train_df,whole_df)\n",
    "test_df = add_syllables_title(test_df,whole_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Poem</th>\n",
       "      <th>Title</th>\n",
       "      <th>syllables</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the mockingbird has testified\\nto springs exis...</td>\n",
       "      <td>[10, 9, 11];testified almond</td>\n",
       "      <td>[10, 9, 11]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ive actually\\nheld a black card in my hand\\nan...</td>\n",
       "      <td>[5, 9, 8];held card</td>\n",
       "      <td>[5, 9, 8]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>although alas the\\ncoat on his back is a coat\\...</td>\n",
       "      <td>[6, 9, 5];lining although</td>\n",
       "      <td>[6, 9, 5]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>these memories dont\\nmean anything to you and\\...</td>\n",
       "      <td>[7, 10, 7];memories mean</td>\n",
       "      <td>[7, 10, 7]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a mad old lady\\nwhooped her tits out at me in\\...</td>\n",
       "      <td>[6, 12, 5];whooped pub</td>\n",
       "      <td>[6, 12, 5]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136820</th>\n",
       "      <td>the manager hugged\\nme like u back see i am\\na...</td>\n",
       "      <td>[7, 9, 7];hugged manager</td>\n",
       "      <td>[7, 9, 7]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136821</th>\n",
       "      <td>an entire plot\\nbased around klaus tripping an...</td>\n",
       "      <td>[5, 10, 7];klaus tripping</td>\n",
       "      <td>[5, 10, 7]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136822</th>\n",
       "      <td>why cant paulina\\nhave social media i\\nmiss he...</td>\n",
       "      <td>[6, 8, 7];paulina maddy</td>\n",
       "      <td>[6, 8, 7]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136823</th>\n",
       "      <td>i feel used he said\\nand old as usual\\ni belie...</td>\n",
       "      <td>[7, 5, 16];height usual</td>\n",
       "      <td>[7, 5, 16]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136824</th>\n",
       "      <td>tip if youre upset\\ntake a walk or jog it off\\...</td>\n",
       "      <td>[8, 9, 8];jog tip</td>\n",
       "      <td>[8, 9, 8]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>136825 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     Poem  \\\n",
       "0       the mockingbird has testified\\nto springs exis...   \n",
       "1       ive actually\\nheld a black card in my hand\\nan...   \n",
       "2       although alas the\\ncoat on his back is a coat\\...   \n",
       "3       these memories dont\\nmean anything to you and\\...   \n",
       "4       a mad old lady\\nwhooped her tits out at me in\\...   \n",
       "...                                                   ...   \n",
       "136820  the manager hugged\\nme like u back see i am\\na...   \n",
       "136821  an entire plot\\nbased around klaus tripping an...   \n",
       "136822  why cant paulina\\nhave social media i\\nmiss he...   \n",
       "136823  i feel used he said\\nand old as usual\\ni belie...   \n",
       "136824  tip if youre upset\\ntake a walk or jog it off\\...   \n",
       "\n",
       "                               Title    syllables  \n",
       "0       [10, 9, 11];testified almond  [10, 9, 11]  \n",
       "1                [5, 9, 8];held card    [5, 9, 8]  \n",
       "2          [6, 9, 5];lining although    [6, 9, 5]  \n",
       "3           [7, 10, 7];memories mean   [7, 10, 7]  \n",
       "4             [6, 12, 5];whooped pub   [6, 12, 5]  \n",
       "...                              ...          ...  \n",
       "136820      [7, 9, 7];hugged manager    [7, 9, 7]  \n",
       "136821     [5, 10, 7];klaus tripping   [5, 10, 7]  \n",
       "136822       [6, 8, 7];paulina maddy    [6, 8, 7]  \n",
       "136823       [7, 5, 16];height usual   [7, 5, 16]  \n",
       "136824             [8, 9, 8];jog tip    [8, 9, 8]  \n",
       "\n",
       "[136825 rows x 3 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-28f129b9b7e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"labels\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df[\"labels\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'encode_sentences_wsyl' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-fc7c56b0cd04>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencode_sentences_wsyl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtest_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencode_sentences_wsyl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'encode_sentences_wsyl' is not defined"
     ]
    }
   ],
   "source": [
    "train_model = encode_sentences_wsyl(tokenizer,train_df)\n",
    "test_model = encode_sentences_wsyl(tokenizer,test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  0, 646, 306,  ...,   1,   1,   1],\n",
       "         [  0, 646, 401,  ...,   1,   1,   1],\n",
       "         [  0, 646, 245,  ...,   1,   1,   1],\n",
       "         ...,\n",
       "         [  0, 646, 398,  ...,   1,   1,   1],\n",
       "         [  0, 646, 401,  ...,   1,   1,   1],\n",
       "         [  0, 646, 406,  ...,   1,   1,   1]]),\n",
       " 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]]),\n",
       " 'labels': tensor([[    0,   608,    24,  ...,     1,     1,     1],\n",
       "         [    0,  4356,  1158,  ...,     1,     1,     1],\n",
       "         [    0, 17271,  2892,  ...,     1,     1,     1],\n",
       "         ...,\n",
       "         [    0,  7409,    10,  ...,     1,     1,     1],\n",
       "         [    0,  7252,    37,  ...,     1,     1,     1],\n",
       "         [    0,   141,   171,  ...,     1,     1,     1]]),\n",
       " 'syllables': tensor([[ 4,  8,  7],\n",
       "         [ 6,  9,  5],\n",
       "         [ 5,  8,  7],\n",
       "         ...,\n",
       "         [ 8,  6,  6],\n",
       "         [ 6,  9,  6],\n",
       "         [ 7, 11,  5]])}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = PoemDataset(train_model)\n",
    "eval_ds = PoemDataset(test_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoemDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.labels = df['labels']\n",
    "        self.mask = df['attention_mask']\n",
    "        self.input = df['input_ids']\n",
    "        self.syllables = df['syllables']\n",
    "        #self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        input_ids = self.input[idx]\n",
    "        attention_masks = self.mask[idx]\n",
    "        target_ids = self.labels[idx]\n",
    "        syllables = self.syllables[idx]\n",
    "        batch = {\n",
    "          \"input_ids\": input_ids,\n",
    "          \"decoder_attention_mask\": torch.tensor([1] * 128),\n",
    "          \"attention_mask\": attention_masks,\n",
    "          \"label_ids\": target_ids\n",
    "          \"syllables\":syllables\n",
    "        }\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Collator_poem_gen(DataCollatorForSeq2Seq):\n",
    "        \n",
    "    def __call__(self, batch):\n",
    "        input_ids_batch = []\n",
    "        input_ids_len = []\n",
    "        \n",
    "        labels_batch = []\n",
    "        labels_len = []\n",
    "        \n",
    "        for example in batch:\n",
    "            input_ids = example[\"Title\"]\n",
    "            input_ids_len.append(len(input_ids))\n",
    "            input_ids_batch.append(input_ids)\n",
    "            \n",
    "            labels = example[\"Poem\"]\n",
    "            labels_len.append(len(labels))\n",
    "            labels_batch.append(labels)\n",
    "\n",
    "        input_ids_padded = self.process_encoded_text(input_ids_batch, input_ids_len, self.tokenizer.pad_token_id)\n",
    "        labels_padded = self.process_encoded_text(labels_batch, labels_len, self.label_pad_token_id)\n",
    "        \n",
    "        attention_mask = self.generate_attention_mask(input_ids_padded, self.tokenizer.pad_token_id)\n",
    "        decoder_attention_mask = self.generate_attention_mask(labels_padded, self.tokenizer.pad_token_id)\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": input_ids_padded,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"decoder_attention_mask\": decoder_attention_mask,\n",
    "            \"labels\": labels_padded,\n",
    "        }\n",
    "\n",
    "    def process_encoded_text(self, sequences, sequences_len, pad_token_id):\n",
    "        sequences_max_len = np.max(sequences_len)\n",
    "        max_length = min(sequences_max_len, self.max_length)\n",
    "        padded_sequences = self.pad_sequences(sequences, max_length, pad_token_id)\n",
    "        return torch.LongTensor(padded_sequences)\n",
    "\n",
    "    def generate_attention_mask(self, input_ids, pad_token_id):\n",
    "        return (input_ids != pad_token_id).long()\n",
    "    \n",
    "    def pad_sequences(self, sequences, max_length, pad_token_id):\n",
    "        num_samples = len(sequences)\n",
    "        padded_sequences = np.full((num_samples, max_length), pad_token_id)\n",
    "        for i, sequence in enumerate(sequences):\n",
    "            sequence = np.array(sequence)[:max_length]\n",
    "            padded_sequences[i, :len(sequence)] = sequence\n",
    "        return padded_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "If using all scalar values, you must pass an index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/ql/35698m6j55sfgbxr9rn581040000gn/T/ipykernel_44066/1651573470.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0minput_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mfrom_dict\u001b[0;34m(cls, data, orient, dtype, columns)\u001b[0m\n\u001b[1;32m   1591\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"only recognize index or columns for orient\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1592\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1593\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1594\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1595\u001b[0m     def to_numpy(\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    612\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m             \u001b[0;31m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 614\u001b[0;31m             \u001b[0mmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict_to_mgr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtyp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmanager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    615\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMaskedArray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m             \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmrecords\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmrecords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36mdict_to_mgr\u001b[0;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[1;32m    462\u001b[0m         \u001b[0;31m# TODO: can we get rid of the dt64tz special case above?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 464\u001b[0;31m     return arrays_to_mgr(\n\u001b[0m\u001b[1;32m    465\u001b[0m         \u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtyp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtyp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsolidate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m     )\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, arr_names, index, columns, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;31m# figure out the index, if necessary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_extract_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36m_extract_index\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    623\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mindexes\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mraw_lengths\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 625\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"If using all scalar values, you must pass an index\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhave_series\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: If using all scalar values, you must pass an index"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "input_df = pd.DataFrame.from_dict(model_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '�'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '—'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'ë'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '1'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '–'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '2'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '0'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '3'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '7'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '5'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '8'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '4'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '6'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '9'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '£'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'é'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '…'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '•'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'ó'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'á'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'ä'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'í'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'Â'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'ü'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'ç'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'ö'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'â'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '©'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'ã'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '€'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'š'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'ć'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'č'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'ł'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'ş'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'à'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '­'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'ê'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'ú'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '×'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'ñ'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'ß'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'è'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '®'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '»'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '´'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'ı'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'ô'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'å'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '«'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '​'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '️'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '°'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '″'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '·'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '™'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'Ã'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'î'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'ø'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '½'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '―'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'ā'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '¢'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'É'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'р'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'е'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '►'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'а'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'ي'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '■'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'ğ'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'û'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '‑'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'ر'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'ا'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'ï'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '●'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'æ'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'Ö'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '¥'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '‎'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'ل'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'Ü'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'ن'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'م'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'я'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '′'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'و'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'о'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'º'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'и'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'т'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'ा'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '¨'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'с'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'н'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'д'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'ō'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'ة'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'ت'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'د'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '§'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'ð'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '‐'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'ه'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '²'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'س'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'л'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'ب'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'к'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '★'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'ع'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'ū'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '→'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'у'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '−'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'в'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'ī'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'ē'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '✓'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'µ'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '的'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'α'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '±'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'Í'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '♦'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '。'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '🙂'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '☆'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '✔'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'м'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'ο'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '¶'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'ו'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '♥'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'َ'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'ι'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '≥'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '›'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'י'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'ы'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '˜'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'μ'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'ь'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'ε'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'ν'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'τ'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '・'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'β'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '、'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '¯'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'ר'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'ρ'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '中'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'σ'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'ς'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'ל'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'い'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'ה'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'ת'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'し'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '人'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'に'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'の'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '†'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'ン'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '̶'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'κ'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '一'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'ِ'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'π'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'た'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'で'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'λ'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'て'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'な'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '大'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'と'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'א'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'ْ'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'ש'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'る'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'ツ'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'γ'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'מ'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'ב'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'を'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'ー'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '░'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'が'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'Δ'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'は'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '是'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'ま'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'イ'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '≤'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '↑'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '不'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'っ'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'う'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'す'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'か'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'ス'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '上'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'り'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'も'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'ω'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'ん'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'れ'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '│'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'ら'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'リ'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '生'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'ア'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'ラ'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'く'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'き'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'こ'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'カ'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'ジ'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '之'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'ˈ'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'ル'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '▓'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'ּ'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '子'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'þ'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'さ'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '─'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'コ'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '作'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '」'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '方'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'あ'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'だ'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'ト'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '【'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'シ'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '】'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '█'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'テ'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '将'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'ロ'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'ク'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'ノ'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '者'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'タ'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'ッ'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'レ'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '「'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'ウ'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '天'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'ュ'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'バ'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '三'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '∼'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '機'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '├'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'ニ'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'フ'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'プ'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '代'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '光'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'オ'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'Û'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'マ'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'チ'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'ャ'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'グ'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '士'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '使'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'ド'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'ブ'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '王'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '版'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'サ'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'メ'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'ェ'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '女'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'キ'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'ズ'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'ム'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '═'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'ビ'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'パ'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'ミ'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'ネ'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'エ'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '神'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'ナ'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '田'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'ダ'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '五'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'デ'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'ハ'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'ケ'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'ベ'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '▬'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'ザ'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'ィ'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'セ'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'ガ'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'モ'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'ワ'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'ⓘ'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: ''\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'ソ'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '▒'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'ゴ'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '武'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '━'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '』'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '『'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '装'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'ㅋ'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '龍'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'ァ'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'ォ'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'ヤ'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '戦'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'ギ'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '≡'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'ゼ'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '◼'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'ヴ'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'ヘ'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '魔'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '▀'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '黒'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '闘'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '姫'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '▄'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '裏'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '極'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '\u0000'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '\u0001'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '\u0002'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '\u0003'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '\u0004'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '\u0005'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '\u0006'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '\u0007'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '\b'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '\u000e'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '\u000f'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '\u0010'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '\u0011'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '\u0012'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '\u0013'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '\u0014'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '\u0015'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '\u0016'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '\u0017'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '\u0018'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '\u0019'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '\u001a'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '\u001b'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '醒'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '覚'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '喚'\n",
      "  warnings.warn(\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/nltk/tokenize/sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '契'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#here im making a dictionary of all tokens and their associated syllable count\n",
    "from nltk.tokenize import SyllableTokenizer\n",
    "syllable_dict = {}\n",
    "for token in range(50265):\n",
    "    syl_tokenizer = SyllableTokenizer()\n",
    "    t = torch.tensor(token)\n",
    "    #print(tokenizer.decode(t),len(syl_tokenizer.tokenize(tokenizer.decode(t).strip())))\n",
    "    syllable_dict[token] = len(syl_tokenizer.tokenize(tokenizer.decode(t).strip()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16 \n",
    "\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    \"BART-base-syllable-loss\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    #learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "   # weight_decay=0.01,\n",
    "   # save_total_limit=3,\n",
    "   # num_train_epochs=1,\n",
    "   # predict_with_generate=True,\n",
    "    \n",
    "    learning_rate=1e-4,\n",
    "    weight_decay=0.01,\n",
    "    adam_beta1=0.9,\n",
    "    adam_beta2=0.999,\n",
    "    adam_epsilon=1e-6,\n",
    "    max_grad_norm=1.0,\n",
    "    num_train_epochs=10,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    warmup_ratio=0.1,\n",
    "   # label_names=\"labels\",\n",
    "    #fp16=True,\n",
    "  #  use_auth_token=False\n",
    "    #push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from transformers import Trainer\n",
    "\n",
    "class Syllable_Trainer(Seq2SeqTrainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.get(\"labels\")\n",
    "\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get('logits')\n",
    "        loss_fct = nn.CrossEntropyLoss(reduction='none')\n",
    "\n",
    "        loss = loss_fct(logits.view(-1, 50265), labels.view(-1))\n",
    "\n",
    "        \n",
    "        \n",
    "        #add to the loss dist between predicted labels\n",
    "        #need a differentiable way to add loss from syllable counbt\n",
    "        #will increase or decrease the loss based on the adherence to proper syllable count\n",
    "        tokens = logits.argmax(axis=2).view(-1)\n",
    "        observed_syllables = torch.tensor([syllable_dict[t] for t in tokens.tolist()])\n",
    "        correct_syllables = torch.tensor([syllable_dict[t] for t in labels.view(-1).tolist()])\n",
    "        \n",
    "        #do not c\n",
    "        y = (torch.ones(labels.shape).view(-1)) * -.1\n",
    "        syl_diff = torch.add(torch.where(torch.abs(observed_syllables - correct_syllables) * .1 > 0 ,\n",
    "                                   torch.abs(observed_syllables - correct_syllables) * .1, y),\n",
    "                                   1.0).cuda()\n",
    "\n",
    "        loss = torch.mul(loss,syl_diff)\n",
    "        loss = torch.mean(loss)\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_collator = Collator_poem_gen(tokenizer=tokenizer, max_length=512)\n",
    "#train_ds = PoemDataset(model_input)\n",
    "#eval_ds = PoemDataset(model_input)\n",
    "trainer = Syllable_Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "   # data_collator=data_collator,\n",
    "    train_dataset=train_ds,\n",
    "    \n",
    "    eval_dataset=eval_ds,\n",
    "    #use_auth_token=False,\n",
    "    \n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -c \"from huggingface_hub.hf_api import HfFolder; HfFolder.save_token('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')\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 136825\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 85520\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='24765' max='85520' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [24765/85520 4:24:15 < 10:48:20, 1.56 it/s, Epoch 2.90/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.443700</td>\n",
       "      <td>0.416811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.407600</td>\n",
       "      <td>0.396644</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to BART-base-syllable-loss/checkpoint-500\n",
      "Configuration saved in BART-base-syllable-loss/checkpoint-500/config.json\n",
      "Model weights saved in BART-base-syllable-loss/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base-syllable-loss/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in BART-base-syllable-loss/checkpoint-500/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base-syllable-loss/checkpoint-1000\n",
      "Configuration saved in BART-base-syllable-loss/checkpoint-1000/config.json\n",
      "Model weights saved in BART-base-syllable-loss/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base-syllable-loss/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in BART-base-syllable-loss/checkpoint-1000/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base-syllable-loss/checkpoint-1500\n",
      "Configuration saved in BART-base-syllable-loss/checkpoint-1500/config.json\n",
      "Model weights saved in BART-base-syllable-loss/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base-syllable-loss/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in BART-base-syllable-loss/checkpoint-1500/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base-syllable-loss/checkpoint-2000\n",
      "Configuration saved in BART-base-syllable-loss/checkpoint-2000/config.json\n",
      "Model weights saved in BART-base-syllable-loss/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base-syllable-loss/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in BART-base-syllable-loss/checkpoint-2000/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base-syllable-loss/checkpoint-2500\n",
      "Configuration saved in BART-base-syllable-loss/checkpoint-2500/config.json\n",
      "Model weights saved in BART-base-syllable-loss/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base-syllable-loss/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in BART-base-syllable-loss/checkpoint-2500/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base-syllable-loss/checkpoint-3000\n",
      "Configuration saved in BART-base-syllable-loss/checkpoint-3000/config.json\n",
      "Model weights saved in BART-base-syllable-loss/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base-syllable-loss/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in BART-base-syllable-loss/checkpoint-3000/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base-syllable-loss/checkpoint-3500\n",
      "Configuration saved in BART-base-syllable-loss/checkpoint-3500/config.json\n",
      "Model weights saved in BART-base-syllable-loss/checkpoint-3500/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base-syllable-loss/checkpoint-3500/tokenizer_config.json\n",
      "Special tokens file saved in BART-base-syllable-loss/checkpoint-3500/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base-syllable-loss/checkpoint-4000\n",
      "Configuration saved in BART-base-syllable-loss/checkpoint-4000/config.json\n",
      "Model weights saved in BART-base-syllable-loss/checkpoint-4000/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base-syllable-loss/checkpoint-4000/tokenizer_config.json\n",
      "Special tokens file saved in BART-base-syllable-loss/checkpoint-4000/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base-syllable-loss/checkpoint-4500\n",
      "Configuration saved in BART-base-syllable-loss/checkpoint-4500/config.json\n",
      "Model weights saved in BART-base-syllable-loss/checkpoint-4500/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base-syllable-loss/checkpoint-4500/tokenizer_config.json\n",
      "Special tokens file saved in BART-base-syllable-loss/checkpoint-4500/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base-syllable-loss/checkpoint-5000\n",
      "Configuration saved in BART-base-syllable-loss/checkpoint-5000/config.json\n",
      "Model weights saved in BART-base-syllable-loss/checkpoint-5000/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base-syllable-loss/checkpoint-5000/tokenizer_config.json\n",
      "Special tokens file saved in BART-base-syllable-loss/checkpoint-5000/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base-syllable-loss/checkpoint-5500\n",
      "Configuration saved in BART-base-syllable-loss/checkpoint-5500/config.json\n",
      "Model weights saved in BART-base-syllable-loss/checkpoint-5500/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base-syllable-loss/checkpoint-5500/tokenizer_config.json\n",
      "Special tokens file saved in BART-base-syllable-loss/checkpoint-5500/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base-syllable-loss/checkpoint-6000\n",
      "Configuration saved in BART-base-syllable-loss/checkpoint-6000/config.json\n",
      "Model weights saved in BART-base-syllable-loss/checkpoint-6000/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base-syllable-loss/checkpoint-6000/tokenizer_config.json\n",
      "Special tokens file saved in BART-base-syllable-loss/checkpoint-6000/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base-syllable-loss/checkpoint-6500\n",
      "Configuration saved in BART-base-syllable-loss/checkpoint-6500/config.json\n",
      "Model weights saved in BART-base-syllable-loss/checkpoint-6500/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base-syllable-loss/checkpoint-6500/tokenizer_config.json\n",
      "Special tokens file saved in BART-base-syllable-loss/checkpoint-6500/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base-syllable-loss/checkpoint-7000\n",
      "Configuration saved in BART-base-syllable-loss/checkpoint-7000/config.json\n",
      "Model weights saved in BART-base-syllable-loss/checkpoint-7000/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base-syllable-loss/checkpoint-7000/tokenizer_config.json\n",
      "Special tokens file saved in BART-base-syllable-loss/checkpoint-7000/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base-syllable-loss/checkpoint-7500\n",
      "Configuration saved in BART-base-syllable-loss/checkpoint-7500/config.json\n",
      "Model weights saved in BART-base-syllable-loss/checkpoint-7500/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base-syllable-loss/checkpoint-7500/tokenizer_config.json\n",
      "Special tokens file saved in BART-base-syllable-loss/checkpoint-7500/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base-syllable-loss/checkpoint-8000\n",
      "Configuration saved in BART-base-syllable-loss/checkpoint-8000/config.json\n",
      "Model weights saved in BART-base-syllable-loss/checkpoint-8000/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base-syllable-loss/checkpoint-8000/tokenizer_config.json\n",
      "Special tokens file saved in BART-base-syllable-loss/checkpoint-8000/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base-syllable-loss/checkpoint-8500\n",
      "Configuration saved in BART-base-syllable-loss/checkpoint-8500/config.json\n",
      "Model weights saved in BART-base-syllable-loss/checkpoint-8500/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base-syllable-loss/checkpoint-8500/tokenizer_config.json\n",
      "Special tokens file saved in BART-base-syllable-loss/checkpoint-8500/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7202\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to BART-base-syllable-loss/checkpoint-9000\n",
      "Configuration saved in BART-base-syllable-loss/checkpoint-9000/config.json\n",
      "Model weights saved in BART-base-syllable-loss/checkpoint-9000/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base-syllable-loss/checkpoint-9000/tokenizer_config.json\n",
      "Special tokens file saved in BART-base-syllable-loss/checkpoint-9000/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base-syllable-loss/checkpoint-9500\n",
      "Configuration saved in BART-base-syllable-loss/checkpoint-9500/config.json\n",
      "Model weights saved in BART-base-syllable-loss/checkpoint-9500/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base-syllable-loss/checkpoint-9500/tokenizer_config.json\n",
      "Special tokens file saved in BART-base-syllable-loss/checkpoint-9500/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base-syllable-loss/checkpoint-10000\n",
      "Configuration saved in BART-base-syllable-loss/checkpoint-10000/config.json\n",
      "Model weights saved in BART-base-syllable-loss/checkpoint-10000/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base-syllable-loss/checkpoint-10000/tokenizer_config.json\n",
      "Special tokens file saved in BART-base-syllable-loss/checkpoint-10000/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to BART-base-syllable-loss/checkpoint-10500\n",
      "Configuration saved in BART-base-syllable-loss/checkpoint-10500/config.json\n",
      "Model weights saved in BART-base-syllable-loss/checkpoint-10500/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base-syllable-loss/checkpoint-10500/tokenizer_config.json\n",
      "Special tokens file saved in BART-base-syllable-loss/checkpoint-10500/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base-syllable-loss/checkpoint-11000\n",
      "Configuration saved in BART-base-syllable-loss/checkpoint-11000/config.json\n",
      "Model weights saved in BART-base-syllable-loss/checkpoint-11000/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base-syllable-loss/checkpoint-11000/tokenizer_config.json\n",
      "Special tokens file saved in BART-base-syllable-loss/checkpoint-11000/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base-syllable-loss/checkpoint-11500\n",
      "Configuration saved in BART-base-syllable-loss/checkpoint-11500/config.json\n",
      "Model weights saved in BART-base-syllable-loss/checkpoint-11500/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base-syllable-loss/checkpoint-11500/tokenizer_config.json\n",
      "Special tokens file saved in BART-base-syllable-loss/checkpoint-11500/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base-syllable-loss/checkpoint-12000\n",
      "Configuration saved in BART-base-syllable-loss/checkpoint-12000/config.json\n",
      "Model weights saved in BART-base-syllable-loss/checkpoint-12000/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base-syllable-loss/checkpoint-12000/tokenizer_config.json\n",
      "Special tokens file saved in BART-base-syllable-loss/checkpoint-12000/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base-syllable-loss/checkpoint-12500\n",
      "Configuration saved in BART-base-syllable-loss/checkpoint-12500/config.json\n",
      "Model weights saved in BART-base-syllable-loss/checkpoint-12500/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base-syllable-loss/checkpoint-12500/tokenizer_config.json\n",
      "Special tokens file saved in BART-base-syllable-loss/checkpoint-12500/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base-syllable-loss/checkpoint-13000\n",
      "Configuration saved in BART-base-syllable-loss/checkpoint-13000/config.json\n",
      "Model weights saved in BART-base-syllable-loss/checkpoint-13000/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base-syllable-loss/checkpoint-13000/tokenizer_config.json\n",
      "Special tokens file saved in BART-base-syllable-loss/checkpoint-13000/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base-syllable-loss/checkpoint-13500\n",
      "Configuration saved in BART-base-syllable-loss/checkpoint-13500/config.json\n",
      "Model weights saved in BART-base-syllable-loss/checkpoint-13500/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base-syllable-loss/checkpoint-13500/tokenizer_config.json\n",
      "Special tokens file saved in BART-base-syllable-loss/checkpoint-13500/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base-syllable-loss/checkpoint-14000\n",
      "Configuration saved in BART-base-syllable-loss/checkpoint-14000/config.json\n",
      "Model weights saved in BART-base-syllable-loss/checkpoint-14000/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base-syllable-loss/checkpoint-14000/tokenizer_config.json\n",
      "Special tokens file saved in BART-base-syllable-loss/checkpoint-14000/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base-syllable-loss/checkpoint-14500\n",
      "Configuration saved in BART-base-syllable-loss/checkpoint-14500/config.json\n",
      "Model weights saved in BART-base-syllable-loss/checkpoint-14500/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base-syllable-loss/checkpoint-14500/tokenizer_config.json\n",
      "Special tokens file saved in BART-base-syllable-loss/checkpoint-14500/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base-syllable-loss/checkpoint-15000\n",
      "Configuration saved in BART-base-syllable-loss/checkpoint-15000/config.json\n",
      "Model weights saved in BART-base-syllable-loss/checkpoint-15000/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base-syllable-loss/checkpoint-15000/tokenizer_config.json\n",
      "Special tokens file saved in BART-base-syllable-loss/checkpoint-15000/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base-syllable-loss/checkpoint-15500\n",
      "Configuration saved in BART-base-syllable-loss/checkpoint-15500/config.json\n",
      "Model weights saved in BART-base-syllable-loss/checkpoint-15500/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base-syllable-loss/checkpoint-15500/tokenizer_config.json\n",
      "Special tokens file saved in BART-base-syllable-loss/checkpoint-15500/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base-syllable-loss/checkpoint-16000\n",
      "Configuration saved in BART-base-syllable-loss/checkpoint-16000/config.json\n",
      "Model weights saved in BART-base-syllable-loss/checkpoint-16000/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base-syllable-loss/checkpoint-16000/tokenizer_config.json\n",
      "Special tokens file saved in BART-base-syllable-loss/checkpoint-16000/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base-syllable-loss/checkpoint-16500\n",
      "Configuration saved in BART-base-syllable-loss/checkpoint-16500/config.json\n",
      "Model weights saved in BART-base-syllable-loss/checkpoint-16500/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base-syllable-loss/checkpoint-16500/tokenizer_config.json\n",
      "Special tokens file saved in BART-base-syllable-loss/checkpoint-16500/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base-syllable-loss/checkpoint-17000\n",
      "Configuration saved in BART-base-syllable-loss/checkpoint-17000/config.json\n",
      "Model weights saved in BART-base-syllable-loss/checkpoint-17000/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base-syllable-loss/checkpoint-17000/tokenizer_config.json\n",
      "Special tokens file saved in BART-base-syllable-loss/checkpoint-17000/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7202\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to BART-base-syllable-loss/checkpoint-17500\n",
      "Configuration saved in BART-base-syllable-loss/checkpoint-17500/config.json\n",
      "Model weights saved in BART-base-syllable-loss/checkpoint-17500/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base-syllable-loss/checkpoint-17500/tokenizer_config.json\n",
      "Special tokens file saved in BART-base-syllable-loss/checkpoint-17500/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base-syllable-loss/checkpoint-18000\n",
      "Configuration saved in BART-base-syllable-loss/checkpoint-18000/config.json\n",
      "Model weights saved in BART-base-syllable-loss/checkpoint-18000/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base-syllable-loss/checkpoint-18000/tokenizer_config.json\n",
      "Special tokens file saved in BART-base-syllable-loss/checkpoint-18000/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base-syllable-loss/checkpoint-18500\n",
      "Configuration saved in BART-base-syllable-loss/checkpoint-18500/config.json\n",
      "Model weights saved in BART-base-syllable-loss/checkpoint-18500/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base-syllable-loss/checkpoint-18500/tokenizer_config.json\n",
      "Special tokens file saved in BART-base-syllable-loss/checkpoint-18500/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base-syllable-loss/checkpoint-19000\n",
      "Configuration saved in BART-base-syllable-loss/checkpoint-19000/config.json\n",
      "Model weights saved in BART-base-syllable-loss/checkpoint-19000/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base-syllable-loss/checkpoint-19000/tokenizer_config.json\n",
      "Special tokens file saved in BART-base-syllable-loss/checkpoint-19000/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base-syllable-loss/checkpoint-19500\n",
      "Configuration saved in BART-base-syllable-loss/checkpoint-19500/config.json\n",
      "Model weights saved in BART-base-syllable-loss/checkpoint-19500/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base-syllable-loss/checkpoint-19500/tokenizer_config.json\n",
      "Special tokens file saved in BART-base-syllable-loss/checkpoint-19500/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base-syllable-loss/checkpoint-20000\n",
      "Configuration saved in BART-base-syllable-loss/checkpoint-20000/config.json\n",
      "Model weights saved in BART-base-syllable-loss/checkpoint-20000/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base-syllable-loss/checkpoint-20000/tokenizer_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens file saved in BART-base-syllable-loss/checkpoint-20000/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base-syllable-loss/checkpoint-20500\n",
      "Configuration saved in BART-base-syllable-loss/checkpoint-20500/config.json\n",
      "Model weights saved in BART-base-syllable-loss/checkpoint-20500/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base-syllable-loss/checkpoint-20500/tokenizer_config.json\n",
      "Special tokens file saved in BART-base-syllable-loss/checkpoint-20500/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base-syllable-loss/checkpoint-21000\n",
      "Configuration saved in BART-base-syllable-loss/checkpoint-21000/config.json\n",
      "Model weights saved in BART-base-syllable-loss/checkpoint-21000/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base-syllable-loss/checkpoint-21000/tokenizer_config.json\n",
      "Special tokens file saved in BART-base-syllable-loss/checkpoint-21000/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base-syllable-loss/checkpoint-21500\n",
      "Configuration saved in BART-base-syllable-loss/checkpoint-21500/config.json\n",
      "Model weights saved in BART-base-syllable-loss/checkpoint-21500/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base-syllable-loss/checkpoint-21500/tokenizer_config.json\n",
      "Special tokens file saved in BART-base-syllable-loss/checkpoint-21500/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base-syllable-loss/checkpoint-22000\n",
      "Configuration saved in BART-base-syllable-loss/checkpoint-22000/config.json\n",
      "Model weights saved in BART-base-syllable-loss/checkpoint-22000/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base-syllable-loss/checkpoint-22000/tokenizer_config.json\n",
      "Special tokens file saved in BART-base-syllable-loss/checkpoint-22000/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base-syllable-loss/checkpoint-22500\n",
      "Configuration saved in BART-base-syllable-loss/checkpoint-22500/config.json\n",
      "Model weights saved in BART-base-syllable-loss/checkpoint-22500/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base-syllable-loss/checkpoint-22500/tokenizer_config.json\n",
      "Special tokens file saved in BART-base-syllable-loss/checkpoint-22500/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base-syllable-loss/checkpoint-23000\n",
      "Configuration saved in BART-base-syllable-loss/checkpoint-23000/config.json\n",
      "Model weights saved in BART-base-syllable-loss/checkpoint-23000/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base-syllable-loss/checkpoint-23000/tokenizer_config.json\n",
      "Special tokens file saved in BART-base-syllable-loss/checkpoint-23000/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base-syllable-loss/checkpoint-23500\n",
      "Configuration saved in BART-base-syllable-loss/checkpoint-23500/config.json\n",
      "Model weights saved in BART-base-syllable-loss/checkpoint-23500/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base-syllable-loss/checkpoint-23500/tokenizer_config.json\n",
      "Special tokens file saved in BART-base-syllable-loss/checkpoint-23500/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base-syllable-loss/checkpoint-24000\n",
      "Configuration saved in BART-base-syllable-loss/checkpoint-24000/config.json\n",
      "Model weights saved in BART-base-syllable-loss/checkpoint-24000/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base-syllable-loss/checkpoint-24000/tokenizer_config.json\n",
      "Special tokens file saved in BART-base-syllable-loss/checkpoint-24000/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base-syllable-loss/checkpoint-24500\n",
      "Configuration saved in BART-base-syllable-loss/checkpoint-24500/config.json\n",
      "Model weights saved in BART-base-syllable-loss/checkpoint-24500/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base-syllable-loss/checkpoint-24500/tokenizer_config.json\n",
      "Special tokens file saved in BART-base-syllable-loss/checkpoint-24500/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "del trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Seq2SeqTrainer' object has no attribute 'plot'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-7f90a5714312>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'Seq2SeqTrainer' object has no attribute 'plot'"
     ]
    }
   ],
   "source": [
    "trainer.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
