{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Haiku Generation GAN fine-tuning w/ fine-tuned Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-a068916ada98>:12: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"rouge\")\n"
     ]
    }
   ],
   "source": [
    "from transformers import BartTokenizer, BartModel,BartForConditionalGeneration\n",
    "from torch.distributions import Categorical\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from undecorated import undecorated\n",
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from transformers import  Seq2SeqTrainingArguments, Seq2SeqTrainer,DataCollatorForSeq2Seq\n",
    "metric = load_metric(\"rouge\")\n",
    "import torch.optim as optim\n",
    "import torchvision.utils as vutils\n",
    "import types\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.distributions import Categorical\n",
    "import random\n",
    "#from torch.linalg import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gan_utils import get_end_mask,get_valid_mask,Discriminator,discriminator_train_standard, generate_random_input,reinforce_loss,Generator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import PoemDataset,encode_sentences\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-base')\n",
    "#generator = BartForConditionalGeneration.from_pretrained('facebook/bart-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data_dir =  '/home/alexander/nlg-project/data/'\n",
    "test_df = pd.read_csv(processed_data_dir + 'test_data.csv')\n",
    "test_model = encode_sentences(tokenizer,test_df)\n",
    "\n",
    "train_df = pd.read_csv(processed_data_dir + 'train_data.csv')\n",
    "train_model = encode_sentences(tokenizer,train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#logprob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#_,logprob = simple_decoding(generator,noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = PoemDataset(train_model)\n",
    "eval_ds = PoemDataset(test_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#discriminator = discriminator.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fixed_noise = generate_random_input(bsize,'cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = '/home/alexander/nlg-project/GAN_models_finetuned/'\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    " #reduced to a binary problem \n",
    "bsize = 12\n",
    "generator =  Generator().cuda()#BartForConditionalGeneration.from_pretrained('facebook/bart-base').cuda()\n",
    "\n",
    "#first time was loaded from finetuned BART \n",
    "generator.load_state_dict(torch.load(PATH + 'generator.pt')['model_state_dict'])\n",
    "discriminator = Discriminator().cuda()\n",
    "discriminator.load_state_dict(torch.load(PATH + 'discriminator.pt')['model_state_dict'])\n",
    "criterion = nn.BCEWithLogitsLoss(reduction='none')\n",
    "\n",
    "# Create batch of latent vectors that we will use to visualize\n",
    "#  the progression of the generator\n",
    "fixed_noise = generate_random_input(bsize,'cuda')\n",
    "\n",
    "# Establish convention for real and fake labels during training\n",
    "real_label = 1.\n",
    "ewma_reward = 0.0#torch.tensor(0.0)\n",
    "fake_label = 0.\n",
    "lr = .000001\n",
    "beta1 = .9\n",
    "# Setup Adam optimizers for both G and D\n",
    "optimizerD = optim.Adam(discriminator.parameters(), lr=lr*.05, betas=(beta1, 0.99)) \n",
    "optimizerG = optim.Adam(generator.parameters(), lr=lr, betas=(beta1, 0.99))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'=20.9'                     \u001b[0m\u001b[01;34mGAN_models\u001b[0m/             \u001b[01;34mdata\u001b[0m/\r\n",
      " BART-GAN-finetuned.ipynb   \u001b[01;34mGAN_models_finetuned\u001b[0m/   data_processing.ipynb\r\n",
      " BART-GAN.ipynb             G_3epochs.txt           \u001b[01;34mdata_raw\u001b[0m/\r\n",
      " \u001b[01;34mBART-base\u001b[0m/                 G_losses_2.txt          finetuned_bart.ipynb\r\n",
      " D_3epochs.txt              G_losses_3.txt          utils.py\r\n",
      " D_losses_2.txt             __init__.py\r\n",
      " D_losses_3.txt             \u001b[01;34m__pycache__\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fixed_noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#out.sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "with open(\"g_loss_finetuned_four.txt\", \"w\") as f:\n",
    "    for s in G_losses:\n",
    "        f.write(str(s) +\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "with open(\"d_loss_finetuned_four.txt\", \"w\") as f:\n",
    "    for s in D_losses:\n",
    "        f.write(str(s) +\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training Loop...\n",
      "Label Noise: 0.05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-f6867a44ab39>:82: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  output_tokens = torch.cat([output_tokens,torch.tensor(next_word).unsqueeze(1)],axis=1)\n",
      "/home/alexander/anaconda3/lib/python3.8/site-packages/torch/nn/functional.py:1625: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disc LogProbs True\n",
      "tensor([[-0.5322, -0.2014, -0.4056, -0.5340, -0.9182, -0.0259,  0.1825,  0.5351,\n",
      "          1.3155,  1.3597,  1.6611,  2.1858,  1.7843,  2.2845,  2.4338,  2.4559,\n",
      "          2.7625,  2.9814,  3.0954,  3.2381,  3.1147]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward>)\n",
      "Disc LogProbs False\n",
      "tensor([[ 0.2824, -0.2379, -0.3094, -0.0000, -0.0000,  0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "gen loss\n",
      "tensor([[ 0.0236, -0.0232, -0.0266,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cum rewards\n",
      "tensor([[ 0.1403, -0.1539, -0.2133,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "gen logprobs\n",
      "tensor([[-1.2456e-01, -2.2157e-01, -1.6169e-01, -1.7237e-02, -3.8172e-02,\n",
      "         -8.9531e-03, -3.8681e-03, -4.9973e-04, -1.4305e-04, -5.5313e-05,\n",
      "         -2.4796e-05, -4.0054e-05, -7.4387e-05, -3.8147e-05, -2.6703e-05]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "[' y used to<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>']\n",
      "[0/30][0/11403]\tLoss_D: 8.3254\tLoss_G: -0.0087\tD(x): 1.6345\tD(G(z)): -0.0874 / -0.0087\n",
      "['</s><s> used to suck the\\nless neither i nor i\\ni</s><pad>', '</s><s> used to suck the\\nless neither i nor i\\ni</s><pad>']\n",
      "saving geneartor\n",
      "[0/30][128/11403]\tLoss_D: 8.1834\tLoss_G: 0.0161\tD(x): 1.5679\tD(G(z)): 0.0258 / 0.0161\n",
      "Disc LogProbs True\n",
      "tensor([[ 0.0173,  0.1255,  0.3549,  0.7576, -0.2077,  0.0096, -0.3719, -0.3399,\n",
      "         -0.4260, -0.7055, -0.4668, -0.1764,  0.9298,  1.5591,  1.6704,  1.8491,\n",
      "          2.0984,  2.4949,  2.0556,  2.9398,  0.0000]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward>)\n",
      "Disc LogProbs False\n",
      "tensor([[-0.1435, -0.0015, -0.8552, -0.7034, -0.0000, -0.2474, -0.2837, -0.5005,\n",
      "         -0.6828, -0.4939, -0.3200, -0.3391, -0.5984, -0.7014, -0.6689]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "gen loss\n",
      "tensor([[ 0.0024,  0.0005, -0.0436, -0.1025,  0.0000,  0.0746,  0.0554, -0.0219,\n",
      "         -0.0363, -0.0093,  0.0115,  0.0095, -0.0234, -0.0353, -0.3400]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cum rewards\n",
      "tensor([[-0.0716, -0.0009, -0.5606, -0.4788,  0.0000, -0.1757, -0.2012, -0.3502,\n",
      "         -0.4696, -0.3458, -0.2267, -0.2399, -0.4151, -0.4814, -0.4607]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "gen logprobs\n",
      "tensor([[-1.0725e-02, -1.7376e-03, -1.6291e-01, -5.5119e-01, -5.1475e+00,\n",
      "         -6.3739e-01, -6.0511e-01, -3.8100e-01, -2.0547e-01, -1.7463e-01,\n",
      "         -1.7369e-01, -1.7913e-01, -1.9110e-01, -1.8722e-01, -2.0245e+00]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "['\\n\\ni i<pad> i i i i i i i i i y']\n",
      "[0/30][256/11403]\tLoss_D: 13.8566\tLoss_G: -0.0676\tD(x): 1.3015\tD(G(z)): -0.4405 / -0.0676\n",
      "[0/30][384/11403]\tLoss_D: 6.0521\tLoss_G: -0.0036\tD(x): 0.8303\tD(G(z)): -0.0024 / -0.0036\n",
      "['</s><s> wowed with my\\nless neither i nor i\\ni neither y</s>', '</s><s> wowed with my\\nless neither i nor i\\ni neither y</s>']\n",
      "Disc LogProbs True\n",
      "tensor([[0.1380, 0.5830, 0.2286, 1.1501, 2.8850, 2.8544, 2.7895, 2.9322, 2.5655,\n",
      "         2.9592, 2.8803, 3.1195, 3.1700, 3.1978, 3.0961, 3.2978, 3.5410, 2.5811,\n",
      "         3.3608, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, -0.0000, -0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000]], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "Disc LogProbs False\n",
      "tensor([[-0., 0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., 0., 0.]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "gen loss\n",
      "tensor([[-0., -0., -0., -0., -0., -0., -0., -0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cum rewards\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "gen logprobs\n",
      "tensor([[-7.6294e-05, -1.9703e-03, -6.8665e-05, -1.5259e-05, -3.8147e-06,\n",
      "         -1.9073e-06, -1.9073e-06, -1.9073e-06,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "['<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>']\n",
      "[0/30][512/11403]\tLoss_D: 3.2661\tLoss_G: 0.0000\tD(x): 1.1706\tD(G(z)): 0.0000 / 0.0000\n",
      "[0/30][640/11403]\tLoss_D: 1.9168\tLoss_G: 0.0000\tD(x): 2.3518\tD(G(z)): 0.0000 / 0.0000\n",
      "Disc LogProbs True\n",
      "tensor([[0.7411, 0.8027, 1.0775, 1.8466, 2.6457, 3.0493, 3.0889, 3.5063, 3.5605,\n",
      "         3.7244, 3.6024, 3.5565, 3.6215, 3.5977, 3.7191, 3.8050, 3.8494, 3.9246,\n",
      "         3.9997, 3.9700, 3.9538, 3.9637, 3.7770, 3.8264, 0.0000]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "Disc LogProbs False\n",
      "tensor([[ 0.0295, -0.7878, -1.3132, -0.8275, -1.1426, -0.1187,  0.4383,  0.8127,\n",
      "          1.2844,  1.4164,  1.4497,  1.3557,  1.0633,  0.5868,  0.8799]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "gen loss\n",
      "tensor([[-4.7093e-05, -2.4531e-04, -1.1772e-01, -3.0893e-01, -1.4040e-01,\n",
      "         -3.0554e-02,  2.0456e-01,  2.3485e+00,  1.7373e+00,  7.6370e+00,\n",
      "          3.8222e+00,  1.0870e+00,  9.3926e-01,  1.4749e+00,  1.0924e+00]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cum rewards\n",
      "tensor([[ 0.0148, -0.4871, -0.8008, -0.5550, -0.7358, -0.0847,  0.3081,  0.5505,\n",
      "          0.8091,  0.8708,  0.8856,  0.8430,  0.6952,  0.4075,  0.5909]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "gen logprobs\n",
      "tensor([[-5.0354e-04, -4.1199e-04, -1.2950e-01, -4.6576e-01, -1.6634e-01,\n",
      "         -1.5836e-01, -1.0237e+00, -5.3112e+00, -2.4789e+00, -1.0016e+01,\n",
      "         -4.9172e+00, -1.4794e+00, -1.6004e+00, -4.9288e+00, -2.2636e+00]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "['\\n\\nthe fertile\\nof my fucking harvest safely of my empty alone never']\n",
      "[0/30][768/11403]\tLoss_D: 16.9583\tLoss_G: 0.3999\tD(x): 1.8858\tD(G(z)): 0.2058 / 0.3999\n",
      "[0/30][896/11403]\tLoss_D: 10.2431\tLoss_G: -0.0648\tD(x): 0.7722\tD(G(z)): -1.1009 / -0.0648\n",
      "['</s><s> useless\\nless than the power\\nof my cle</s><pad><pad><pad><pad>', '</s><s> useless\\nless than the power\\nof my cle</s><pad><pad><pad><pad>']\n",
      "saving geneartor\n",
      "Disc LogProbs True\n",
      "tensor([[ 0.0479,  0.3043, -0.2370,  0.4955,  2.3505,  2.5207,  1.9582,  2.1192,\n",
      "          0.9892,  1.6668,  2.3261,  2.7783,  2.9793,  2.6585,  2.4060,  2.4566,\n",
      "          0.5914,  3.0490,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "Disc LogProbs False\n",
      "tensor([[-0.2194, -1.2692, -1.9629, -1.5978, -1.0871, -0.7404, -0.8247, -0.9178,\n",
      "         -0.8586, -1.4053, -1.7105, -1.7936, -1.6134, -1.9378, -1.9236]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "gen loss\n",
      "tensor([[ 3.1576e-03,  1.4818e-05, -7.2822e-02, -5.0617e-03,  8.3509e-03,\n",
      "          1.8256e-01,  1.1570e-01,  4.0491e-02,  1.7206e-02, -3.6632e-02,\n",
      "         -1.0872e-01, -8.6520e-02, -8.1629e-02, -5.5156e-02, -5.9581e-02]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cum rewards\n",
      "tensor([[-0.1093, -0.7296, -1.0476, -0.9400, -0.7064, -0.5056, -0.5577, -0.6131,\n",
      "         -0.5782, -0.8658, -0.9911, -1.0210, -0.9540, -1.0689, -1.0644]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "gen logprobs\n",
      "tensor([[-4.3449e-03, -1.3924e-04, -3.4409e-01, -4.8643e-02, -6.4428e-02,\n",
      "         -5.5253e-01, -4.1577e-01, -1.8167e-01, -6.6744e-02, -1.2305e+00,\n",
      "         -7.0078e-01, -4.6756e-01, -6.9189e-01, -2.3681e-01, -2.6086e-01]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "['\\n\\nthe quiet quiet quiet never\\nnever quiet fucking\\nnever fucking fucking']\n",
      "[0/30][1024/11403]\tLoss_D: 6.9506\tLoss_G: -0.1389\tD(x): 1.3794\tD(G(z)): -1.4688 / -0.1389\n",
      "[0/30][1152/11403]\tLoss_D: 4.7381\tLoss_G: -0.1013\tD(x): 2.3929\tD(G(z)): -1.6593 / -0.1013\n",
      "Disc LogProbs True\n",
      "tensor([[1.9305, 3.0918, 2.4316, 2.0192, 2.2001, 2.9677, 3.3275, 3.0158, 2.8678,\n",
      "         3.1749, 3.3434, 3.5174, 3.4910, 3.6821, 3.6898, 3.7303, 3.8847, 3.5519,\n",
      "         3.9602]], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "Disc LogProbs False\n",
      "tensor([[-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., 0., 0.]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "gen loss\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -0., -0., -0., -0., -0.]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cum rewards\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "gen logprobs\n",
      "tensor([[-3.8147e-05, -5.1498e-05, -3.0518e-05, -1.1444e-05, -5.7220e-06,\n",
      "         -3.8147e-06, -3.8147e-06, -1.9073e-06, -1.9073e-06, -1.9073e-06,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "['<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/30][1280/11403]\tLoss_D: 1.5200\tLoss_G: 0.0000\tD(x): 2.7161\tD(G(z)): 0.0000 / 0.0000\n",
      "[0/30][1408/11403]\tLoss_D: 1.1352\tLoss_G: 0.0000\tD(x): 1.9956\tD(G(z)): 0.0000 / 0.0000\n",
      "['</s><s> use each\\nless than yom i will neither\\nspo my<pad></s>', '</s><s> use each\\nless than yom i will neither\\nspo my<pad></s>']\n",
      "Disc LogProbs True\n",
      "tensor([[2.4995, 2.8206, 2.6507, 3.0191, 3.1966, 3.3760, 3.4891, 3.7785, 3.8598,\n",
      "         3.9574, 4.0414, 4.0705, 4.0805, 4.1356, 4.1345, 4.0306, 4.0808, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "Disc LogProbs False\n",
      "tensor([[0., 0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., 0., 0., 0.]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "gen loss\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -0., -0., -0., -0.]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cum rewards\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "gen logprobs\n",
      "tensor([[-3.2425e-05, -4.1962e-05, -2.6703e-05, -9.5367e-06, -5.7220e-06,\n",
      "         -3.8147e-06, -1.9073e-06, -1.9073e-06, -1.9073e-06, -1.9073e-06,\n",
      "         -1.9073e-06,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "['<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>']\n",
      "[0/30][1536/11403]\tLoss_D: 0.6723\tLoss_G: 0.0000\tD(x): 2.4583\tD(G(z)): 0.0000 / 0.0000\n",
      "[0/30][1664/11403]\tLoss_D: 0.8640\tLoss_G: 0.0000\tD(x): 2.0552\tD(G(z)): 0.0000 / 0.0000\n",
      "Disc LogProbs True\n",
      "tensor([[2.0036, 1.2152, 1.9316, 2.6778, 3.3517, 3.5955, 3.7790, 3.8122, 3.8477,\n",
      "         3.9232, 3.9841, 4.0724, 4.0775, 4.0543, 4.0777, 4.1332, 3.9426, 4.0471,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000]], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "Disc LogProbs False\n",
      "tensor([[0., 0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., 0., 0., 0.]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "gen loss\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cum rewards\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "gen logprobs\n",
      "tensor([[-5.7220e-05, -4.5776e-05, -3.8147e-05, -1.5259e-05, -7.6294e-06,\n",
      "         -5.7220e-06, -5.7220e-06, -3.8147e-06, -3.8147e-06, -1.9073e-06,\n",
      "         -1.9073e-06, -1.9073e-06, -1.9073e-06, -1.9073e-06, -1.9073e-06]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "['<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>']\n",
      "[0/30][1792/11403]\tLoss_D: 0.6586\tLoss_G: 0.0000\tD(x): 2.0592\tD(G(z)): 0.0000 / 0.0000\n",
      "[0/30][1920/11403]\tLoss_D: 0.4952\tLoss_G: 0.0000\tD(x): 2.0676\tD(G(z)): 0.0000 / 0.0000\n",
      "['</s><s> use each\\nless than yom i will neither\\nspo my<pad></s>', '</s><s> use each\\nless than yom i will neither\\nspo my<pad></s>']\n",
      "saving geneartor\n",
      "Disc LogProbs True\n",
      "tensor([[2.9634, 1.7462, 3.3580, 3.1686, 3.5500, 3.6238, 3.8467, 3.8665, 4.0144,\n",
      "         4.0434, 4.1013, 4.1078, 4.1403, 4.1934, 4.2004, 4.2386, 4.2300, 4.1574,\n",
      "         4.1902, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000]], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "Disc LogProbs False\n",
      "tensor([[0., 0., 0., -0., -0., -0., -0., -0., -0., -0., -0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "gen loss\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., -0., -0., -0., -0., -0., -0.]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cum rewards\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "gen logprobs\n",
      "tensor([[-1.9073e-05, -3.6240e-05, -2.0981e-05, -7.6294e-06, -3.8147e-06,\n",
      "         -1.9073e-06, -1.9073e-06, -1.9073e-06, -1.9073e-06,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "['<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>']\n",
      "[0/30][2048/11403]\tLoss_D: 0.4884\tLoss_G: 0.0000\tD(x): 2.4000\tD(G(z)): 0.0000 / 0.0000\n",
      "[0/30][2176/11403]\tLoss_D: 0.4008\tLoss_G: 0.0000\tD(x): 3.4608\tD(G(z)): 0.0000 / 0.0000\n",
      "Disc LogProbs True\n",
      "tensor([[3.1044, 1.9969, 2.8380, 3.3019, 3.5809, 3.6893, 3.8492, 4.0013, 4.0248,\n",
      "         4.0702, 4.1098, 4.0114, 4.1610, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000]], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "Disc LogProbs False\n",
      "tensor([[0., 0., 0., -0., -0., -0., -0., -0., -0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "gen loss\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cum rewards\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "gen logprobs\n",
      "tensor([[-7.6294e-05, -7.2479e-05, -5.9128e-05, -1.9073e-05, -9.5367e-06,\n",
      "         -5.7220e-06, -3.8147e-06, -3.8147e-06, -1.9073e-06, -1.9073e-06,\n",
      "         -1.9073e-06, -1.9073e-06, -1.9073e-06, -1.9073e-06, -1.9073e-06]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "['<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>']\n",
      "[0/30][2304/11403]\tLoss_D: 0.4202\tLoss_G: 0.0000\tD(x): 3.4794\tD(G(z)): 0.0000 / 0.0000\n",
      "[0/30][2432/11403]\tLoss_D: 24.2287\tLoss_G: -0.5625\tD(x): 3.1573\tD(G(z)): 0.9020 / -0.5625\n",
      "['</s><s> useless\\nrapping my fertile\\njuice i neither yi\\ni fucking yi</s>', '</s><s> useless\\nrapping my fertile\\njuice neither i yi\\ni neither yi</s>']\n",
      "Disc LogProbs True\n",
      "tensor([[3.4571, 1.6548, 2.1517, 1.7615, 2.8744, 3.2493, 3.3366, 3.2413, 3.3755,\n",
      "         3.5325, 3.7559, 3.5405, 3.5516, 3.8209, 3.8408, 3.9247, 3.9348, 4.1951,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward>)\n",
      "Disc LogProbs False\n",
      "tensor([[ 1.5131,  2.9542,  1.1077,  1.9428,  1.2236,  1.7354,  1.5980,  1.5521,\n",
      "          0.0538, -1.2613, -1.4921, -1.3849, -1.6119, -1.9156, -1.7121]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "gen loss\n",
      "tensor([[ 1.6318,  7.4256,  6.4843,  5.7409,  1.6864,  1.0515,  0.8917,  0.8853,\n",
      "          0.2345, -4.4266, -9.1926, -1.9910, -0.9812, -7.6253, -0.9941]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cum rewards\n",
      "tensor([[ 0.6390,  1.1712,  0.6997,  1.0618,  0.7772,  0.9996,  0.9476,  0.9291,\n",
      "          0.0384, -0.7978, -0.9040, -0.8565, -0.9534, -1.0618, -0.9918]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "gen logprobs\n",
      "tensor([[ -2.5378,  -6.3187,  -9.2143,  -5.3865,  -2.1587,  -1.0478,  -0.9371,\n",
      "          -0.9488,  -5.5257,  -5.5762, -10.2140,  -2.3354,  -1.0335,  -7.2083,\n",
      "          -1.0064]], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "[' acneoy misdemeanorpretty acneuuuliterallyabortion pink acneu melancholyu']\n",
      "[0/30][2560/11403]\tLoss_D: 16.5054\tLoss_G: -0.0423\tD(x): 2.7493\tD(G(z)): -0.0010 / -0.0423\n",
      "[0/30][2688/11403]\tLoss_D: 25.4243\tLoss_G: 0.0893\tD(x): 2.5631\tD(G(z)): 1.1391 / 0.0893\n",
      "Disc LogProbs True\n",
      "tensor([[0.3345, 0.4529, 0.2428, 1.0875, 2.6927, 3.0481, 2.6597, 2.2872, 2.3794,\n",
      "         2.6589, 2.8366, 2.8862, 3.8090, 3.8913, 3.9746, 4.0674, 4.0435, 4.1100,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "Disc LogProbs False\n",
      "tensor([[ 0.7713,  0.9229,  0.3789,  0.1691,  0.2083, -0.0816, -0.0206,  0.0870,\n",
      "          0.0371,  0.1398,  0.0583, -0.5946, -0.2179, -1.2370, -0.6772]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "gen loss\n",
      "tensor([[ 0.9267,  1.1029,  0.4106,  0.5894,  2.7900,  0.2937,  0.2098,  0.2769,\n",
      "          0.3428,  0.3621,  2.1279, -1.1285,  0.0956, -2.6014, -0.2295]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cum rewards\n",
      "tensor([[ 0.3676,  0.5607,  0.2602,  0.1195,  0.1479, -0.0582, -0.0147,  0.0621,\n",
      "          0.0265,  0.0997,  0.0416, -0.4126, -0.1551, -0.7858, -0.4660]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "gen logprobs\n",
      "tensor([[-1.5045, -1.3632, -0.8074, -1.6021, -7.0414, -1.5448, -0.8980, -0.8918,\n",
      "         -1.2472, -1.0404, -7.3381, -6.8685, -1.0250, -4.8398, -1.0542]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "['less cuteless cutepretty cute acne acne cute acnesorrylessness cuteu cute']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/30][2816/11403]\tLoss_D: 12.0616\tLoss_G: 0.0580\tD(x): 1.4555\tD(G(z)): -0.4115 / 0.0580\n",
      "[0/30][2944/11403]\tLoss_D: 9.7987\tLoss_G: -0.0087\tD(x): 2.3776\tD(G(z)): -0.8134 / -0.0087\n",
      "['</s><s> useless\\nless power neither i\\nand neither i y</s><pad><pad>', '</s><s> useless\\nless power neither i\\nand neither i y</s><pad><pad>']\n",
      "saving geneartor\n",
      "Disc LogProbs True\n",
      "tensor([[1.8828, 1.6833, 2.2605, 3.0096, 3.4158, 3.5884, 3.7006, 3.7374, 3.8502,\n",
      "         3.8945, 3.9311, 3.9304, 3.6714, 3.6935, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000]], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "Disc LogProbs False\n",
      "tensor([[ 1.0998,  0.4729,  0.1700, -0.1543, -1.2760, -1.7594, -2.0341, -2.2801,\n",
      "         -2.4399, -2.4882, -2.6513, -2.5967, -2.6213, -2.7269, -2.8766]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "gen loss\n",
      "tensor([[ 1.6438,  0.4853,  1.0187,  0.7222,  0.0333, -0.2469, -0.3325, -0.3880,\n",
      "         -0.4993, -0.5086, -0.6201, -3.4728, -0.4312, -0.4304, -1.1457]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cum rewards\n",
      "tensor([[ 0.5005,  0.3018,  0.1179, -0.1091, -0.8031, -1.0082, -1.0979, -1.1634,\n",
      "         -1.1995, -1.2095, -1.2403, -1.2304, -1.2349, -1.2531, -1.2762]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "gen logprobs\n",
      "tensor([[-1.2390, -0.4303, -1.0790, -1.0071, -1.4358, -1.3568, -1.2241, -1.1509,\n",
      "         -1.3379, -1.3273, -1.4978, -8.5926, -1.0551, -1.0083, -2.5462]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "[' adorableless acne cute adorable adorable acne cute cute acne acne gorgeous adorable adorableless']\n",
      "[0/30][3072/11403]\tLoss_D: 5.6559\tLoss_G: -0.2537\tD(x): 2.7544\tD(G(z)): -1.7128 / -0.2537\n",
      "[0/30][3200/11403]\tLoss_D: 6.7820\tLoss_G: -0.1058\tD(x): 1.4292\tD(G(z)): -2.3488 / -0.1058\n",
      "Disc LogProbs True\n",
      "tensor([[ 1.8238,  0.4296,  0.7470,  0.1623, -0.9193, -1.0803, -1.4106, -1.1217,\n",
      "         -0.8276, -0.2913, -0.1963,  2.6540,  2.7530,  2.6536,  2.9876,  3.0844,\n",
      "          3.1312,  3.0450,  2.5981,  3.6166,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward>)\n",
      "Disc LogProbs False\n",
      "tensor([[-0.2324,  0.2212,  1.3684, -0.5215, -2.2199, -1.9972, -0.4768, -1.4561,\n",
      "         -0.3341,  0.0863, -0.8291,  0.3493,  0.0223, -0.7158, -0.5958]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "gen loss\n",
      "tensor([[ 1.7878e+00,  1.1979e-02,  1.0767e+00,  1.5246e-02, -3.6422e+00,\n",
      "         -1.4808e-02,  2.3222e-01, -1.9430e-01,  1.0711e-03,  1.3548e-01,\n",
      "         -4.6429e-02,  5.9968e-02,  1.5925e-02, -1.2321e-01, -2.0447e-04]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cum rewards\n",
      "tensor([[-0.1157,  0.1432,  0.8260, -0.3613, -1.1459, -1.0864, -0.3342, -0.8883,\n",
      "         -0.2365,  0.0616, -0.5605,  0.2470,  0.0159, -0.4905, -0.4134]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "gen logprobs\n",
      "tensor([[-6.1280, -0.0218, -0.8729, -0.3307, -4.9323, -0.0218, -3.1707, -0.4040,\n",
      "         -0.0063, -0.2888, -0.3034, -0.0916, -0.0376, -1.4832, -0.0343]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "[' acne\\nthe cute acne\\nless cute\\nju cute\\nju\\nju']\n",
      "[0/30][3328/11403]\tLoss_D: 10.7140\tLoss_G: -0.1054\tD(x): 1.7857\tD(G(z)): -0.7265 / -0.1054\n",
      "[0/30][3456/11403]\tLoss_D: 4.4031\tLoss_G: -0.0054\tD(x): 1.9322\tD(G(z)): 0.0426 / -0.0054\n",
      "['</s><s> useless\\nless than the power\\nof my cle</s><pad><pad>', '</s><s> useless\\nless than the power\\nof my cle</s><pad><pad>']\n",
      "Disc LogProbs True\n",
      "tensor([[2.3537, 3.4346, 2.0518, 2.6388, 2.7269, 3.5848, 3.6935, 3.7528, 3.7727,\n",
      "         3.7710, 3.7942, 3.8371, 3.9233, 3.9504, 4.0404, 4.0610, 3.9692, 4.1218,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward>)\n",
      "Disc LogProbs False\n",
      "tensor([[ 0.0497, -0.2710, -0.2793, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "gen loss\n",
      "tensor([[ 0.0076, -0.0547, -0.0542, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cum rewards\n",
      "tensor([[ 0.0248, -0.1751, -0.1928,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "gen logprobs\n",
      "tensor([[-4.3993e-01, -2.9956e-01, -2.7053e-01, -1.3597e-01, -5.7240e-03,\n",
      "         -5.8556e-04, -1.9836e-04, -8.5831e-05, -2.2888e-05, -9.5367e-06,\n",
      "         -5.7220e-06, -5.7220e-06, -5.7220e-06, -3.8147e-06, -3.8147e-06]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "['uuu<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>']\n",
      "[0/30][3584/11403]\tLoss_D: 3.6963\tLoss_G: 0.0141\tD(x): 2.3044\tD(G(z)): 0.0142 / 0.0141\n",
      "[0/30][3712/11403]\tLoss_D: 3.3700\tLoss_G: -0.0064\tD(x): 2.1607\tD(G(z)): 0.0057 / -0.0064\n",
      "Disc LogProbs True\n",
      "tensor([[1.6824, 1.8972, 1.8556, 2.9011, 3.3052, 3.5330, 3.6509, 3.7710, 3.9049,\n",
      "         3.9637, 4.0327, 4.0675, 4.0985, 4.1701, 4.1979, 4.2203, 4.1446, 4.1867,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward>)\n",
      "Disc LogProbs False\n",
      "tensor([[-0.2279, -0.4693, -0.3709, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "gen loss\n",
      "tensor([[-0.3279, -0.0025, -0.0725, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cum rewards\n",
      "tensor([[-0.1135, -0.2995, -0.2549,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "gen logprobs\n",
      "tensor([[-2.1075e+00, -7.2460e-03, -2.4407e-01, -8.7130e-02, -6.8188e-03,\n",
      "         -6.0081e-04, -1.4877e-04, -5.7220e-05, -1.1444e-05, -3.8147e-06,\n",
      "         -1.9073e-06, -1.9073e-06, -1.9073e-06, -1.9073e-06, -1.9073e-06]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "['uuu<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>']\n",
      "[0/30][3840/11403]\tLoss_D: 3.8143\tLoss_G: -0.0072\tD(x): 1.9043\tD(G(z)): 0.0645 / -0.0072\n",
      "[0/30][3968/11403]\tLoss_D: 0.9851\tLoss_G: 0.0000\tD(x): 1.7913\tD(G(z)): 0.0000 / 0.0000\n",
      "['</s><s> useless\\nless than the power\\nof my cle<pad>i</s><pad><pad>', '</s><s> wow i forgot\\ni used to suck the power\\nof my cle</s><pad>']\n",
      "saving geneartor\n",
      "Disc LogProbs True\n",
      "tensor([[1.4495, 1.6836, 2.3790, 3.3571, 3.4735, 3.6470, 3.7453, 3.8151, 3.9424,\n",
      "         3.9789, 4.0262, 4.1027, 4.1691, 4.1296, 4.1819, 4.2206, 4.2327, 4.1307,\n",
      "         4.1476]], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "Disc LogProbs False\n",
      "tensor([[-0., 0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., 0., 0., 0.]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "gen loss\n",
      "tensor([[0., 0., 0., 0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cum rewards\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "gen logprobs\n",
      "tensor([[-5.5313e-05, -2.4014e-03, -1.3351e-05, -1.9073e-06,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "['<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>']\n",
      "[0/30][4096/11403]\tLoss_D: 0.7366\tLoss_G: 0.0000\tD(x): 3.1183\tD(G(z)): 0.0000 / 0.0000\n",
      "[0/30][4224/11403]\tLoss_D: 15.5080\tLoss_G: -0.2182\tD(x): 2.9349\tD(G(z)): 0.0347 / -0.2182\n",
      "Disc LogProbs True\n",
      "tensor([[3.6468, 1.5802, 1.8845, 2.7495, 2.7360, 3.3893, 3.5441, 3.7983, 3.5328,\n",
      "         3.6145, 3.6502, 3.4854, 3.8280, 3.6453, 3.8915, 3.9854, 4.1329, 3.8932,\n",
      "         4.0713, 0.0000]], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "Disc LogProbs False\n",
      "tensor([[-1.3611, -1.6855, -1.9288, -1.6090, -1.9228, -1.4836, -0.8455, -1.3079,\n",
      "         -2.0725, -2.3226, -2.1596, -2.4701, -2.0206, -2.1594, -1.9615]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "gen loss\n",
      "tensor([[-0.2318, -0.3311, -2.7056, -2.1391, -3.9165, -0.8696, -0.1317, -0.7000,\n",
      "         -0.1877, -5.0067, -1.5037, -0.3180, -0.6975, -0.5585, -0.2407]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cum rewards\n",
      "tensor([[-0.5919, -0.8934, -1.0373, -0.9445, -1.0616, -0.8997, -0.5703, -0.8204,\n",
      "         -1.1091, -1.1735, -1.1330, -1.2058, -1.0941, -1.1330, -1.0763]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "gen logprobs\n",
      "tensor([[-0.9900, -0.6181, -3.9818, -3.6457, -5.5648, -1.6048, -0.6197, -1.5131,\n",
      "         -0.2498, -6.1375, -1.9396, -0.3750, -0.9473, -0.7204, -0.3350]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "['\\n\\nnot your year will\\nnever be years so\\nwinter will be']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/30][4352/11403]\tLoss_D: 13.7675\tLoss_G: -0.3609\tD(x): 2.6887\tD(G(z)): -0.8959 / -0.3609\n",
      "[0/30][4480/11403]\tLoss_D: 28.7499\tLoss_G: 0.5592\tD(x): 0.9064\tD(G(z)): 0.0607 / 0.5592\n",
      "['</s><s> use your power\\nto study but never use\\nit for research</s><pad><pad><pad>', '</s><s> used to suck my\\njuice but i forgot to\\nuse my power</s><pad>']\n",
      "Disc LogProbs True\n",
      "tensor([[ 0.9649,  0.0750, -0.9509,  1.6573,  0.5589, -0.0456, -0.1080,  0.0758,\n",
      "         -0.3541, -0.9003, -1.0902, -1.3983, -1.7294, -1.6264, -1.4490,  0.5767,\n",
      "          3.3696,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,  0.0000]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "Disc LogProbs False\n",
      "tensor([[ 1.1274, -1.4131, -0.8931,  0.5276,  1.2461, -1.4977, -0.9765, -1.1273,\n",
      "         -1.3658, -2.6076, -2.2842, -2.3780, -2.5938, -2.7388, -2.3996]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "gen loss\n",
      "tensor([[ 0.7272, -1.0239, -4.2683,  2.6507,  1.4302, -0.0809, -0.1780, -4.8309,\n",
      "         -2.6853, -2.5994, -0.0620, -0.8959, -0.1956, -1.1045, -0.4785]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cum rewards\n",
      "tensor([[ 0.5107, -0.7910, -0.5825,  0.3654,  0.7884, -0.9057, -0.6468, -0.7295,\n",
      "         -0.8477, -1.2324, -1.1645, -1.1861, -1.2299, -1.2551, -1.1908]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "gen logprobs\n",
      "tensor([[ -1.0322,  -1.7144, -10.9807,  -4.7405,  -1.4561,  -0.1137,  -0.3930,\n",
      "          -9.0179,  -4.1065,  -2.5028,  -0.0638,  -0.9028,  -0.1888,  -1.0407,\n",
      "          -0.4799]], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "[' the the passionof never\\nwinter weather always is\\nwinter never\\nwinter']\n",
      "[0/30][4608/11403]\tLoss_D: 30.8441\tLoss_G: -0.1240\tD(x): 0.1601\tD(G(z)): -0.3489 / -0.1240\n",
      "[0/30][4736/11403]\tLoss_D: 18.2082\tLoss_G: -0.1812\tD(x): 1.0541\tD(G(z)): -0.7004 / -0.1812\n",
      "Disc LogProbs True\n",
      "tensor([[ 0.9575,  0.2276,  1.4613,  2.0430,  1.8865,  2.2819,  2.1768,  2.2633,\n",
      "          2.2807, -0.7913, -0.8252, -1.3282, -0.6936,  0.7256,  2.9201,  0.0000,\n",
      "          0.0000,  0.0000]], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "Disc LogProbs False\n",
      "tensor([[ 0.2379,  0.2463, -0.5508,  0.0086, -1.9415, -1.9072, -1.5145, -1.5809,\n",
      "         -0.5146, -1.1078, -1.3339, -0.9910, -0.0275,  0.3804,  0.7232]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "gen loss\n",
      "tensor([[ 0.0919,  0.0241,  0.1490,  0.4660, -0.0468, -0.6414, -2.6266, -0.8669,\n",
      "          0.2711, -0.5385, -0.0147, -1.2411,  0.3210,  1.1835,  1.7445]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cum rewards\n",
      "tensor([[ 0.1184,  0.1593, -0.3734,  0.0061, -1.0674, -1.0584, -0.9133, -0.9409,\n",
      "         -0.3597, -0.7192, -0.8328, -0.6551, -0.0196,  0.2685,  0.4952]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "gen logprobs\n",
      "tensor([[-0.1785, -0.0434, -6.4840, -1.1578, -0.0698, -0.9688, -5.0811, -1.5921,\n",
      "         -7.3885, -1.6679, -0.0337, -4.7961, -0.8521, -1.7800, -1.9567]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "[' my love withless\\nonly being days darkness on\\nsum day day will']\n",
      "[0/30][4864/11403]\tLoss_D: 18.7763\tLoss_G: -0.1391\tD(x): 0.7329\tD(G(z)): -0.6357 / -0.1391\n",
      "[0/30][4992/11403]\tLoss_D: 11.3715\tLoss_G: -0.0949\tD(x): 1.7078\tD(G(z)): -1.0359 / -0.0949\n",
      "['</s><s> useless day\\nto study the power\\nof my power</s><pad><pad><pad><pad>', '</s><s> used to suck on\\nthe power of my clement\\ni love it</s><pad>']\n",
      "saving geneartor\n",
      "Disc LogProbs True\n",
      "tensor([[ 1.3151,  1.4228,  1.6150,  1.1822,  2.4965,  1.6685,  2.0033,  1.3242,\n",
      "          0.6477, -1.0413, -0.7341, -1.2016, -0.6166, -0.0742,  2.0537,  3.6463,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward>)\n",
      "Disc LogProbs False\n",
      "tensor([[-0.8697,  0.6534, -0.9151,  0.1054, -1.3744, -1.8441, -1.6214, -1.6599,\n",
      "         -1.6653, -1.9788, -1.3477, -0.9597, -1.4070, -0.6377,  0.4519]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "gen loss\n",
      "tensor([[-0.1799,  0.1690, -0.2413,  0.3039, -0.9020, -0.5778, -4.3021, -0.4908,\n",
      "         -1.1637, -5.1187, -0.0639, -0.8317, -1.0441, -0.0505,  4.2015]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cum rewards\n",
      "tensor([[-0.4094,  0.4102, -0.5951,  0.0746, -0.8496, -1.0376, -0.9569, -0.9720,\n",
      "         -0.9741, -1.0816, -0.8393, -0.6373, -0.8665, -0.4407,  0.3174]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "gen logprobs\n",
      "tensor([[-0.9664, -0.2668, -0.6488, -1.0206, -1.4398, -0.7094, -5.8631, -0.6554,\n",
      "         -1.5497, -5.9629, -0.1036, -2.0082, -1.6228, -0.2322, -7.7721]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "[' my her skin on skin\\nshe is the belly of the\\nwinter hair']\n",
      "[0/30][5120/11403]\tLoss_D: 24.5185\tLoss_G: 0.1748\tD(x): 0.5637\tD(G(z)): -0.3411 / 0.1748\n",
      "[0/30][5248/11403]\tLoss_D: 29.6359\tLoss_G: -0.1732\tD(x): -0.1428\tD(G(z)): -0.1991 / -0.1732\n",
      "Disc LogProbs True\n",
      "tensor([[0.5976, 1.9277, 2.1491, 2.5995, 3.2693, 3.0385, 3.2263, 3.3545, 3.3461,\n",
      "         3.4411, 3.4978, 3.4974, 3.4210, 3.4954, 3.5341, 3.3217, 3.8431, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "Disc LogProbs False\n",
      "tensor([[-0.5212,  0.1854, -0.0131, -0.2845, -0.8157, -0.9334, -0.3336, -0.6416,\n",
      "          0.5160, -0.1481, -0.4733, -0.5423, -0.3545, -1.3131, -1.3160]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "gen loss\n",
      "tensor([[ 4.1574e-04,  1.6767e-02,  2.8302e-02,  2.4237e-03, -3.5043e-02,\n",
      "         -7.8220e-02,  5.4968e-03, -1.9080e-02,  2.1394e+00,  1.4848e-01,\n",
      "         -1.0607e-02, -7.4730e-02,  9.3034e-02, -4.6693e-01, -5.5480e-02]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cum rewards\n",
      "tensor([[-0.2549,  0.1201, -0.0091, -0.2002, -0.5510, -0.6217, -0.2360, -0.4432,\n",
      "          0.3606, -0.1056, -0.3319, -0.3781, -0.2506, -0.8230, -0.8243]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "gen logprobs\n",
      "tensor([[-0.0336, -0.0433, -0.1096, -0.0362, -0.1235, -0.2206, -0.1762, -0.1084,\n",
      "         -3.4074, -0.9186, -0.1640, -0.6738, -5.6015, -0.8402, -0.0996]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "[' a body with the body of the body used the body of just the body']\n",
      "[0/30][5376/11403]\tLoss_D: 13.0066\tLoss_G: -0.0563\tD(x): 1.3137\tD(G(z)): -0.4244 / -0.0563\n",
      "['</s><s> useless day\\nto study the power of\\nless than a week</s><pad><pad>', '</s><s> used to use\\nless than a week to study\\nand i forgot</s><pad><pad>']\n",
      "[0/30][5504/11403]\tLoss_D: 14.7076\tLoss_G: -0.2305\tD(x): 0.9456\tD(G(z)): -0.9974 / -0.2305\n",
      "Disc LogProbs True\n",
      "tensor([[1.1712, 0.2466, 0.1197, 0.4280, 2.2742, 2.3932, 2.7687, 2.6792, 2.6118,\n",
      "         2.6213, 2.8781, 3.0114, 3.1605, 3.3832, 3.6265, 3.6684, 3.5919, 3.9357,\n",
      "         0.0000]], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "Disc LogProbs False\n",
      "tensor([[-0.4577, -0.2653, -0.7690,  0.0000,  0.0000,  0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "gen loss\n",
      "tensor([[-0.0078, -0.0146, -0.0039,  0.0000,  0.0000, -0.0000,  0.0000,  0.0000,\n",
      "          0.0000, -0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cum rewards\n",
      "tensor([[-0.2249, -0.1714, -0.5096,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "gen logprobs\n",
      "tensor([[-4.6477e-02, -1.2852e-01, -8.6136e-03, -7.2165e-03, -1.9073e-06,\n",
      "          0.0000e+00, -1.9073e-06, -1.9073e-06, -1.9073e-06,  0.0000e+00,\n",
      "          0.0000e+00, -1.9073e-06, -1.9073e-06, -5.7220e-06, -1.9073e-06]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "[' y useless<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>']\n",
      "[0/30][5632/11403]\tLoss_D: 6.6024\tLoss_G: -0.0177\tD(x): 1.8565\tD(G(z)): -0.0976 / -0.0177\n",
      "[0/30][5760/11403]\tLoss_D: 10.3388\tLoss_G: 0.0381\tD(x): 1.4979\tD(G(z)): -0.0654 / 0.0381\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disc LogProbs True\n",
      "tensor([[-0.4232,  0.2895,  1.5050,  1.0786,  2.9776,  3.2056,  3.2262,  3.0468,\n",
      "          3.0888,  2.8100,  3.0642,  2.9066,  2.9773,  3.5387,  3.5956,  3.6615,\n",
      "          3.7101,  3.8644,  3.5356,  3.5902,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward>)\n",
      "Disc LogProbs False\n",
      "tensor([[-0.0000, -0.3649,  0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,  0.0000]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "gen loss\n",
      "tensor([[ 0.0000, -0.0088,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cum rewards\n",
      "tensor([[ 0.0000, -0.2346,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "gen logprobs\n",
      "tensor([[-1.6594e-03, -4.0219e-02, -1.9073e-05, -1.9073e-06, -1.9073e-06,\n",
      "         -1.9073e-06, -1.9073e-06, -1.9073e-06,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "['<pad> body<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>']\n",
      "[0/30][5888/11403]\tLoss_D: 3.9537\tLoss_G: -0.0004\tD(x): 1.7740\tD(G(z)): -0.0250 / -0.0004\n",
      "['</s><s> useless\\nmornings to study the\\npower of my day</s><pad><pad>', '</s><s> used to suck\\njuice but i forgot to\\nuse it today</s><pad>']\n",
      "saving geneartor\n",
      "[0/30][6016/11403]\tLoss_D: 2.7314\tLoss_G: -0.0002\tD(x): 2.4308\tD(G(z)): -0.0203 / -0.0002\n",
      "Disc LogProbs True\n",
      "tensor([[0.4179, 1.6322, 2.4966, 1.2461, 1.1076, 2.7546, 2.5790, 3.1241, 2.7240,\n",
      "         2.1409, 1.8704, 2.7362, 2.6568, 3.3273, 3.4017, 3.6227, 3.6308, 4.0189,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward>)\n",
      "Disc LogProbs False\n",
      "tensor([[-0.0000, -0.4049,  0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "gen loss\n",
      "tensor([[ 0.0000, -0.0015,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cum rewards\n",
      "tensor([[ 0.0000, -0.2596,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "gen logprobs\n",
      "tensor([[-1.0891e-03, -6.2046e-03, -2.0981e-05, -1.9073e-06, -1.9073e-06,\n",
      "         -1.9073e-06, -1.9073e-06,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "['<pad> body<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>']\n",
      "[0/30][6144/11403]\tLoss_D: 2.5580\tLoss_G: 0.0057\tD(x): 1.7827\tD(G(z)): -0.0247 / 0.0057\n",
      "[0/30][6272/11403]\tLoss_D: 1.0388\tLoss_G: 0.0000\tD(x): 2.3486\tD(G(z)): 0.0000 / 0.0000\n",
      "Disc LogProbs True\n",
      "tensor([[3.6841, 3.7275, 2.6895, 3.2967, 3.6122, 3.7535, 3.8733, 3.9766, 4.0276,\n",
      "         4.0661, 4.1236, 4.1236, 4.1236, 4.1437, 4.2282, 4.1140, 4.1815, 0.0000]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "Disc LogProbs False\n",
      "tensor([[0., 0., 0., -0., -0., -0., -0., -0., -0., -0., -0., -0., 0., 0., 0.]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "gen loss\n",
      "tensor([[0., 0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cum rewards\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "gen logprobs\n",
      "tensor([[-5.7220e-06, -1.5259e-05,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "['<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>']\n",
      "[0/30][6400/11403]\tLoss_D: 0.5737\tLoss_G: 0.0000\tD(x): 3.3419\tD(G(z)): 0.0000 / 0.0000\n",
      "['</s><s> useless\\nmonday to study and\\nspo each day</s><pad>', '</s><s> use your\\nless than a day to study\\nand return it</s><pad>']\n",
      "[0/30][6528/11403]\tLoss_D: 0.6769\tLoss_G: 0.0000\tD(x): 2.3353\tD(G(z)): 0.0000 / 0.0000\n",
      "Disc LogProbs True\n",
      "tensor([[4.0399, 2.0568, 2.5414, 3.3947, 3.5949, 3.7382, 3.8457, 3.9913, 4.0538,\n",
      "         4.0835, 4.1889, 4.2282, 4.2373, 4.2533, 4.1859, 4.2719, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "Disc LogProbs False\n",
      "tensor([[0., 0., 0., -0., -0., -0., -0., -0., -0., -0., -0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "gen loss\n",
      "tensor([[0., 0., 0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cum rewards\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "gen logprobs\n",
      "tensor([[-7.6294e-06, -9.5367e-06, -1.9073e-06,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "['<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>']\n",
      "[0/30][6656/11403]\tLoss_D: 0.5118\tLoss_G: 0.0000\tD(x): 1.9933\tD(G(z)): 0.0000 / 0.0000\n",
      "[0/30][6784/11403]\tLoss_D: 0.4651\tLoss_G: 0.0000\tD(x): 3.4214\tD(G(z)): 0.0000 / 0.0000\n",
      "Disc LogProbs True\n",
      "tensor([[4.1428, 3.9347, 2.9807, 3.2977, 3.6094, 3.8065, 3.8991, 3.9871, 4.0313,\n",
      "         4.1081, 4.1736, 4.2190, 4.2462, 4.2462, 4.2533, 4.3040, 4.2316, 4.2819,\n",
      "         0.0000, 0.0000, 0.0000]], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "Disc LogProbs False\n",
      "tensor([[0., 0., 0., 0., -0., -0., -0., -0., -0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "gen loss\n",
      "tensor([[0., 0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cum rewards\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "gen logprobs\n",
      "tensor([[-5.7220e-06, -1.9073e-05,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "['<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>']\n",
      "[0/30][6912/11403]\tLoss_D: 0.3982\tLoss_G: 0.0000\tD(x): 3.2496\tD(G(z)): 0.0000 / 0.0000\n",
      "['</s><s> useless\\nmonday to study and\\nspo each day</s><pad>', '</s><s> use your\\nless than a day to study\\nand return it</s><pad>']\n",
      "saving geneartor\n",
      "[0/30][7040/11403]\tLoss_D: 0.3467\tLoss_G: 0.0000\tD(x): 3.6876\tD(G(z)): 0.0000 / 0.0000\n",
      "Disc LogProbs True\n",
      "tensor([[3.9993, 1.9462, 2.6412, 3.0880, 3.6837, 3.8316, 3.9388, 3.9798, 4.0722,\n",
      "         4.1218, 4.2008, 4.2409, 4.2560, 4.2464, 4.3298, 4.3220, 4.3640, 4.3522,\n",
      "         4.3706, 4.3438, 4.3097]], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "Disc LogProbs False\n",
      "tensor([[0., 0., 0., 0., 0., -0., -0., -0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "gen loss\n",
      "tensor([[0., 0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cum rewards\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "gen logprobs\n",
      "tensor([[-7.6294e-06, -7.6294e-06,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "['<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/30][7168/11403]\tLoss_D: 0.3680\tLoss_G: 0.0000\tD(x): 3.4282\tD(G(z)): 0.0000 / 0.0000\n",
      "[0/30][7296/11403]\tLoss_D: 0.3250\tLoss_G: 0.0000\tD(x): 3.6913\tD(G(z)): 0.0000 / 0.0000\n",
      "Disc LogProbs True\n",
      "tensor([[4.2690, 2.0657, 3.1312, 3.3598, 3.5582, 3.7066, 3.9482, 4.0602, 4.1276,\n",
      "         4.1919, 4.2758, 4.2900, 4.3576, 4.3440, 4.3543, 4.3664, 4.3652, 4.4150,\n",
      "         4.3587, 4.3027, 4.3372, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "Disc LogProbs False\n",
      "tensor([[0., 0., 0., 0., 0., 0., -0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "gen loss\n",
      "tensor([[0., 0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cum rewards\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "gen logprobs\n",
      "tensor([[-5.7220e-06, -5.7220e-06,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "['<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>']\n",
      "[0/30][7424/11403]\tLoss_D: 0.3734\tLoss_G: 0.0000\tD(x): 3.1583\tD(G(z)): 0.0000 / 0.0000\n",
      "['</s><s> useless\\nmonday to study and\\nspo each day</s><pad>', '</s><s> use your\\nless than a day to study\\nand return it</s><pad>']\n",
      "[0/30][7552/11403]\tLoss_D: 0.2973\tLoss_G: 0.0000\tD(x): 3.7606\tD(G(z)): 0.0000 / 0.0000\n",
      "Disc LogProbs True\n",
      "tensor([[4.4733, 4.4329, 2.9851, 3.4168, 3.6830, 3.9071, 4.0116, 4.1024, 4.1285,\n",
      "         4.2096, 4.2576, 4.2547, 4.2911, 4.3680, 4.3338, 4.3860, 4.3886, 4.3841,\n",
      "         4.3359, 4.3552, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "Disc LogProbs False\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "gen loss\n",
      "tensor([[0., 0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cum rewards\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "gen logprobs\n",
      "tensor([[-5.7220e-06, -5.7220e-06,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "['<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>']\n",
      "[0/30][7680/11403]\tLoss_D: 0.3450\tLoss_G: 0.0000\tD(x): 2.1819\tD(G(z)): 0.0000 / 0.0000\n",
      "[0/30][7808/11403]\tLoss_D: 0.2705\tLoss_G: 0.0000\tD(x): 3.8058\tD(G(z)): 0.0000 / 0.0000\n",
      "Disc LogProbs True\n",
      "tensor([[4.3294, 4.3891, 4.5307, 3.5012, 3.7888, 3.9126, 4.0517, 4.0916, 4.2264,\n",
      "         4.2471, 4.3034, 4.3134, 4.3645, 4.4131, 4.4216, 4.4094, 4.4539, 4.4287,\n",
      "         4.4461, 4.4416, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000]], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "Disc LogProbs False\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "gen loss\n",
      "tensor([[0., 0., 0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cum rewards\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "gen logprobs\n",
      "tensor([[-7.6294e-06, -5.7220e-06, -1.9073e-06,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "['<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>']\n",
      "[0/30][7936/11403]\tLoss_D: 0.2794\tLoss_G: 0.0000\tD(x): 2.8469\tD(G(z)): 0.0000 / 0.0000\n",
      "['</s><s> useless\\nmonday to study and\\nspo each day</s><pad>', '</s><s> use your\\nless than a day to study\\nand return it</s><pad>']\n",
      "saving geneartor\n",
      "[0/30][8064/11403]\tLoss_D: 0.2732\tLoss_G: 0.0000\tD(x): 2.7194\tD(G(z)): 0.0000 / 0.0000\n",
      "Disc LogProbs True\n",
      "tensor([[4.4074, 4.5299, 4.4442, 3.5439, 3.8147, 3.9375, 4.0116, 4.0723, 4.0919,\n",
      "         4.2481, 4.3057, 4.3801, 4.4083, 4.3565, 4.4168, 4.4364, 4.4681, 4.4686,\n",
      "         4.4981, 4.5115, 4.4856, 4.4518]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward>)\n",
      "Disc LogProbs False\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "gen loss\n",
      "tensor([[0., 0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cum rewards\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "gen logprobs\n",
      "tensor([[-5.7220e-06, -7.6294e-06,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "['<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>']\n",
      "[0/30][8192/11403]\tLoss_D: 0.2476\tLoss_G: 0.0000\tD(x): 3.4863\tD(G(z)): 0.0000 / 0.0000\n",
      "[0/30][8320/11403]\tLoss_D: 0.2205\tLoss_G: 0.0000\tD(x): 3.1608\tD(G(z)): 0.0000 / 0.0000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-5745ad09d2a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0;31m#errG = loss.detach()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;31m# Calculate gradients for G\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m         \u001b[0mlossG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0mD_G_z2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlossG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0;31m# Update G\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \"\"\"\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    126\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#this code has been heavily modified, but based on https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html\n",
    "#not copies though so id \n",
    "# Training Loop\n",
    "device = 'cuda'\n",
    "\n",
    "# Lists to keep track of progress\n",
    "img_list = []\n",
    "G_losses = []\n",
    "D_losses = []\n",
    "iters = 0\n",
    "num_epochs = 30\n",
    "\n",
    "train_dl = DataLoader(train_ds,batch_size=bsize,num_workers=2)\n",
    "print(\"Starting Training Loop...\")\n",
    "# For each epoch\n",
    "for epoch in range(num_epochs):\n",
    "    print(\"Label Noise:\",0.05)\n",
    "    # For each batch in the dataloader\n",
    "    for i, data in enumerate(train_dl, 0):\n",
    "       # print(i)\n",
    "        \n",
    "        if len(data['input_ids']) < bsize:\n",
    "            continue\n",
    "       # print(data)\n",
    "        ############################\n",
    "        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "        ###########################\n",
    "        ## Train with all-real batch\n",
    "        \n",
    "        # Format batch\n",
    "        real_inputs = data['input_ids'].to(device)\n",
    "        real_input_attention = data['attention_mask'].to(device)\n",
    "        real_input_test = {'input_ids':real_inputs,'attention_mask':real_input_attention}\n",
    "        \n",
    "        \n",
    "        real_gpu = data['label_ids'].to(device)\n",
    "        attention_gpu = data['decoder_attention_mask'].to(device)\n",
    "        b_size = real_gpu.size(0)\n",
    "        #\n",
    "        label = torch.full((b_size,), 1.0 , dtype=torch.float, device=device) #.95 instead of real to inject noise\n",
    "        label = torch.bernoulli(label)\n",
    "       # print(label)\n",
    "        # Forward pass real batch through D\n",
    "        real_mask = get_valid_mask(real_gpu)\n",
    "\n",
    "        output_real,loss_real = discriminator_train(real_gpu[:,1:],label,criterion,mask=real_mask)\n",
    "\n",
    "        loss_real.backward()\n",
    "\n",
    "        #del label\n",
    "        #output = discriminator(real_gpu,attention_mask=attention_gpu).view(-1)\n",
    "        # Calculate loss on all-real batch\n",
    "        #errD_real = criterion(output, label)\n",
    "        # Calculate gradients for D in backward pass\n",
    "        #errD_real.backward()\n",
    "        D_x = output_real.detach().mean().item()\n",
    "        #del output_real\n",
    "        ## Train with all-fake batch\n",
    "        # Generate batch of latent vectors\n",
    "        noise = generate_random_input(bsize,device)#torch.randn(b_size, nz, 1, 1, device=device)\n",
    "        # Generate fake image batch with G\n",
    "        #print(fake)\n",
    "       # print(noise)\n",
    "        if random.random() > .5:\n",
    "\n",
    "            fake,fake_logprobs = generator(noise)\n",
    "        else:\n",
    "\n",
    "            #sometimes the discirminator learns words in real input, this way some will be based on these same words\n",
    "            fake,fake_logprobs = generator(real_input_test)\n",
    "            #real_input_test\n",
    "  \n",
    "        \n",
    "        fake_mask = get_valid_mask(fake)\n",
    "        end_mask = get_end_mask(fake)\n",
    "  \n",
    "        #generate_sequence(generator,noise)\n",
    "       # print(len(fake_logprobs))\n",
    "        #a = 1/0\n",
    "        #fake = generator.generate_with_grad(noise['input_ids'].cuda(),attention_mask=noise[\"attention_mask\"].cuda()) #,max_length=128\n",
    "       # print(fake)\n",
    "        #print((fake.logits))\n",
    "        #fake = decode(fake.logits)\n",
    "      #  print(fake)\n",
    "        # I am injecting some false positives into the fake labels so the discriminator does a worse job\n",
    "        label.fill_(0.00) \n",
    "        label = torch.bernoulli(label)\n",
    "       # print(label)\n",
    "        # Classify all fake batch with D\n",
    "        output_fake,loss_fake = discriminator_train(fake,label,criterion,mask=fake_mask)\n",
    "\n",
    "        loss_fake.backward()\n",
    "       # print('fake')\n",
    "       # print(loss_fake.detach())\n",
    "        #output = discriminator(fake).view(-1)\n",
    "        # Calculate D's loss on the all-fake batch\n",
    "        #errD_fake = criterion(output, label)\n",
    "        # Calculate the gradients for this batch, accumulated (summed) with previous gradients\n",
    "       # errD_fake.backward()\n",
    "        D_G_z1 = output_fake.detach().cpu().mean().item()\n",
    "        # Compute error of D as sum over the fake and the real batches\n",
    "        errD = (torch.mean(loss_real.detach()) + torch.mean(loss_fake.detach())).cpu()\n",
    "       # del output_real\n",
    "        # Update D\n",
    "        optimizerD.step()\n",
    "        \n",
    "\n",
    "        ############################\n",
    "        # (2) Update G network: maximize log(D(G(z)))\n",
    "        ###########################\n",
    "        \n",
    "        #generator.zero_grad()\n",
    "\n",
    "        lossG, cumulative_rewards, ewma_reward = reinforce_loss(output_fake,fake_logprobs,.3,.08,epoch,fake_mask,end_mask)\n",
    "        if i % 256 == 0:\n",
    "            print('Disc LogProbs True')\n",
    "            print(output_real[0:1])\n",
    "            print('Disc LogProbs False')\n",
    "            print(output_fake[0:1])\n",
    "            print('gen loss')\n",
    "            print(lossG[0:1])\n",
    "            print('cum rewards')\n",
    "            print(cumulative_rewards[0:1])\n",
    "            print('gen logprobs')\n",
    "            print(fake_logprobs[0:1])\n",
    "            print(tokenizer.batch_decode(fake[0:1].cpu()))\n",
    "\n",
    "        #print(lossG)\n",
    "        lossG = torch.mean(lossG)\n",
    "       # print(lossG)\n",
    "        #print(loss)\n",
    "        #label.fill_(real_label)  # fake labels are real for generator cost\n",
    "        # Since we just updated D, perform another forward pass of all-fake batch through D\n",
    "       # print(fake)\n",
    "        #output = discriminator(fake).view(-1)\n",
    "        # Calculate G's loss based on this output\n",
    "        #errG = criterion(output, label)\n",
    "        #print(errG)\n",
    "        #errG = loss.detach()\n",
    "        # Calculate gradients for G\n",
    "        lossG.backward()\n",
    "        D_G_z2 = lossG.detach().cpu().mean().item()\n",
    "        # Update G\n",
    "        \n",
    "        optimizerG.step()\n",
    "        generator.zero_grad()\n",
    "        discriminator.zero_grad()\n",
    "        #a = 1/0\n",
    "        # Output training stats\n",
    "        if i % 128 == 0:\n",
    "            print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n",
    "                  % (epoch, num_epochs, i, len(train_dl),\n",
    "                     errD.item(), lossG.item(), D_x, D_G_z1, D_G_z2))\n",
    "          #  print('ewma_reward:',ewma_reward)\n",
    "\n",
    "        # Save Losses for plotting later\n",
    "        G_losses.append(lossG.detach().item())\n",
    "        D_losses.append(errD.detach().item())\n",
    "        \n",
    "        del lossG\n",
    "        del errD\n",
    "\n",
    "        # Check how the generator is doing by saving G's output on fixed_noise\n",
    "        if (iters % 500 == 0) or ((epoch == num_epochs-1) and (i == len(dataloader)-1)):\n",
    "            with torch.no_grad():\n",
    "                fake = tokenizer.batch_decode(generator.model.generate(fixed_noise['input_ids'],attention_mask=fixed_noise[\"attention_mask\"],max_length=32,num_beams= 50,early_stopping = True))\n",
    "                print(fake[0:2])\n",
    "        if (iters % 1000 == 0 ):\n",
    "            print('saving geneartor')\n",
    "            torch.save({\n",
    "            'epoch': 15,\n",
    "            'model_state_dict': generator.state_dict(),\n",
    "            'optimizer_state_dict': optimizerG.state_dict(),\n",
    "            'loss': 0.3355,\n",
    "            }, PATH + 'generator.pt')\n",
    "            torch.save({\n",
    "            'epoch': 15,\n",
    "            'model_state_dict': discriminator.state_dict(),\n",
    "            'optimizer_state_dict': optimizerD.state_dict(),\n",
    "            'loss': 0.3355,\n",
    "            }, PATH + 'discriminator.pt')\n",
    "            #img_list.append(vutils.make_grid(fake, padding=2, normalize=True))\n",
    "\n",
    "        iters += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "early poem\n",
    "\n",
    "> leaving later\\nin between the Court of justice\\nand the Wander in'\n",
    "\n",
    "' we what\\nwould destiny do if she\\ndied and did not live'\n",
    "\n",
    " everyone easter\\nthe patient waits for the light\\nto dim   (wish it was every...)\n",
    "\n",
    "sampling from latent space sometimes never really impvoes\n",
    "\n",
    " you should be writing\\nlike an old man in love with\\nan old man\n",
    "\n",
    "epoch after epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "            'epoch': 15,\n",
    "            'model_state_dict': generator.state_dict(),\n",
    "            'optimizer_state_dict': optimizerG.state_dict(),\n",
    "            'loss': 0.3355,\n",
    "            }, PATH + 'four_it_ft_generator_bad.pt')\n",
    "torch.save({\n",
    "            'epoch': 15,\n",
    "            'model_state_dict': discriminator.state_dict(),\n",
    "            'optimizer_state_dict': optimizerD.state_dict(),\n",
    "            'loss': 0.3355,\n",
    "            }, PATH + 'four_ft_discriminator_bad.pt')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(\"cats\", return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16 \n",
    "\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    \"BART-base\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    #learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "   # weight_decay=0.01,\n",
    "   # save_total_limit=3,\n",
    "   # num_train_epochs=1,\n",
    "   # predict_with_generate=True,\n",
    "    \n",
    "    learning_rate=1e-4,\n",
    "    weight_decay=0.01,\n",
    "    adam_beta1=0.9,\n",
    "    adam_beta2=0.999,\n",
    "    adam_epsilon=1e-6,\n",
    "    max_grad_norm=1.0,\n",
    "    num_train_epochs=10,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    warmup_ratio=0.1,\n",
    "   # label_names=\"labels\",\n",
    "    #fp16=True,\n",
    "  #  use_auth_token=False\n",
    "    #push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_ds = PoemDataset(model_input)\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=BartForConditionalGeneration.from_pretrained('BART-base/checkpoint-25000'),\n",
    "    args=args,\n",
    "   # data_collator=data_collator,\n",
    "    train_dataset=train_ds,\n",
    "    \n",
    "    eval_dataset=eval_ds,\n",
    "    #use_auth_token=False,\n",
    "    \n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir GAN_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "            'epoch': 15,\n",
    "            'model_state_dict': generator.state_dict(),\n",
    "            'optimizer_state_dict': optimizerG.state_dict(),\n",
    "            'loss': 0.3355,\n",
    "            }, PATH + 'generator.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "            'epoch': 15,\n",
    "            'model_state_dict': discriminator.state_dict(),\n",
    "            'optimizer_state_dict': optimizerD.state_dict(),\n",
    "            'loss': 0.3355,\n",
    "            }, PATH + 'discriminator.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd GAN_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
