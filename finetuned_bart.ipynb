{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-333f30692099>:7: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"rouge\")\n"
     ]
    }
   ],
   "source": [
    "from transformers import BartTokenizer, BartModel,BartForConditionalGeneration\n",
    "import pandas as pd\n",
    "from datasets import load_metric\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import  Seq2SeqTrainingArguments, Seq2SeqTrainer,DataCollatorForSeq2Seq\n",
    "metric = load_metric(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import encode_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Model \n",
    "# use checkpoint at 24000 iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-base')\n",
    "model = BartForConditionalGeneration.from_pretrained('BART-base/checkpoint-25000')\n",
    "\n",
    "\n",
    "\n",
    "#last_hidden_states = outputs.last_hidden_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retreive data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data_dir = '/home/alexander/nlg-project/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Unnamed: 0                                               Poem  \\\n",
      "0             7542  the mockingbird has testified\\nto springs exis...   \n",
      "1           116211  ive actually\\nheld a black card in my hand\\nan...   \n",
      "2            31718  although alas the\\ncoat on his back is a coat\\...   \n",
      "3            97130  these memories dont\\nmean anything to you and\\...   \n",
      "4            97063  a mad old lady\\nwhooped her tits out at me in\\...   \n",
      "...            ...                                                ...   \n",
      "136820      133722  the manager hugged\\nme like u back see i am\\na...   \n",
      "136821       50010  an entire plot\\nbased around klaus tripping an...   \n",
      "136822       96222  why cant paulina\\nhave social media i\\nmiss he...   \n",
      "136823        5797  i feel used he said\\nand old as usual\\ni belie...   \n",
      "136824       51245  tip if youre upset\\ntake a walk or jog it off\\...   \n",
      "\n",
      "                   Title  \n",
      "0       testified almond  \n",
      "1              held card  \n",
      "2        lining although  \n",
      "3          memories mean  \n",
      "4            whooped pub  \n",
      "...                  ...  \n",
      "136820    hugged manager  \n",
      "136821    klaus tripping  \n",
      "136822     paulina maddy  \n",
      "136823      height usual  \n",
      "136824           jog tip  \n",
      "\n",
      "[136825 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "test_df = pd.read_csv(processed_data_dir + 'test_data.csv')\n",
    "train_df = pd.read_csv(processed_data_dir + 'train_data.csv')\n",
    "print(train_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model = encode_sentences(tokenizer,train_df)\n",
    "test_model = encode_sentences(tokenizer,test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoemDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.labels = df['labels']\n",
    "        self.mask = df['attention_mask']\n",
    "        self.input = df['input_ids']\n",
    "        #self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        input_ids = self.input[idx]\n",
    "        attention_masks = self.mask[idx]\n",
    "        target_ids = self.labels[idx]\n",
    "        batch = {\n",
    "          \"input_ids\": input_ids,\n",
    "          \"decoder_attention_mask\": torch.tensor([1] * 64),\n",
    "          \"attention_mask\": attention_masks,\n",
    "          \"label_ids\": target_ids,\n",
    "        }\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = PoemDataset(train_model)\n",
    "eval_ds = PoemDataset(test_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16 \n",
    "\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    \"BART-base\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    #learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "   # weight_decay=0.01,\n",
    "   # save_total_limit=3,\n",
    "   # num_train_epochs=1,\n",
    "   # predict_with_generate=True,\n",
    "    \n",
    "    learning_rate=1e-4,\n",
    "    weight_decay=0.01,\n",
    "    adam_beta1=0.9,\n",
    "    adam_beta2=0.999,\n",
    "    adam_epsilon=1e-6,\n",
    "    max_grad_norm=1.0,\n",
    "    num_train_epochs=10,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    warmup_ratio=0.1,\n",
    "   # label_names=\"labels\",\n",
    "    #fp16=True,\n",
    "  #  use_auth_token=False\n",
    "    #push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_collator = Collator_poem_gen(tokenizer=tokenizer, max_length=512)\n",
    "#train_ds = PoemDataset(model_input)\n",
    "#eval_ds = PoemDataset(model_input)\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "   # data_collator=data_collator,\n",
    "    train_dataset=train_ds,\n",
    "    \n",
    "    eval_dataset=eval_ds,\n",
    "    #use_auth_token=False,\n",
    "    \n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -c \"from huggingface_hub.hf_api import HfFolder; HfFolder.save_token('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')\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexander/anaconda3/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 136825\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 85520\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='51075' max='85520' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [51075/85520 10:50:22 < 7:18:38, 1.31 it/s, Epoch 5.97/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.455700</td>\n",
       "      <td>0.427779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.423600</td>\n",
       "      <td>0.412270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.395700</td>\n",
       "      <td>0.405117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.358900</td>\n",
       "      <td>0.405889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.330200</td>\n",
       "      <td>0.409589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.303500</td>\n",
       "      <td>0.416410</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to BART-base/checkpoint-500\n",
      "Configuration saved in BART-base/checkpoint-500/config.json\n",
      "Model weights saved in BART-base/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in BART-base/checkpoint-500/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base/checkpoint-1000\n",
      "Configuration saved in BART-base/checkpoint-1000/config.json\n",
      "Model weights saved in BART-base/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in BART-base/checkpoint-1000/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base/checkpoint-1500\n",
      "Configuration saved in BART-base/checkpoint-1500/config.json\n",
      "Model weights saved in BART-base/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in BART-base/checkpoint-1500/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base/checkpoint-2000\n",
      "Configuration saved in BART-base/checkpoint-2000/config.json\n",
      "Model weights saved in BART-base/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in BART-base/checkpoint-2000/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base/checkpoint-2500\n",
      "Configuration saved in BART-base/checkpoint-2500/config.json\n",
      "Model weights saved in BART-base/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in BART-base/checkpoint-2500/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base/checkpoint-3000\n",
      "Configuration saved in BART-base/checkpoint-3000/config.json\n",
      "Model weights saved in BART-base/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in BART-base/checkpoint-3000/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base/checkpoint-3500\n",
      "Configuration saved in BART-base/checkpoint-3500/config.json\n",
      "Model weights saved in BART-base/checkpoint-3500/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base/checkpoint-3500/tokenizer_config.json\n",
      "Special tokens file saved in BART-base/checkpoint-3500/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base/checkpoint-4000\n",
      "Configuration saved in BART-base/checkpoint-4000/config.json\n",
      "Model weights saved in BART-base/checkpoint-4000/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base/checkpoint-4000/tokenizer_config.json\n",
      "Special tokens file saved in BART-base/checkpoint-4000/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base/checkpoint-4500\n",
      "Configuration saved in BART-base/checkpoint-4500/config.json\n",
      "Model weights saved in BART-base/checkpoint-4500/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base/checkpoint-4500/tokenizer_config.json\n",
      "Special tokens file saved in BART-base/checkpoint-4500/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base/checkpoint-5000\n",
      "Configuration saved in BART-base/checkpoint-5000/config.json\n",
      "Model weights saved in BART-base/checkpoint-5000/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base/checkpoint-5000/tokenizer_config.json\n",
      "Special tokens file saved in BART-base/checkpoint-5000/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base/checkpoint-5500\n",
      "Configuration saved in BART-base/checkpoint-5500/config.json\n",
      "Model weights saved in BART-base/checkpoint-5500/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base/checkpoint-5500/tokenizer_config.json\n",
      "Special tokens file saved in BART-base/checkpoint-5500/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base/checkpoint-6000\n",
      "Configuration saved in BART-base/checkpoint-6000/config.json\n",
      "Model weights saved in BART-base/checkpoint-6000/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base/checkpoint-6000/tokenizer_config.json\n",
      "Special tokens file saved in BART-base/checkpoint-6000/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base/checkpoint-6500\n",
      "Configuration saved in BART-base/checkpoint-6500/config.json\n",
      "Model weights saved in BART-base/checkpoint-6500/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base/checkpoint-6500/tokenizer_config.json\n",
      "Special tokens file saved in BART-base/checkpoint-6500/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base/checkpoint-7000\n",
      "Configuration saved in BART-base/checkpoint-7000/config.json\n",
      "Model weights saved in BART-base/checkpoint-7000/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base/checkpoint-7000/tokenizer_config.json\n",
      "Special tokens file saved in BART-base/checkpoint-7000/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base/checkpoint-7500\n",
      "Configuration saved in BART-base/checkpoint-7500/config.json\n",
      "Model weights saved in BART-base/checkpoint-7500/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base/checkpoint-7500/tokenizer_config.json\n",
      "Special tokens file saved in BART-base/checkpoint-7500/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base/checkpoint-8000\n",
      "Configuration saved in BART-base/checkpoint-8000/config.json\n",
      "Model weights saved in BART-base/checkpoint-8000/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base/checkpoint-8000/tokenizer_config.json\n",
      "Special tokens file saved in BART-base/checkpoint-8000/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base/checkpoint-8500\n",
      "Configuration saved in BART-base/checkpoint-8500/config.json\n",
      "Model weights saved in BART-base/checkpoint-8500/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base/checkpoint-8500/tokenizer_config.json\n",
      "Special tokens file saved in BART-base/checkpoint-8500/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7202\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to BART-base/checkpoint-9000\n",
      "Configuration saved in BART-base/checkpoint-9000/config.json\n",
      "Model weights saved in BART-base/checkpoint-9000/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base/checkpoint-9000/tokenizer_config.json\n",
      "Special tokens file saved in BART-base/checkpoint-9000/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base/checkpoint-9500\n",
      "Configuration saved in BART-base/checkpoint-9500/config.json\n",
      "Model weights saved in BART-base/checkpoint-9500/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base/checkpoint-9500/tokenizer_config.json\n",
      "Special tokens file saved in BART-base/checkpoint-9500/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base/checkpoint-10000\n",
      "Configuration saved in BART-base/checkpoint-10000/config.json\n",
      "Model weights saved in BART-base/checkpoint-10000/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base/checkpoint-10000/tokenizer_config.json\n",
      "Special tokens file saved in BART-base/checkpoint-10000/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base/checkpoint-10500\n",
      "Configuration saved in BART-base/checkpoint-10500/config.json\n",
      "Model weights saved in BART-base/checkpoint-10500/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base/checkpoint-10500/tokenizer_config.json\n",
      "Special tokens file saved in BART-base/checkpoint-10500/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base/checkpoint-11500\n",
      "Configuration saved in BART-base/checkpoint-11500/config.json\n",
      "Model weights saved in BART-base/checkpoint-11500/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base/checkpoint-11500/tokenizer_config.json\n",
      "Special tokens file saved in BART-base/checkpoint-11500/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base/checkpoint-12000\n",
      "Configuration saved in BART-base/checkpoint-12000/config.json\n",
      "Model weights saved in BART-base/checkpoint-12000/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base/checkpoint-12000/tokenizer_config.json\n",
      "Special tokens file saved in BART-base/checkpoint-12000/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base/checkpoint-12500\n",
      "Configuration saved in BART-base/checkpoint-12500/config.json\n",
      "Model weights saved in BART-base/checkpoint-12500/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base/checkpoint-12500/tokenizer_config.json\n",
      "Special tokens file saved in BART-base/checkpoint-12500/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to BART-base/checkpoint-13000\n",
      "Configuration saved in BART-base/checkpoint-13000/config.json\n",
      "Model weights saved in BART-base/checkpoint-13000/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base/checkpoint-13000/tokenizer_config.json\n",
      "Special tokens file saved in BART-base/checkpoint-13000/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base/checkpoint-13500\n",
      "Configuration saved in BART-base/checkpoint-13500/config.json\n",
      "Model weights saved in BART-base/checkpoint-13500/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base/checkpoint-13500/tokenizer_config.json\n",
      "Special tokens file saved in BART-base/checkpoint-13500/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base/checkpoint-14000\n",
      "Configuration saved in BART-base/checkpoint-14000/config.json\n",
      "Model weights saved in BART-base/checkpoint-14000/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base/checkpoint-14000/tokenizer_config.json\n",
      "Special tokens file saved in BART-base/checkpoint-14000/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base/checkpoint-14500\n",
      "Configuration saved in BART-base/checkpoint-14500/config.json\n",
      "Model weights saved in BART-base/checkpoint-14500/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base/checkpoint-14500/tokenizer_config.json\n",
      "Special tokens file saved in BART-base/checkpoint-14500/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base/checkpoint-15000\n",
      "Configuration saved in BART-base/checkpoint-15000/config.json\n",
      "Model weights saved in BART-base/checkpoint-15000/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base/checkpoint-15000/tokenizer_config.json\n",
      "Special tokens file saved in BART-base/checkpoint-15000/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base/checkpoint-15500\n",
      "Configuration saved in BART-base/checkpoint-15500/config.json\n",
      "Model weights saved in BART-base/checkpoint-15500/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base/checkpoint-15500/tokenizer_config.json\n",
      "Special tokens file saved in BART-base/checkpoint-15500/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base/checkpoint-16000\n",
      "Configuration saved in BART-base/checkpoint-16000/config.json\n",
      "Model weights saved in BART-base/checkpoint-16000/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base/checkpoint-16000/tokenizer_config.json\n",
      "Special tokens file saved in BART-base/checkpoint-16000/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base/checkpoint-16500\n",
      "Configuration saved in BART-base/checkpoint-16500/config.json\n",
      "Model weights saved in BART-base/checkpoint-16500/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base/checkpoint-16500/tokenizer_config.json\n",
      "Special tokens file saved in BART-base/checkpoint-16500/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base/checkpoint-17000\n",
      "Configuration saved in BART-base/checkpoint-17000/config.json\n",
      "Model weights saved in BART-base/checkpoint-17000/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base/checkpoint-17000/tokenizer_config.json\n",
      "Special tokens file saved in BART-base/checkpoint-17000/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7202\n",
      "  Batch size = 16\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Saving model checkpoint to BART-base/checkpoint-19500\n",
      "Configuration saved in BART-base/checkpoint-19500/config.json\n",
      "Model weights saved in BART-base/checkpoint-19500/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base/checkpoint-19500/tokenizer_config.json\n",
      "Special tokens file saved in BART-base/checkpoint-19500/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base/checkpoint-20000\n",
      "Configuration saved in BART-base/checkpoint-20000/config.json\n",
      "Model weights saved in BART-base/checkpoint-20000/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base/checkpoint-20000/tokenizer_config.json\n",
      "Special tokens file saved in BART-base/checkpoint-20000/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base/checkpoint-20500\n",
      "Configuration saved in BART-base/checkpoint-20500/config.json\n",
      "Model weights saved in BART-base/checkpoint-20500/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base/checkpoint-20500/tokenizer_config.json\n",
      "Special tokens file saved in BART-base/checkpoint-20500/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base/checkpoint-21000\n",
      "Configuration saved in BART-base/checkpoint-21000/config.json\n",
      "Model weights saved in BART-base/checkpoint-21000/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base/checkpoint-21000/tokenizer_config.json\n",
      "Special tokens file saved in BART-base/checkpoint-21000/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base/checkpoint-21500\n",
      "Configuration saved in BART-base/checkpoint-21500/config.json\n",
      "Model weights saved in BART-base/checkpoint-21500/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base/checkpoint-21500/tokenizer_config.json\n",
      "Special tokens file saved in BART-base/checkpoint-21500/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base/checkpoint-22000\n",
      "Configuration saved in BART-base/checkpoint-22000/config.json\n",
      "Model weights saved in BART-base/checkpoint-22000/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base/checkpoint-22000/tokenizer_config.json\n",
      "Special tokens file saved in BART-base/checkpoint-22000/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base/checkpoint-22500\n",
      "Configuration saved in BART-base/checkpoint-22500/config.json\n",
      "Model weights saved in BART-base/checkpoint-22500/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base/checkpoint-22500/tokenizer_config.json\n",
      "Special tokens file saved in BART-base/checkpoint-22500/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base/checkpoint-23000\n",
      "Configuration saved in BART-base/checkpoint-23000/config.json\n",
      "Model weights saved in BART-base/checkpoint-23000/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base/checkpoint-23000/tokenizer_config.json\n",
      "Special tokens file saved in BART-base/checkpoint-23000/special_tokens_map.json\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Saving model checkpoint to BART-base/checkpoint-26000\n",
      "Configuration saved in BART-base/checkpoint-26000/config.json\n",
      "Model weights saved in BART-base/checkpoint-26000/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base/checkpoint-26000/tokenizer_config.json\n",
      "Special tokens file saved in BART-base/checkpoint-26000/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base/checkpoint-26500\n",
      "Configuration saved in BART-base/checkpoint-26500/config.json\n",
      "Model weights saved in BART-base/checkpoint-26500/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base/checkpoint-26500/tokenizer_config.json\n",
      "Special tokens file saved in BART-base/checkpoint-26500/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base/checkpoint-27000\n",
      "Configuration saved in BART-base/checkpoint-27000/config.json\n",
      "Model weights saved in BART-base/checkpoint-27000/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base/checkpoint-27000/tokenizer_config.json\n",
      "Special tokens file saved in BART-base/checkpoint-27000/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base/checkpoint-27500\n",
      "Configuration saved in BART-base/checkpoint-27500/config.json\n",
      "Model weights saved in BART-base/checkpoint-27500/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base/checkpoint-27500/tokenizer_config.json\n",
      "Special tokens file saved in BART-base/checkpoint-27500/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base/checkpoint-28000\n",
      "Configuration saved in BART-base/checkpoint-28000/config.json\n",
      "Model weights saved in BART-base/checkpoint-28000/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base/checkpoint-28000/tokenizer_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens file saved in BART-base/checkpoint-28000/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base/checkpoint-28500\n",
      "Configuration saved in BART-base/checkpoint-28500/config.json\n",
      "Model weights saved in BART-base/checkpoint-28500/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base/checkpoint-28500/tokenizer_config.json\n",
      "Special tokens file saved in BART-base/checkpoint-28500/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base/checkpoint-29000\n",
      "Configuration saved in BART-base/checkpoint-29000/config.json\n",
      "Model weights saved in BART-base/checkpoint-29000/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base/checkpoint-29000/tokenizer_config.json\n",
      "Special tokens file saved in BART-base/checkpoint-29000/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base/checkpoint-29500\n",
      "Configuration saved in BART-base/checkpoint-29500/config.json\n",
      "Model weights saved in BART-base/checkpoint-29500/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base/checkpoint-29500/tokenizer_config.json\n",
      "Special tokens file saved in BART-base/checkpoint-29500/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base/checkpoint-30000\n",
      "Configuration saved in BART-base/checkpoint-30000/config.json\n",
      "Model weights saved in BART-base/checkpoint-30000/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base/checkpoint-30000/tokenizer_config.json\n",
      "Special tokens file saved in BART-base/checkpoint-30000/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base/checkpoint-30500\n",
      "Configuration saved in BART-base/checkpoint-30500/config.json\n",
      "Model weights saved in BART-base/checkpoint-30500/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base/checkpoint-30500/tokenizer_config.json\n",
      "Special tokens file saved in BART-base/checkpoint-30500/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base/checkpoint-31000\n",
      "Configuration saved in BART-base/checkpoint-31000/config.json\n",
      "Model weights saved in BART-base/checkpoint-31000/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base/checkpoint-31000/tokenizer_config.json\n",
      "Special tokens file saved in BART-base/checkpoint-31000/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base/checkpoint-31500\n",
      "Configuration saved in BART-base/checkpoint-31500/config.json\n",
      "Model weights saved in BART-base/checkpoint-31500/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base/checkpoint-31500/tokenizer_config.json\n",
      "Special tokens file saved in BART-base/checkpoint-31500/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base/checkpoint-32000\n",
      "Configuration saved in BART-base/checkpoint-32000/config.json\n",
      "Model weights saved in BART-base/checkpoint-32000/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base/checkpoint-32000/tokenizer_config.json\n",
      "Special tokens file saved in BART-base/checkpoint-32000/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base/checkpoint-32500\n",
      "Configuration saved in BART-base/checkpoint-32500/config.json\n",
      "Model weights saved in BART-base/checkpoint-32500/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base/checkpoint-32500/tokenizer_config.json\n",
      "Special tokens file saved in BART-base/checkpoint-32500/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base/checkpoint-33000\n",
      "Configuration saved in BART-base/checkpoint-33000/config.json\n",
      "Model weights saved in BART-base/checkpoint-33000/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base/checkpoint-33000/tokenizer_config.json\n",
      "Special tokens file saved in BART-base/checkpoint-33000/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base/checkpoint-33500\n",
      "Configuration saved in BART-base/checkpoint-33500/config.json\n",
      "Model weights saved in BART-base/checkpoint-33500/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base/checkpoint-33500/tokenizer_config.json\n",
      "Special tokens file saved in BART-base/checkpoint-33500/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base/checkpoint-34000\n",
      "Configuration saved in BART-base/checkpoint-34000/config.json\n",
      "Model weights saved in BART-base/checkpoint-34000/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base/checkpoint-34000/tokenizer_config.json\n",
      "Special tokens file saved in BART-base/checkpoint-34000/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7202\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to BART-base/checkpoint-34500\n",
      "Configuration saved in BART-base/checkpoint-34500/config.json\n",
      "Model weights saved in BART-base/checkpoint-34500/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base/checkpoint-34500/tokenizer_config.json\n",
      "Special tokens file saved in BART-base/checkpoint-34500/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base/checkpoint-35000\n",
      "Configuration saved in BART-base/checkpoint-35000/config.json\n",
      "Model weights saved in BART-base/checkpoint-35000/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base/checkpoint-35000/tokenizer_config.json\n",
      "Special tokens file saved in BART-base/checkpoint-35000/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base/checkpoint-35500\n",
      "Configuration saved in BART-base/checkpoint-35500/config.json\n",
      "Model weights saved in BART-base/checkpoint-35500/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base/checkpoint-35500/tokenizer_config.json\n",
      "Special tokens file saved in BART-base/checkpoint-35500/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base/checkpoint-36000\n",
      "Configuration saved in BART-base/checkpoint-36000/config.json\n",
      "Model weights saved in BART-base/checkpoint-36000/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base/checkpoint-36000/tokenizer_config.json\n",
      "Special tokens file saved in BART-base/checkpoint-36000/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base/checkpoint-36500\n",
      "Configuration saved in BART-base/checkpoint-36500/config.json\n",
      "Model weights saved in BART-base/checkpoint-36500/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base/checkpoint-36500/tokenizer_config.json\n",
      "Special tokens file saved in BART-base/checkpoint-36500/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base/checkpoint-37000\n",
      "Configuration saved in BART-base/checkpoint-37000/config.json\n",
      "Model weights saved in BART-base/checkpoint-37000/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base/checkpoint-37000/tokenizer_config.json\n",
      "Special tokens file saved in BART-base/checkpoint-37000/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base/checkpoint-37500\n",
      "Configuration saved in BART-base/checkpoint-37500/config.json\n",
      "Model weights saved in BART-base/checkpoint-37500/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base/checkpoint-37500/tokenizer_config.json\n",
      "Special tokens file saved in BART-base/checkpoint-37500/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base/checkpoint-38000\n",
      "Configuration saved in BART-base/checkpoint-38000/config.json\n",
      "Model weights saved in BART-base/checkpoint-38000/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base/checkpoint-38000/tokenizer_config.json\n",
      "Special tokens file saved in BART-base/checkpoint-38000/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base/checkpoint-38500\n",
      "Configuration saved in BART-base/checkpoint-38500/config.json\n",
      "Model weights saved in BART-base/checkpoint-38500/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base/checkpoint-38500/tokenizer_config.json\n",
      "Special tokens file saved in BART-base/checkpoint-38500/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base/checkpoint-39000\n",
      "Configuration saved in BART-base/checkpoint-39000/config.json\n",
      "Model weights saved in BART-base/checkpoint-39000/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base/checkpoint-39000/tokenizer_config.json\n",
      "Special tokens file saved in BART-base/checkpoint-39000/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base/checkpoint-39500\n",
      "Configuration saved in BART-base/checkpoint-39500/config.json\n",
      "Model weights saved in BART-base/checkpoint-39500/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base/checkpoint-39500/tokenizer_config.json\n",
      "Special tokens file saved in BART-base/checkpoint-39500/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base/checkpoint-40000\n",
      "Configuration saved in BART-base/checkpoint-40000/config.json\n",
      "Model weights saved in BART-base/checkpoint-40000/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer config file saved in BART-base/checkpoint-40000/tokenizer_config.json\n",
      "Special tokens file saved in BART-base/checkpoint-40000/special_tokens_map.json\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Saving model checkpoint to BART-base/checkpoint-48500\n",
      "Configuration saved in BART-base/checkpoint-48500/config.json\n",
      "Model weights saved in BART-base/checkpoint-48500/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base/checkpoint-48500/tokenizer_config.json\n",
      "Special tokens file saved in BART-base/checkpoint-48500/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base/checkpoint-49000\n",
      "Configuration saved in BART-base/checkpoint-49000/config.json\n",
      "Model weights saved in BART-base/checkpoint-49000/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base/checkpoint-49000/tokenizer_config.json\n",
      "Special tokens file saved in BART-base/checkpoint-49000/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base/checkpoint-49500\n",
      "Configuration saved in BART-base/checkpoint-49500/config.json\n",
      "Model weights saved in BART-base/checkpoint-49500/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base/checkpoint-49500/tokenizer_config.json\n",
      "Special tokens file saved in BART-base/checkpoint-49500/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base/checkpoint-50000\n",
      "Configuration saved in BART-base/checkpoint-50000/config.json\n",
      "Model weights saved in BART-base/checkpoint-50000/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base/checkpoint-50000/tokenizer_config.json\n",
      "Special tokens file saved in BART-base/checkpoint-50000/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base/checkpoint-50500\n",
      "Configuration saved in BART-base/checkpoint-50500/config.json\n",
      "Model weights saved in BART-base/checkpoint-50500/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base/checkpoint-50500/tokenizer_config.json\n",
      "Special tokens file saved in BART-base/checkpoint-50500/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base/checkpoint-51000\n",
      "Configuration saved in BART-base/checkpoint-51000/config.json\n",
      "Model weights saved in BART-base/checkpoint-51000/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base/checkpoint-51000/tokenizer_config.json\n",
      "Special tokens file saved in BART-base/checkpoint-51000/special_tokens_map.json\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-3435b262f1ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1498\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inner_training_loop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_find_batch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1499\u001b[0m         )\n\u001b[0;32m-> 1500\u001b[0;31m         return inner_training_loop(\n\u001b[0m\u001b[1;32m   1501\u001b[0m             \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1502\u001b[0m             \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1740\u001b[0m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1742\u001b[0;31m                     \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1743\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1744\u001b[0m                 if (\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2502\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepspeed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2503\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2504\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2505\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2506\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \"\"\"\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    126\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 7202\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.4164099395275116}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
