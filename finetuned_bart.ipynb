{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-333f30692099>:7: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"rouge\")\n"
     ]
    }
   ],
   "source": [
    "from transformers import BartTokenizer, BartModel,BartForConditionalGeneration\n",
    "import pandas as pd\n",
    "from datasets import load_metric\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import  Seq2SeqTrainingArguments, Seq2SeqTrainer,DataCollatorForSeq2Seq\n",
    "metric = load_metric(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import PoemDataset,encode_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Model \n",
    "# use checkpoint at 34000 iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'=20.9'           __init__.py    data_processing.ipynb   utils.py\r\n",
      " BART-GAN.ipynb   \u001b[0m\u001b[01;34m__pycache__\u001b[0m/   \u001b[01;34mdata_raw\u001b[0m/\r\n",
      " \u001b[01;34mGAN_models\u001b[0m/      \u001b[01;34mdata\u001b[0m/          finetuned_bart.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-base')\n",
    "model = BartForConditionalGeneration.from_pretrained('BART-base/checkpoint-25000')\n",
    "\n",
    "\n",
    "\n",
    "#last_hidden_states = outputs.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-178-0b7b6bf5747b>:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  output_tokens = torch.cat([output_tokens,torch.tensor(next_word)],axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad> <s>  i  have  a  card \n",
      " held  in  my  hand  and  i  dont \n",
      " even  know  what  it  is </s>\n"
     ]
    }
   ],
   "source": [
    "generated_str, probs = simple_decoding(model,inputs)\n",
    "print(' '.join(tokenizer.batch_decode(generated_str)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_inputs_for_generation(input_ids, past=None, attention_mask=None, use_cache=None, **kwargs):\n",
    "    # if model is used as a decoder in encoder-decoder model, the decoder attention mask is created on the fly\n",
    "    if attention_mask is None:\n",
    "        attention_mask = input_ids.new_ones(input_ids.shape)\n",
    "\n",
    "    if past:\n",
    "        input_ids = input_ids[:, -1:]\n",
    "    # first step, decoder_cached_states are empty\n",
    "    return {\n",
    "        \"input_ids\": input_ids,  # encoder_outputs is defined. input_ids not needed\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"past_key_values\": past,\n",
    "        \"use_cache\": use_cache,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_processor = LogitsProcessorList(\n",
    "      [\n",
    "            MinLengthLogitsProcessor(10, eos_token_id=model.config.eos_token_id),\n",
    "        ]\n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0, 11706,  1886, 50264,     2]])\n",
      "tensor([[    0, 11706,  1886, 50264,     2, 50118]])\n",
      "tensor([[    0, 11706,  1886, 50264,     2, 50118,     1]])\n",
      "tensor([[    0, 11706,  1886, 50264,     2, 50118,     1,     1]])\n",
      "tensor([[    0, 11706,  1886, 50264,     2, 50118,     1,     1,     1]])\n",
      "tensor([[    0, 11706,  1886, 50264,     2, 50118,     1,     1,     1,     1]])\n",
      "tensor([[    0, 11706,  1886, 50264,     2, 50118,     1,     1,     1,     1,\n",
      "             1]])\n",
      "tensor([[    0, 11706,  1886, 50264,     2, 50118,     1,     1,     1,     1,\n",
      "             1,     1]])\n",
      "tensor([[    0, 11706,  1886, 50264,     2, 50118,     1,     1,     1,     1,\n",
      "             1,     1,     1]])\n",
      "tensor([[    0, 11706,  1886, 50264,     2, 50118,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1]])\n",
      "tensor([[    0, 11706,  1886, 50264,     2, 50118,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1]])\n",
      "tensor([[    0, 11706,  1886, 50264,     2, 50118,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1]])\n",
      "tensor([[    0, 11706,  1886, 50264,     2, 50118,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1]])\n",
      "tensor([[    0, 11706,  1886, 50264,     2, 50118,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1]])\n",
      "tensor([[    0, 11706,  1886, 50264,     2, 50118,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1]])\n",
      "tensor([[    0, 11706,  1886, 50264,     2, 50118,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1]])\n",
      "tensor([[    0, 11706,  1886, 50264,     2, 50118,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1]])\n",
      "tensor([[    0, 11706,  1886, 50264,     2, 50118,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1]])\n",
      "tensor([[    0, 11706,  1886, 50264,     2, 50118,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1]])\n",
      "tensor([[    0, 11706,  1886, 50264,     2, 50118,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1]])\n",
      "tensor([[    0, 11706,  1886, 50264,     2, 50118,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1]])\n",
      "tensor([[    0, 11706,  1886, 50264,     2, 50118,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1]])\n",
      "tensor([[    0, 11706,  1886, 50264,     2, 50118,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1]])\n",
      "tensor([[    0, 11706,  1886, 50264,     2, 50118,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1]])\n",
      "tensor([[    0, 11706,  1886, 50264,     2, 50118,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1]])\n",
      "tensor([[    0, 11706,  1886, 50264,     2, 50118,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1]])\n",
      "tensor([[    0, 11706,  1886, 50264,     2, 50118,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1]])\n",
      "tensor([[    0, 11706,  1886, 50264,     2, 50118,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1]])\n",
      "tensor([[    0, 11706,  1886, 50264,     2, 50118,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1]])\n",
      "tensor([[    0, 11706,  1886, 50264,     2, 50118,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1]])\n",
      "tensor([[    0, 11706,  1886, 50264,     2, 50118,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1]])\n",
      "tensor([[    0, 11706,  1886, 50264,     2, 50118,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1]])\n",
      "tensor([[    0, 11706,  1886, 50264,     2, 50118,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1]])\n",
      "tensor([[    0, 11706,  1886, 50264,     2, 50118,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1]])\n",
      "tensor([[    0, 11706,  1886, 50264,     2, 50118,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1]])\n",
      "tensor([[    0, 11706,  1886, 50264,     2, 50118,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1]])\n",
      "tensor([[    0, 11706,  1886, 50264,     2, 50118,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1]])\n",
      "tensor([[    0, 11706,  1886, 50264,     2, 50118,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1]])\n",
      "tensor([[    0, 11706,  1886, 50264,     2, 50118,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1]])\n",
      "tensor([[    0, 11706,  1886, 50264,     2, 50118,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1]])\n",
      "tensor([[    0, 11706,  1886, 50264,     2, 50118,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1]])\n",
      "tensor([[    0, 11706,  1886, 50264,     2, 50118,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1]])\n",
      "tensor([[    0, 11706,  1886, 50264,     2, 50118,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1]])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0, 11706,  1886, 50264,     2, 50118,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1]])\n",
      "tensor([[    0, 11706,  1886, 50264,     2, 50118,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1]])\n",
      "tensor([[    0, 11706,  1886, 50264,     2, 50118,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1]])\n",
      "tensor([[    0, 11706,  1886, 50264,     2, 50118,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1]])\n",
      "tensor([[    0, 11706,  1886, 50264,     2, 50118,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1]])\n",
      "tensor([[    0, 11706,  1886, 50264,     2, 50118,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1]])\n",
      "tensor([[    0, 11706,  1886, 50264,     2, 50118,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1]])\n",
      "tensor([[    0, 11706,  1886, 50264,     2, 50118,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1]])\n",
      "tensor([[    0, 11706,  1886, 50264,     2, 50118,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1]])\n",
      "tensor([[    0, 11706,  1886, 50264,     2, 50118,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1]])\n",
      "tensor([[    0, 11706,  1886, 50264,     2, 50118,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1]])\n",
      "tensor([[    0, 11706,  1886, 50264,     2, 50118,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1]])\n",
      "tensor([[    0, 11706,  1886, 50264,     2, 50118,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1]])\n",
      "tensor([[    0, 11706,  1886, 50264,     2, 50118,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1]])\n",
      "tensor([[    0, 11706,  1886, 50264,     2, 50118,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1]])\n",
      "tensor([[    0, 11706,  1886, 50264,     2, 50118,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1]])\n",
      "tensor([[    0, 11706,  1886, 50264,     2, 50118,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1]])\n",
      "tensor([[    0, 11706,  1886, 50264,     2, 50118,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1]])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-156-69e00ddb93e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcomplex_decoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-155-21598e364a02>\u001b[0m in \u001b[0;36mcomplex_decoder\u001b[0;34m(model, inputs)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m# forward pass to get next token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         outputs = model(\n\u001b[0m\u001b[1;32m     23\u001b[0m             \u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/transformers/models/bart/modeling_bart.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1357\u001b[0m                 )\n\u001b[1;32m   1358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1359\u001b[0;31m         outputs = self.model(\n\u001b[0m\u001b[1;32m   1360\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/transformers/models/bart/modeling_bart.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1245\u001b[0m         \u001b[0;31m# decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1246\u001b[0;31m         decoder_outputs = self.decoder(\n\u001b[0m\u001b[1;32m   1247\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoder_input_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1248\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/transformers/models/bart/modeling_bart.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, head_mask, cross_attn_head_mask, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1102\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1104\u001b[0;31m                 layer_outputs = decoder_layer(\n\u001b[0m\u001b[1;32m   1105\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1106\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/transformers/models/bart/modeling_bart.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, encoder_hidden_states, encoder_attention_mask, layer_head_mask, cross_attn_layer_head_mask, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[1;32m    454\u001b[0m         \u001b[0;31m# Fully Connected\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m         \u001b[0mresidual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 456\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    457\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation_dropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1674\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1675\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1676\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1677\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1678\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "complex_decoder(model.cpu(),inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1026, 1244,  307, 1744,  284]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retreive data\n",
    "Right now just using kaggle corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/alexander/nlg-project/data'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data_dir = '/home/alexander/nlg-project/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ive actually\\nheld a black card in my hand\\nand cant believe it'"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['Poem'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Unnamed: 0                                               Poem  \\\n",
      "0             7542  the mockingbird has testified\\nto springs exis...   \n",
      "1           116211  ive actually\\nheld a black card in my hand\\nan...   \n",
      "2            31718  although alas the\\ncoat on his back is a coat\\...   \n",
      "3            97130  these memories dont\\nmean anything to you and\\...   \n",
      "4            97063  a mad old lady\\nwhooped her tits out at me in\\...   \n",
      "...            ...                                                ...   \n",
      "136820      133722  the manager hugged\\nme like u back see i am\\na...   \n",
      "136821       50010  an entire plot\\nbased around klaus tripping an...   \n",
      "136822       96222  why cant paulina\\nhave social media i\\nmiss he...   \n",
      "136823        5797  i feel used he said\\nand old as usual\\ni belie...   \n",
      "136824       51245  tip if youre upset\\ntake a walk or jog it off\\...   \n",
      "\n",
      "                   Title  \n",
      "0       testified almond  \n",
      "1              held card  \n",
      "2        lining although  \n",
      "3          memories mean  \n",
      "4            whooped pub  \n",
      "...                  ...  \n",
      "136820    hugged manager  \n",
      "136821    klaus tripping  \n",
      "136822     paulina maddy  \n",
      "136823      height usual  \n",
      "136824           jog tip  \n",
      "\n",
      "[136825 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "test_df = pd.read_csv(processed_data_dir + 'test_data.csv')\n",
    "train_df = pd.read_csv(processed_data_dir + 'train_data.csv')\n",
    "print(train_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_input' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-70-ac51b5b7d6f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_model\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'attention_mask'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_model\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'attention_mask'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'decoder_attention_mask'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_model\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'decoder_attention_mask'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'labels'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_model\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'labels'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_input\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel_input\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'attention_mask'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel_input\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'decoder_attention_mask'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel_input\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'labels'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model_input' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'encode_sentences' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-9ccb015c9619>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencode_sentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtest_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencode_sentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'encode_sentences' is not defined"
     ]
    }
   ],
   "source": [
    "train_model = encode_sentences(tokenizer,train_df)\n",
    "test_model = encode_sentences(tokenizer,test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    0,  2841, 47320,  ...,     1,     1,     1],\n",
       "         [    0,    70,   254,  ...,     1,     1,     1],\n",
       "         [    0,  2649,  3006,  ...,     1,     1,     1],\n",
       "         ...,\n",
       "         [    0, 17668,  6148,  ...,     1,     1,     1],\n",
       "         [    0, 13789,  7252,  ...,     1,     1,     1],\n",
       "         [    0, 16328,  9699,  ...,     1,     1,     1]]),\n",
       " 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]]),\n",
       " 'labels': tensor([[    0,   608,    24,  ...,     1,     1,     1],\n",
       "         [    0,  4356,  1158,  ...,     1,     1,     1],\n",
       "         [    0, 17271,  2892,  ...,     1,     1,     1],\n",
       "         ...,\n",
       "         [    0,  7409,    10,  ...,     1,     1,     1],\n",
       "         [    0,  7252,    37,  ...,     1,     1,     1],\n",
       "         [    0,   141,   171,  ...,     1,     1,     1]])}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = PoemDataset(train_model)\n",
    "eval_ds = PoemDataset(test_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoemDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.labels = df['labels']\n",
    "        self.mask = df['attention_mask']\n",
    "        self.input = df['input_ids']\n",
    "        #self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        input_ids = self.input[idx]\n",
    "        attention_masks = self.mask[idx]\n",
    "        target_ids = self.labels[idx]\n",
    "        batch = {\n",
    "          \"input_ids\": input_ids,\n",
    "          \"decoder_attention_mask\": torch.tensor([1] * 128),\n",
    "          \"attention_mask\": attention_masks,\n",
    "          \"label_ids\": target_ids,\n",
    "        }\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Collator_poem_gen(DataCollatorForSeq2Seq):\n",
    "        \n",
    "    def __call__(self, batch):\n",
    "        input_ids_batch = []\n",
    "        input_ids_len = []\n",
    "        \n",
    "        labels_batch = []\n",
    "        labels_len = []\n",
    "        \n",
    "        for example in batch:\n",
    "            input_ids = example[\"Title\"]\n",
    "            input_ids_len.append(len(input_ids))\n",
    "            input_ids_batch.append(input_ids)\n",
    "            \n",
    "            labels = example[\"Poem\"]\n",
    "            labels_len.append(len(labels))\n",
    "            labels_batch.append(labels)\n",
    "\n",
    "        input_ids_padded = self.process_encoded_text(input_ids_batch, input_ids_len, self.tokenizer.pad_token_id)\n",
    "        labels_padded = self.process_encoded_text(labels_batch, labels_len, self.label_pad_token_id)\n",
    "        \n",
    "        attention_mask = self.generate_attention_mask(input_ids_padded, self.tokenizer.pad_token_id)\n",
    "        decoder_attention_mask = self.generate_attention_mask(labels_padded, self.tokenizer.pad_token_id)\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": input_ids_padded,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"decoder_attention_mask\": decoder_attention_mask,\n",
    "            \"labels\": labels_padded,\n",
    "        }\n",
    "\n",
    "    def process_encoded_text(self, sequences, sequences_len, pad_token_id):\n",
    "        sequences_max_len = np.max(sequences_len)\n",
    "        max_length = min(sequences_max_len, self.max_length)\n",
    "        padded_sequences = self.pad_sequences(sequences, max_length, pad_token_id)\n",
    "        return torch.LongTensor(padded_sequences)\n",
    "\n",
    "    def generate_attention_mask(self, input_ids, pad_token_id):\n",
    "        return (input_ids != pad_token_id).long()\n",
    "    \n",
    "    def pad_sequences(self, sequences, max_length, pad_token_id):\n",
    "        num_samples = len(sequences)\n",
    "        padded_sequences = np.full((num_samples, max_length), pad_token_id)\n",
    "        for i, sequence in enumerate(sequences):\n",
    "            sequence = np.array(sequence)[:max_length]\n",
    "            padded_sequences[i, :len(sequence)] = sequence\n",
    "        return padded_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "If using all scalar values, you must pass an index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/ql/35698m6j55sfgbxr9rn581040000gn/T/ipykernel_44066/1651573470.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0minput_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mfrom_dict\u001b[0;34m(cls, data, orient, dtype, columns)\u001b[0m\n\u001b[1;32m   1591\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"only recognize index or columns for orient\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1592\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1593\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1594\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1595\u001b[0m     def to_numpy(\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    612\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m             \u001b[0;31m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 614\u001b[0;31m             \u001b[0mmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict_to_mgr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtyp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmanager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    615\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMaskedArray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m             \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmrecords\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmrecords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36mdict_to_mgr\u001b[0;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[1;32m    462\u001b[0m         \u001b[0;31m# TODO: can we get rid of the dt64tz special case above?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 464\u001b[0;31m     return arrays_to_mgr(\n\u001b[0m\u001b[1;32m    465\u001b[0m         \u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtyp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtyp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsolidate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m     )\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, arr_names, index, columns, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;31m# figure out the index, if necessary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_extract_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36m_extract_index\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    623\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mindexes\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mraw_lengths\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 625\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"If using all scalar values, you must pass an index\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhave_series\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: If using all scalar values, you must pass an index"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "input_df = pd.DataFrame.from_dict(model_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16 \n",
    "\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    \"BART-base\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    #learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "   # weight_decay=0.01,\n",
    "   # save_total_limit=3,\n",
    "   # num_train_epochs=1,\n",
    "   # predict_with_generate=True,\n",
    "    \n",
    "    learning_rate=1e-4,\n",
    "    weight_decay=0.01,\n",
    "    adam_beta1=0.9,\n",
    "    adam_beta2=0.999,\n",
    "    adam_epsilon=1e-6,\n",
    "    max_grad_norm=1.0,\n",
    "    num_train_epochs=10,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    warmup_ratio=0.1,\n",
    "   # label_names=\"labels\",\n",
    "    #fp16=True,\n",
    "  #  use_auth_token=False\n",
    "    #push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_collator = Collator_poem_gen(tokenizer=tokenizer, max_length=512)\n",
    "#train_ds = PoemDataset(model_input)\n",
    "#eval_ds = PoemDataset(model_input)\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "   # data_collator=data_collator,\n",
    "    train_dataset=train_ds,\n",
    "    \n",
    "    eval_dataset=eval_ds,\n",
    "    #use_auth_token=False,\n",
    "    \n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<transformers.trainer_seq2seq.Seq2SeqTrainer at 0x7f7ec4877af0>"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -c \"from huggingface_hub.hf_api import HfFolder; HfFolder.save_token('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')\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexander/anaconda3/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 136825\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 85520\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='51075' max='85520' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [51075/85520 10:50:22 < 7:18:38, 1.31 it/s, Epoch 5.97/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.455700</td>\n",
       "      <td>0.427779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.423600</td>\n",
       "      <td>0.412270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.395700</td>\n",
       "      <td>0.405117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.358900</td>\n",
       "      <td>0.405889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.330200</td>\n",
       "      <td>0.409589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.303500</td>\n",
       "      <td>0.416410</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to BART-base/checkpoint-500\n",
      "Configuration saved in BART-base/checkpoint-500/config.json\n",
      "Model weights saved in BART-base/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in BART-base/checkpoint-500/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base/checkpoint-1000\n",
      "Configuration saved in BART-base/checkpoint-1000/config.json\n",
      "Model weights saved in BART-base/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in BART-base/checkpoint-1000/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base/checkpoint-1500\n",
      "Configuration saved in BART-base/checkpoint-1500/config.json\n",
      "Model weights saved in BART-base/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in BART-base/checkpoint-1500/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base/checkpoint-2000\n",
      "Configuration saved in BART-base/checkpoint-2000/config.json\n",
      "Model weights saved in BART-base/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in BART-base/checkpoint-2000/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base/checkpoint-2500\n",
      "Configuration saved in BART-base/checkpoint-2500/config.json\n",
      "Model weights saved in BART-base/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in BART-base/checkpoint-2500/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base/checkpoint-3000\n",
      "Configuration saved in BART-base/checkpoint-3000/config.json\n",
      "Model weights saved in BART-base/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in BART-base/checkpoint-3000/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base/checkpoint-3500\n",
      "Configuration saved in BART-base/checkpoint-3500/config.json\n",
      "Model weights saved in BART-base/checkpoint-3500/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base/checkpoint-3500/tokenizer_config.json\n",
      "Special tokens file saved in BART-base/checkpoint-3500/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base/checkpoint-4000\n",
      "Configuration saved in BART-base/checkpoint-4000/config.json\n",
      "Model weights saved in BART-base/checkpoint-4000/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base/checkpoint-4000/tokenizer_config.json\n",
      "Special tokens file saved in BART-base/checkpoint-4000/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base/checkpoint-4500\n",
      "Configuration saved in BART-base/checkpoint-4500/config.json\n",
      "Model weights saved in BART-base/checkpoint-4500/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base/checkpoint-4500/tokenizer_config.json\n",
      "Special tokens file saved in BART-base/checkpoint-4500/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base/checkpoint-5000\n",
      "Configuration saved in BART-base/checkpoint-5000/config.json\n",
      "Model weights saved in BART-base/checkpoint-5000/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base/checkpoint-5000/tokenizer_config.json\n",
      "Special tokens file saved in BART-base/checkpoint-5000/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base/checkpoint-5500\n",
      "Configuration saved in BART-base/checkpoint-5500/config.json\n",
      "Model weights saved in BART-base/checkpoint-5500/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base/checkpoint-5500/tokenizer_config.json\n",
      "Special tokens file saved in BART-base/checkpoint-5500/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base/checkpoint-6000\n",
      "Configuration saved in BART-base/checkpoint-6000/config.json\n",
      "Model weights saved in BART-base/checkpoint-6000/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base/checkpoint-6000/tokenizer_config.json\n",
      "Special tokens file saved in BART-base/checkpoint-6000/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base/checkpoint-6500\n",
      "Configuration saved in BART-base/checkpoint-6500/config.json\n",
      "Model weights saved in BART-base/checkpoint-6500/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base/checkpoint-6500/tokenizer_config.json\n",
      "Special tokens file saved in BART-base/checkpoint-6500/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base/checkpoint-7000\n",
      "Configuration saved in BART-base/checkpoint-7000/config.json\n",
      "Model weights saved in BART-base/checkpoint-7000/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base/checkpoint-7000/tokenizer_config.json\n",
      "Special tokens file saved in BART-base/checkpoint-7000/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base/checkpoint-7500\n",
      "Configuration saved in BART-base/checkpoint-7500/config.json\n",
      "Model weights saved in BART-base/checkpoint-7500/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base/checkpoint-7500/tokenizer_config.json\n",
      "Special tokens file saved in BART-base/checkpoint-7500/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base/checkpoint-8000\n",
      "Configuration saved in BART-base/checkpoint-8000/config.json\n",
      "Model weights saved in BART-base/checkpoint-8000/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base/checkpoint-8000/tokenizer_config.json\n",
      "Special tokens file saved in BART-base/checkpoint-8000/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base/checkpoint-8500\n",
      "Configuration saved in BART-base/checkpoint-8500/config.json\n",
      "Model weights saved in BART-base/checkpoint-8500/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base/checkpoint-8500/tokenizer_config.json\n",
      "Special tokens file saved in BART-base/checkpoint-8500/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7202\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to BART-base/checkpoint-9000\n",
      "Configuration saved in BART-base/checkpoint-9000/config.json\n",
      "Model weights saved in BART-base/checkpoint-9000/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base/checkpoint-9000/tokenizer_config.json\n",
      "Special tokens file saved in BART-base/checkpoint-9000/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base/checkpoint-9500\n",
      "Configuration saved in BART-base/checkpoint-9500/config.json\n",
      "Model weights saved in BART-base/checkpoint-9500/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base/checkpoint-9500/tokenizer_config.json\n",
      "Special tokens file saved in BART-base/checkpoint-9500/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base/checkpoint-10000\n",
      "Configuration saved in BART-base/checkpoint-10000/config.json\n",
      "Model weights saved in BART-base/checkpoint-10000/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base/checkpoint-10000/tokenizer_config.json\n",
      "Special tokens file saved in BART-base/checkpoint-10000/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base/checkpoint-10500\n",
      "Configuration saved in BART-base/checkpoint-10500/config.json\n",
      "Model weights saved in BART-base/checkpoint-10500/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base/checkpoint-10500/tokenizer_config.json\n",
      "Special tokens file saved in BART-base/checkpoint-10500/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base/checkpoint-11500\n",
      "Configuration saved in BART-base/checkpoint-11500/config.json\n",
      "Model weights saved in BART-base/checkpoint-11500/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base/checkpoint-11500/tokenizer_config.json\n",
      "Special tokens file saved in BART-base/checkpoint-11500/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base/checkpoint-12000\n",
      "Configuration saved in BART-base/checkpoint-12000/config.json\n",
      "Model weights saved in BART-base/checkpoint-12000/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base/checkpoint-12000/tokenizer_config.json\n",
      "Special tokens file saved in BART-base/checkpoint-12000/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base/checkpoint-12500\n",
      "Configuration saved in BART-base/checkpoint-12500/config.json\n",
      "Model weights saved in BART-base/checkpoint-12500/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base/checkpoint-12500/tokenizer_config.json\n",
      "Special tokens file saved in BART-base/checkpoint-12500/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to BART-base/checkpoint-13000\n",
      "Configuration saved in BART-base/checkpoint-13000/config.json\n",
      "Model weights saved in BART-base/checkpoint-13000/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base/checkpoint-13000/tokenizer_config.json\n",
      "Special tokens file saved in BART-base/checkpoint-13000/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base/checkpoint-13500\n",
      "Configuration saved in BART-base/checkpoint-13500/config.json\n",
      "Model weights saved in BART-base/checkpoint-13500/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base/checkpoint-13500/tokenizer_config.json\n",
      "Special tokens file saved in BART-base/checkpoint-13500/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base/checkpoint-14000\n",
      "Configuration saved in BART-base/checkpoint-14000/config.json\n",
      "Model weights saved in BART-base/checkpoint-14000/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base/checkpoint-14000/tokenizer_config.json\n",
      "Special tokens file saved in BART-base/checkpoint-14000/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base/checkpoint-14500\n",
      "Configuration saved in BART-base/checkpoint-14500/config.json\n",
      "Model weights saved in BART-base/checkpoint-14500/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base/checkpoint-14500/tokenizer_config.json\n",
      "Special tokens file saved in BART-base/checkpoint-14500/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base/checkpoint-15000\n",
      "Configuration saved in BART-base/checkpoint-15000/config.json\n",
      "Model weights saved in BART-base/checkpoint-15000/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base/checkpoint-15000/tokenizer_config.json\n",
      "Special tokens file saved in BART-base/checkpoint-15000/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base/checkpoint-15500\n",
      "Configuration saved in BART-base/checkpoint-15500/config.json\n",
      "Model weights saved in BART-base/checkpoint-15500/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base/checkpoint-15500/tokenizer_config.json\n",
      "Special tokens file saved in BART-base/checkpoint-15500/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base/checkpoint-16000\n",
      "Configuration saved in BART-base/checkpoint-16000/config.json\n",
      "Model weights saved in BART-base/checkpoint-16000/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base/checkpoint-16000/tokenizer_config.json\n",
      "Special tokens file saved in BART-base/checkpoint-16000/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base/checkpoint-16500\n",
      "Configuration saved in BART-base/checkpoint-16500/config.json\n",
      "Model weights saved in BART-base/checkpoint-16500/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base/checkpoint-16500/tokenizer_config.json\n",
      "Special tokens file saved in BART-base/checkpoint-16500/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base/checkpoint-17000\n",
      "Configuration saved in BART-base/checkpoint-17000/config.json\n",
      "Model weights saved in BART-base/checkpoint-17000/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base/checkpoint-17000/tokenizer_config.json\n",
      "Special tokens file saved in BART-base/checkpoint-17000/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7202\n",
      "  Batch size = 16\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Saving model checkpoint to BART-base/checkpoint-19500\n",
      "Configuration saved in BART-base/checkpoint-19500/config.json\n",
      "Model weights saved in BART-base/checkpoint-19500/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base/checkpoint-19500/tokenizer_config.json\n",
      "Special tokens file saved in BART-base/checkpoint-19500/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base/checkpoint-20000\n",
      "Configuration saved in BART-base/checkpoint-20000/config.json\n",
      "Model weights saved in BART-base/checkpoint-20000/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base/checkpoint-20000/tokenizer_config.json\n",
      "Special tokens file saved in BART-base/checkpoint-20000/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base/checkpoint-20500\n",
      "Configuration saved in BART-base/checkpoint-20500/config.json\n",
      "Model weights saved in BART-base/checkpoint-20500/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base/checkpoint-20500/tokenizer_config.json\n",
      "Special tokens file saved in BART-base/checkpoint-20500/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base/checkpoint-21000\n",
      "Configuration saved in BART-base/checkpoint-21000/config.json\n",
      "Model weights saved in BART-base/checkpoint-21000/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base/checkpoint-21000/tokenizer_config.json\n",
      "Special tokens file saved in BART-base/checkpoint-21000/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base/checkpoint-21500\n",
      "Configuration saved in BART-base/checkpoint-21500/config.json\n",
      "Model weights saved in BART-base/checkpoint-21500/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base/checkpoint-21500/tokenizer_config.json\n",
      "Special tokens file saved in BART-base/checkpoint-21500/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base/checkpoint-22000\n",
      "Configuration saved in BART-base/checkpoint-22000/config.json\n",
      "Model weights saved in BART-base/checkpoint-22000/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base/checkpoint-22000/tokenizer_config.json\n",
      "Special tokens file saved in BART-base/checkpoint-22000/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base/checkpoint-22500\n",
      "Configuration saved in BART-base/checkpoint-22500/config.json\n",
      "Model weights saved in BART-base/checkpoint-22500/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base/checkpoint-22500/tokenizer_config.json\n",
      "Special tokens file saved in BART-base/checkpoint-22500/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base/checkpoint-23000\n",
      "Configuration saved in BART-base/checkpoint-23000/config.json\n",
      "Model weights saved in BART-base/checkpoint-23000/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base/checkpoint-23000/tokenizer_config.json\n",
      "Special tokens file saved in BART-base/checkpoint-23000/special_tokens_map.json\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Saving model checkpoint to BART-base/checkpoint-26000\n",
      "Configuration saved in BART-base/checkpoint-26000/config.json\n",
      "Model weights saved in BART-base/checkpoint-26000/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base/checkpoint-26000/tokenizer_config.json\n",
      "Special tokens file saved in BART-base/checkpoint-26000/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base/checkpoint-26500\n",
      "Configuration saved in BART-base/checkpoint-26500/config.json\n",
      "Model weights saved in BART-base/checkpoint-26500/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base/checkpoint-26500/tokenizer_config.json\n",
      "Special tokens file saved in BART-base/checkpoint-26500/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base/checkpoint-27000\n",
      "Configuration saved in BART-base/checkpoint-27000/config.json\n",
      "Model weights saved in BART-base/checkpoint-27000/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base/checkpoint-27000/tokenizer_config.json\n",
      "Special tokens file saved in BART-base/checkpoint-27000/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base/checkpoint-27500\n",
      "Configuration saved in BART-base/checkpoint-27500/config.json\n",
      "Model weights saved in BART-base/checkpoint-27500/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base/checkpoint-27500/tokenizer_config.json\n",
      "Special tokens file saved in BART-base/checkpoint-27500/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base/checkpoint-28000\n",
      "Configuration saved in BART-base/checkpoint-28000/config.json\n",
      "Model weights saved in BART-base/checkpoint-28000/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base/checkpoint-28000/tokenizer_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens file saved in BART-base/checkpoint-28000/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base/checkpoint-28500\n",
      "Configuration saved in BART-base/checkpoint-28500/config.json\n",
      "Model weights saved in BART-base/checkpoint-28500/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base/checkpoint-28500/tokenizer_config.json\n",
      "Special tokens file saved in BART-base/checkpoint-28500/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base/checkpoint-29000\n",
      "Configuration saved in BART-base/checkpoint-29000/config.json\n",
      "Model weights saved in BART-base/checkpoint-29000/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base/checkpoint-29000/tokenizer_config.json\n",
      "Special tokens file saved in BART-base/checkpoint-29000/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base/checkpoint-29500\n",
      "Configuration saved in BART-base/checkpoint-29500/config.json\n",
      "Model weights saved in BART-base/checkpoint-29500/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base/checkpoint-29500/tokenizer_config.json\n",
      "Special tokens file saved in BART-base/checkpoint-29500/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base/checkpoint-30000\n",
      "Configuration saved in BART-base/checkpoint-30000/config.json\n",
      "Model weights saved in BART-base/checkpoint-30000/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base/checkpoint-30000/tokenizer_config.json\n",
      "Special tokens file saved in BART-base/checkpoint-30000/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base/checkpoint-30500\n",
      "Configuration saved in BART-base/checkpoint-30500/config.json\n",
      "Model weights saved in BART-base/checkpoint-30500/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base/checkpoint-30500/tokenizer_config.json\n",
      "Special tokens file saved in BART-base/checkpoint-30500/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base/checkpoint-31000\n",
      "Configuration saved in BART-base/checkpoint-31000/config.json\n",
      "Model weights saved in BART-base/checkpoint-31000/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base/checkpoint-31000/tokenizer_config.json\n",
      "Special tokens file saved in BART-base/checkpoint-31000/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base/checkpoint-31500\n",
      "Configuration saved in BART-base/checkpoint-31500/config.json\n",
      "Model weights saved in BART-base/checkpoint-31500/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base/checkpoint-31500/tokenizer_config.json\n",
      "Special tokens file saved in BART-base/checkpoint-31500/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base/checkpoint-32000\n",
      "Configuration saved in BART-base/checkpoint-32000/config.json\n",
      "Model weights saved in BART-base/checkpoint-32000/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base/checkpoint-32000/tokenizer_config.json\n",
      "Special tokens file saved in BART-base/checkpoint-32000/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base/checkpoint-32500\n",
      "Configuration saved in BART-base/checkpoint-32500/config.json\n",
      "Model weights saved in BART-base/checkpoint-32500/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base/checkpoint-32500/tokenizer_config.json\n",
      "Special tokens file saved in BART-base/checkpoint-32500/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base/checkpoint-33000\n",
      "Configuration saved in BART-base/checkpoint-33000/config.json\n",
      "Model weights saved in BART-base/checkpoint-33000/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base/checkpoint-33000/tokenizer_config.json\n",
      "Special tokens file saved in BART-base/checkpoint-33000/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base/checkpoint-33500\n",
      "Configuration saved in BART-base/checkpoint-33500/config.json\n",
      "Model weights saved in BART-base/checkpoint-33500/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base/checkpoint-33500/tokenizer_config.json\n",
      "Special tokens file saved in BART-base/checkpoint-33500/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base/checkpoint-34000\n",
      "Configuration saved in BART-base/checkpoint-34000/config.json\n",
      "Model weights saved in BART-base/checkpoint-34000/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base/checkpoint-34000/tokenizer_config.json\n",
      "Special tokens file saved in BART-base/checkpoint-34000/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7202\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to BART-base/checkpoint-34500\n",
      "Configuration saved in BART-base/checkpoint-34500/config.json\n",
      "Model weights saved in BART-base/checkpoint-34500/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base/checkpoint-34500/tokenizer_config.json\n",
      "Special tokens file saved in BART-base/checkpoint-34500/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base/checkpoint-35000\n",
      "Configuration saved in BART-base/checkpoint-35000/config.json\n",
      "Model weights saved in BART-base/checkpoint-35000/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base/checkpoint-35000/tokenizer_config.json\n",
      "Special tokens file saved in BART-base/checkpoint-35000/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base/checkpoint-35500\n",
      "Configuration saved in BART-base/checkpoint-35500/config.json\n",
      "Model weights saved in BART-base/checkpoint-35500/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base/checkpoint-35500/tokenizer_config.json\n",
      "Special tokens file saved in BART-base/checkpoint-35500/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base/checkpoint-36000\n",
      "Configuration saved in BART-base/checkpoint-36000/config.json\n",
      "Model weights saved in BART-base/checkpoint-36000/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base/checkpoint-36000/tokenizer_config.json\n",
      "Special tokens file saved in BART-base/checkpoint-36000/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base/checkpoint-36500\n",
      "Configuration saved in BART-base/checkpoint-36500/config.json\n",
      "Model weights saved in BART-base/checkpoint-36500/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base/checkpoint-36500/tokenizer_config.json\n",
      "Special tokens file saved in BART-base/checkpoint-36500/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base/checkpoint-37000\n",
      "Configuration saved in BART-base/checkpoint-37000/config.json\n",
      "Model weights saved in BART-base/checkpoint-37000/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base/checkpoint-37000/tokenizer_config.json\n",
      "Special tokens file saved in BART-base/checkpoint-37000/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base/checkpoint-37500\n",
      "Configuration saved in BART-base/checkpoint-37500/config.json\n",
      "Model weights saved in BART-base/checkpoint-37500/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base/checkpoint-37500/tokenizer_config.json\n",
      "Special tokens file saved in BART-base/checkpoint-37500/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base/checkpoint-38000\n",
      "Configuration saved in BART-base/checkpoint-38000/config.json\n",
      "Model weights saved in BART-base/checkpoint-38000/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base/checkpoint-38000/tokenizer_config.json\n",
      "Special tokens file saved in BART-base/checkpoint-38000/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base/checkpoint-38500\n",
      "Configuration saved in BART-base/checkpoint-38500/config.json\n",
      "Model weights saved in BART-base/checkpoint-38500/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base/checkpoint-38500/tokenizer_config.json\n",
      "Special tokens file saved in BART-base/checkpoint-38500/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base/checkpoint-39000\n",
      "Configuration saved in BART-base/checkpoint-39000/config.json\n",
      "Model weights saved in BART-base/checkpoint-39000/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base/checkpoint-39000/tokenizer_config.json\n",
      "Special tokens file saved in BART-base/checkpoint-39000/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base/checkpoint-39500\n",
      "Configuration saved in BART-base/checkpoint-39500/config.json\n",
      "Model weights saved in BART-base/checkpoint-39500/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base/checkpoint-39500/tokenizer_config.json\n",
      "Special tokens file saved in BART-base/checkpoint-39500/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base/checkpoint-40000\n",
      "Configuration saved in BART-base/checkpoint-40000/config.json\n",
      "Model weights saved in BART-base/checkpoint-40000/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer config file saved in BART-base/checkpoint-40000/tokenizer_config.json\n",
      "Special tokens file saved in BART-base/checkpoint-40000/special_tokens_map.json\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Saving model checkpoint to BART-base/checkpoint-48500\n",
      "Configuration saved in BART-base/checkpoint-48500/config.json\n",
      "Model weights saved in BART-base/checkpoint-48500/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base/checkpoint-48500/tokenizer_config.json\n",
      "Special tokens file saved in BART-base/checkpoint-48500/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base/checkpoint-49000\n",
      "Configuration saved in BART-base/checkpoint-49000/config.json\n",
      "Model weights saved in BART-base/checkpoint-49000/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base/checkpoint-49000/tokenizer_config.json\n",
      "Special tokens file saved in BART-base/checkpoint-49000/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base/checkpoint-49500\n",
      "Configuration saved in BART-base/checkpoint-49500/config.json\n",
      "Model weights saved in BART-base/checkpoint-49500/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base/checkpoint-49500/tokenizer_config.json\n",
      "Special tokens file saved in BART-base/checkpoint-49500/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base/checkpoint-50000\n",
      "Configuration saved in BART-base/checkpoint-50000/config.json\n",
      "Model weights saved in BART-base/checkpoint-50000/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base/checkpoint-50000/tokenizer_config.json\n",
      "Special tokens file saved in BART-base/checkpoint-50000/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base/checkpoint-50500\n",
      "Configuration saved in BART-base/checkpoint-50500/config.json\n",
      "Model weights saved in BART-base/checkpoint-50500/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base/checkpoint-50500/tokenizer_config.json\n",
      "Special tokens file saved in BART-base/checkpoint-50500/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base/checkpoint-51000\n",
      "Configuration saved in BART-base/checkpoint-51000/config.json\n",
      "Model weights saved in BART-base/checkpoint-51000/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base/checkpoint-51000/tokenizer_config.json\n",
      "Special tokens file saved in BART-base/checkpoint-51000/special_tokens_map.json\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-3435b262f1ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1498\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inner_training_loop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_find_batch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1499\u001b[0m         )\n\u001b[0;32m-> 1500\u001b[0;31m         return inner_training_loop(\n\u001b[0m\u001b[1;32m   1501\u001b[0m             \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1502\u001b[0m             \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1740\u001b[0m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1742\u001b[0;31m                     \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1743\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1744\u001b[0m                 if (\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2502\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepspeed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2503\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2504\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2505\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2506\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \"\"\"\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    126\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 7202\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.4164099395275116}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Seq2SeqTrainer' object has no attribute 'plot'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-7f90a5714312>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'Seq2SeqTrainer' object has no attribute 'plot'"
     ]
    }
   ],
   "source": [
    "trainer.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
