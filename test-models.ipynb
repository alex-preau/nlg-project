{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing all models\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metrics: LM loss, Rouge-1, Rouge-2, BERT-score, self Rouge 1, self similarity, syllable count per line, syllable difference per line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartTokenizer, BartModel,BartForConditionalGeneration\n",
    "from torch.distributions import Categorical\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from undecorated import undecorated\n",
    "import numpy as np\n",
    "import evaluate\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from transformers import  Seq2SeqTrainingArguments, Seq2SeqTrainer,DataCollatorForSeq2Seq\n",
    "import torch.optim as optim\n",
    "import torchvision.utils as vutils\n",
    "import types\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.distributions import Categorical\n",
    "import random\n",
    "from collections import Counter\n",
    "from nltk.tokenize import SyllableTokenizer\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import PoemDataset,encode_sentences_wsyl,encode_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gan_utils import Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = evaluate.load('rouge')\n",
    "bert_score = evaluate.load('bertscore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_generator =  Generator().cuda()#BartForConditionalGeneration.from_pretrained('facebook/bart-base').cuda()\n",
    "#model = TheModelClass(*args, **kwargs)\n",
    "test_generator.load_state_dict(torch.load('/home/alexander/nlg-project/GAN_models_finetuned/' + 'four_it_ft_generator_bad.pt')['model_state_dict'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(\"study\", return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<pad><s> need to study\\nand return the power\\nof my day</s>']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(test_generator.model.eval().cpu().generate(inputs['input_ids'],attention_mask=inputs[\"attention_mask\"],max_length=32, use_cache=True,\n",
    "        decoder_start_token_id = tokenizer.pad_token_id,\n",
    "        num_beams= 100,\n",
    "        early_stopping = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"<pad><s>'s the romantic of a whore\\nthis summer whenever you have a\\nthis financial system of societies that would have you if I had to settle for</s>\"]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(test_generator.model.eval().cpu().generate(inputs['input_ids'],attention_mask=inputs[\"attention_mask\"],max_length=32, use_cache=True,\n",
    "        decoder_start_token_id = tokenizer.pad_token_id,\n",
    "        num_beams= 100,\n",
    "        early_stopping = True))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_finetuned = BartForConditionalGeneration.from_pretrained('/home/alexander/nlg-project/BART-base/' + 'checkpoint-25000')\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(\"happy\", return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<pad><s> happy new year to\\nyou and your family i\\nlove you so so much</s>']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(gen_finetuned.eval().cpu().generate(inputs['input_ids'],attention_mask=inputs[\"attention_mask\"],max_length=32, use_cache=True,\n",
    "        decoder_start_token_id = tokenizer.pad_token_id,\n",
    "        num_beams= 100,\n",
    "        early_stopping = True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def syllable_vec(row):\n",
    "    return [row['line_0_scount'],row['line_1_scount'],row['line_2_scount']]\n",
    "\n",
    "def change_title(row):\n",
    "    #adds syllable info to title\n",
    "    #print(row['Title'])\n",
    "    return str(row['syllables']) + ';' + str(row['Title'])\n",
    "\n",
    "def add_syllables_title(part_df, match_df):\n",
    "    merged = pd.merge(part_df, match_df,how='left', left_on=['Unnamed: 0'], right_index=True)\n",
    "    merged['syllables'] = merged.apply(syllable_vec,axis=1)\n",
    "    merged.drop(columns=['Unnamed: 0_x', 'Unnamed: 0_y','line_0','line_1','line_2','source','valid','line_0_scount',\n",
    "                        'line_1_scount','line_2_scount'],inplace=True)\n",
    "    merged['Title'] = merged.apply(change_title,axis=1)\n",
    "    \n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data_dir =  '/home/alexander/nlg-project/data/'\n",
    "test_df = pd.read_csv(processed_data_dir + 'test_data.csv')\n",
    "whole_df = pd.read_csv(processed_data_dir + 'kaggle_data.csv')\n",
    "test_df = add_syllables_title(test_df,whole_df)\n",
    "test_model_syllables = encode_sentences_wsyl(tokenizer,test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data_dir =  '/home/alexander/nlg-project/data/'\n",
    "test_df = pd.read_csv(processed_data_dir + 'test_data.csv')\n",
    "whole_df = pd.read_csv(processed_data_dir + 'kaggle_data.csv')\n",
    "test_df = add_syllables_title(test_df,whole_df)\n",
    "test_model_syllables = encode_sentences_wsyl(tokenizer,test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_df = pd.read_csv(processed_data_dir + 'test_data.csv')\n",
    "test_model_plain = encode_sentences(tokenizer,test_df)\n",
    "#test_model_plain_mle = encode_sentences_mle(tokenizer,test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "class PoemDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.labels = df['labels']\n",
    "        self.mask = df['attention_mask']\n",
    "        self.input = df['input_ids']\n",
    "        self.decoder_mask = df['decoder_attention_mask']\n",
    "        #self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        input_ids = self.input[idx]\n",
    "        attention_masks = self.mask[idx]\n",
    "        target_ids = self.labels[idx]\n",
    "        decoder_masks = self.decoder_mask[idx  ]             \n",
    "        batch = {\n",
    "          \"input_ids\": input_ids,\n",
    "         # \"decoder_attention_mask\": decoder_masks,\n",
    "          \"attention_mask\": attention_masks,\n",
    "          \"label_ids\": target_ids,\n",
    "        }\n",
    "        return batch\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_ds_syllables = PoemDataset(test_model_syllables)\n",
    "eval_ds_plain = PoemDataset(test_model_plain)\n",
    "#eval_ds_plain_mle = PoemDataset(test_model_plain_mle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    0,   608,    24, 47320, 50118,  5016,    24,     7,  2841,  2841,\n",
       "          608, 50118,   405,   473,    24,     7,  2841,     2,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_ds_plain.__getitem__(0)['label_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_ds_plain.__getitem__(0)['decoder_attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class test_generator():\n",
    "    #tests the generator on several metrics\n",
    "    def __init__(self,path,ds):\n",
    "        test_generator =  Generator().cuda()#BartForConditionalGeneration.from_pretrained('facebook/bart-base').cuda()\n",
    "        #model = TheModelClass(*args, **kwargs)\n",
    "        test_generator.load_state_dict(torch.load(path)['model_state_dict'])\n",
    "\n",
    "        self.ds = ds\n",
    "    \n",
    "        self.model = test_generator.model\n",
    "        batch_size = 16 \n",
    "\n",
    "        args = Seq2SeqTrainingArguments(\n",
    "            \"Gen-testing\",\n",
    "            evaluation_strategy = \"epoch\",\n",
    "            #learning_rate=2e-5,\n",
    "            per_device_train_batch_size=batch_size,\n",
    "            per_device_eval_batch_size=batch_size,\n",
    "           # weight_decay=0.01,\n",
    "           # save_total_limit=3,\n",
    "           # num_train_epochs=1,\n",
    "           # predict_with_generate=True,\n",
    "\n",
    "            learning_rate=1e-4,\n",
    "            weight_decay=0.01,\n",
    "            adam_beta1=0.9,\n",
    "            adam_beta2=0.999,\n",
    "            adam_epsilon=1e-6,\n",
    "            max_grad_norm=1.0,\n",
    "            num_train_epochs=10,\n",
    "            lr_scheduler_type=\"linear\",\n",
    "            warmup_ratio=0.1,\n",
    "        )\n",
    "        \n",
    "        self.trainer = Seq2SeqTrainer(\n",
    "            model=self.model,\n",
    "            args=args,\n",
    "            eval_dataset=ds,\n",
    "            #use_auth_token=False,\n",
    "\n",
    "            tokenizer=tokenizer\n",
    "        )\n",
    "        \n",
    "    def generate_samples(self):\n",
    "        #generates \n",
    "        dl = DataLoader(self.ds,batch_size=16)\n",
    "        self.generations = []\n",
    "        self.generated_tokens = []\n",
    "        self.references = []\n",
    "        \n",
    "        self.titles = []\n",
    "        #generate 480 samples\n",
    "        for i,inputs in enumerate(dl):\n",
    "          #  print(inputs)\n",
    "           # inputs = inputs.cuda()\n",
    "            new_poem = ((self.model.eval().generate(inputs['input_ids'].cuda(),attention_mask=inputs[\"attention_mask\"].cuda(),max_length=32, use_cache=True,\n",
    "                decoder_start_token_id = tokenizer.pad_token_id, num_beams= 100,early_stopping = True)))\n",
    "            self.generated_tokens.append(new_poem)\n",
    "            self.generations.append(tokenizer.batch_decode(new_poem,skip_special_tokens=True))\n",
    "            \n",
    "            self.references.append(tokenizer.batch_decode(inputs['label_ids'],skip_special_tokens=True))\n",
    "            \n",
    "            self.titles.append(tokenizer.batch_decode(inputs['input_ids'],skip_special_tokens=True))\n",
    "            if i > 30:\n",
    "                break\n",
    "           # print(generated)\\\n",
    "        self.generations = [item for sublist in self.generations for item in sublist] \n",
    "        self.references = [item for sublist in self.references for item in sublist] \n",
    "        self.generated_tokens = [item for sublist in self.generated_tokens for item in sublist] \n",
    "        self.titles = [item for sublist in self.titles for item in sublist] \n",
    "       # rouge_out = metric.compute(predictions=self.generations, references=self.references)\n",
    "       # print(rouge_out)\n",
    "    \n",
    "    def rouge(self):\n",
    "        rouge_out = rouge.compute(predictions=self.generations, references=self.references)\n",
    "        print(rouge_out)\n",
    "        \n",
    "    def bert_score(self):\n",
    "        bert_out = bert_score.compute(predictions=self.generations, references=self.references,lang='en')\n",
    "        print(\"BertScore f1 mean: \",np.mean(bert_out['f1']))\n",
    "        print(\"BertScore recall mean: \",np.mean(bert_out['recall']))\n",
    "        print(\"BertScore precision mean: \",np.mean(bert_out['precision']))\n",
    "        \n",
    "    def auto_rouge(self):\n",
    "        #computes the mean rouge-1 and rouge-2 between each generation and the restt of the generations\n",
    "        #quantifies mode collapse \n",
    "        self.auto_rouge1s = []\n",
    "        self.auto_rouge2s = []\n",
    "        \n",
    "        for i in range(len(self.generations)):\n",
    "            \n",
    "            \n",
    "            refs = self.generations[:i] + self.generations[i+1:]\n",
    "            pred = [self.generations[i]] * len(refs)\n",
    "          #  print(pred)\n",
    "\n",
    "            rouge_out = rouge.compute(predictions=pred, references=refs)\n",
    "            self.auto_rouge1s.append(rouge_out['rouge1'])\n",
    "            self.auto_rouge2s.append(rouge_out['rouge2'])\n",
    "        print(\"Auto Rouge-1 mean: \",np.mean(self.auto_rouge1s))\n",
    "        print(\"Auto Rouge-2 mean: \",np.mean(self.auto_rouge2s))\n",
    "    \n",
    "    def self_similarity(self):\n",
    "        #computes the mean self-similarity of each generation\n",
    "        #note this is based on TOKENS and not words\n",
    "        #similarity is (# max same tokens) (# tokens) where tokens do not inclue start, pad, end, or newline\n",
    "        bad_tokens = [0,1,2,50118]\n",
    "        self.auto_similarity = []\n",
    "        for i in range(len(self.generated_tokens)):\n",
    "            curr_gen = self.generated_tokens[i].cpu().tolist()\n",
    "            counts = Counter(curr_gen).most_common()\n",
    "            max_tokens = -1\n",
    "            token_count = 0\n",
    "            for token,c in counts:\n",
    "                if token not in bad_tokens:\n",
    "                    if max_tokens == -1:\n",
    "                        max_tokens = c\n",
    "                    token_count += c\n",
    "            self.auto_similarity.append(max_tokens/token_count)\n",
    "        print(\"Self similarity mean: \",np.mean(self.auto_similarity))\n",
    "            \n",
    "        \n",
    "    def test_generation(self):\n",
    "        #tests the model at the standard generation task, returning the huggingface generator loss \n",
    "        print(self.trainer.evaluate())\n",
    "        \n",
    "    def print_generations(self):\n",
    "        #prints the first 20 generations and coprresponding titles\n",
    "        dl = DataLoader(self.ds,batch_size=16)\n",
    "        generations = []\n",
    "        titles = []\n",
    "        references = []\n",
    "        \n",
    "        for i,inputs in enumerate(dl):\n",
    "          #  print(inputs)\n",
    "           # inputs = inputs.cuda()\n",
    "            new_poem = ((self.model.eval().generate(inputs['input_ids'].cuda(),attention_mask=inputs[\"attention_mask\"].cuda(),max_length=32, use_cache=True,\n",
    "                decoder_start_token_id = tokenizer.pad_token_id, num_beams= 100,early_stopping = True)))\n",
    "            \n",
    "            generations.append(tokenizer.batch_decode(new_poem,skip_special_tokens=True))\n",
    "            titles.append(tokenizer.batch_decode(inputs['input_ids'],skip_special_tokens=True))\n",
    "            references.append(tokenizer.batch_decode(inputs['label_ids'],skip_special_tokens=True))\n",
    "            if i > 2:\n",
    "                break\n",
    "           # print(generated)\\\n",
    "        generations = [item for sublist in generations for item in sublist] \n",
    "        references = [item for sublist in references for item in sublist] \n",
    "        titles = [item for sublist in titles for item in sublist] \n",
    "        print('generations')\n",
    "        print(generations[0:20])\n",
    "        print('titles')\n",
    "        print(titles[0:20])\n",
    "        print('references')\n",
    "        print(references[0:20])\n",
    "        \n",
    "    def test_syllables_general(self):\n",
    "        #returns the mean and std of sllables of each line in generation\n",
    "        #usefull where model is not conditioned on syllables\n",
    "        self.syllables = {}\n",
    "        self.syllables[0] = []\n",
    "        self.syllables[1] = []\n",
    "        self.syllables[2] = []\n",
    "        \n",
    "        for poem in self.generations:\n",
    "            lines = poem.split('\\n')\n",
    "            SSP = SyllableTokenizer()\n",
    "            for i,l in enumerate(lines):\n",
    "                if i < 3:\n",
    "                    #else too many new lines, bad generation\n",
    "                    tokens = SSP.tokenize(l.strip())\n",
    "                 #   print(tokens)\n",
    "                    self.syllables[i].append(len(tokens))\n",
    "        for line_num in range(3):\n",
    "            print('mean syl line ' + str(line_num) + ' : ' + str(np.mean(self.syllables[line_num])))\n",
    "            print('std syl line ' + str(line_num) + ' : ' + str(np.std(self.syllables[line_num])))\n",
    "                    \n",
    "    def test_syllables_specific(self,is_valid):\n",
    "        #returns the mean diff and std between syllables of generations and input desired syllables \n",
    "        #if this is part of the model\n",
    "        if not is_valid:\n",
    "            return\n",
    "        \n",
    "        print(\"Testing Specific Syllables\")\n",
    "        self.syllable_diff = {}\n",
    "        self.syllable_diff[0] = []\n",
    "        self.syllable_diff[1]  = []\n",
    "        self.syllable_diff[2]  = []\n",
    "        #cannot use pregenerated syllables incase some are missing \n",
    "        \n",
    "        for j,poem in enumerate(self.generations):\n",
    "            lines = poem.split('\\n')\n",
    "            SSP = SyllableTokenizer()\n",
    "\n",
    "            title = self.titles[j]\n",
    "\n",
    "            syllables = ast.literal_eval(str(self.titles[j]).split(';')[0].strip())\n",
    "        #    print(syllables)\n",
    "            for i,l in enumerate(lines):\n",
    "                if i < 3:\n",
    "                    #else too many new lines, bad generation\n",
    "                    tokens = SSP.tokenize(l.strip())\n",
    "                 #   print(tokens)\n",
    "                    self.syllable_diff[i].append(np.abs(syllables[i] - len(tokens)))\n",
    "        for line_num in range(3):\n",
    "            print('mean difference syl line ' + str(line_num) + ' : ' + str(np.mean(self.syllable_diff[line_num])))\n",
    "            print('std difference syl line ' + str(line_num) + ' : ' + str(np.std(self.syllable_diff[line_num])))\n",
    "                    \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def test(self,conditioned_syllables=False):\n",
    "        self.test_generation()\n",
    "        self.generate_samples()\n",
    "        self.test_syllables_general()\n",
    "        self.test_syllables_specific(conditioned_syllables)\n",
    "        self.rouge()\n",
    "        self.bert_score()\n",
    "        self.auto_rouge()\n",
    "        self.self_similarity()\n",
    "        self.print_generations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class test_finetuned():\n",
    "    #tests the generator on several metrics\n",
    "    def __init__(self,path,ds):\n",
    "\n",
    "        self.ds = ds\n",
    "        self.model = BartForConditionalGeneration.from_pretrained(path).cuda()\n",
    "\n",
    "        batch_size = 16 \n",
    "\n",
    "        args = Seq2SeqTrainingArguments(\n",
    "            \"Gen-testing\",\n",
    "            evaluation_strategy = \"epoch\",\n",
    "            #learning_rate=2e-5,\n",
    "            per_device_train_batch_size=batch_size,\n",
    "            per_device_eval_batch_size=batch_size,\n",
    "           # weight_decay=0.01,\n",
    "           # save_total_limit=3,\n",
    "           # num_train_epochs=1,\n",
    "           # predict_with_generate=True,\n",
    "\n",
    "            learning_rate=1e-4,\n",
    "            weight_decay=0.01,\n",
    "            adam_beta1=0.9,\n",
    "            adam_beta2=0.999,\n",
    "            adam_epsilon=1e-6,\n",
    "            max_grad_norm=1.0,\n",
    "            num_train_epochs=10,\n",
    "            lr_scheduler_type=\"linear\",\n",
    "            warmup_ratio=0.1,\n",
    "        )\n",
    "        \n",
    "        self.trainer = Seq2SeqTrainer(\n",
    "            model=self.model,\n",
    "            args=args,\n",
    "            eval_dataset=ds,\n",
    "            #use_auth_token=False,\n",
    "\n",
    "            tokenizer=tokenizer\n",
    "        )\n",
    "        \n",
    "    def generate_samples(self):\n",
    "        #generates \n",
    "        dl = DataLoader(self.ds,batch_size=16)\n",
    "        self.generations = []\n",
    "        self.generated_tokens = []\n",
    "        self.references = []\n",
    "        self.titles = []\n",
    "        #generate 480 samples\n",
    "        for i,inputs in enumerate(dl):\n",
    "          #  print(inputs)\n",
    "           # inputs = inputs.cuda()\n",
    "            new_poem = ((self.model.eval().generate(inputs['input_ids'].cuda(),attention_mask=inputs[\"attention_mask\"].cuda(),max_length=32, use_cache=True,\n",
    "                decoder_start_token_id = tokenizer.pad_token_id, num_beams= 100,early_stopping = True)))\n",
    "            self.generated_tokens.append(new_poem)\n",
    "            self.generations.append(tokenizer.batch_decode(new_poem,skip_special_tokens=True))\n",
    "            self.references.append(tokenizer.batch_decode(inputs['label_ids'],skip_special_tokens=True))\n",
    "            self.titles.append(tokenizer.batch_decode(inputs['input_ids'],skip_special_tokens=True))\n",
    "            if i > 30:\n",
    "                break\n",
    "           # print(generated)\\\n",
    "        self.generations = [item for sublist in self.generations for item in sublist] \n",
    "        self.references = [item for sublist in self.references for item in sublist] \n",
    "        self.generated_tokens = [item for sublist in self.generated_tokens for item in sublist] \n",
    "        self.titles = [item for sublist in self.titles for item in sublist] \n",
    "       # rouge_out = metric.compute(predictions=self.generations, references=self.references)\n",
    "       # print(rouge_out)\n",
    "    \n",
    "    def rouge(self):\n",
    "        rouge_out = rouge.compute(predictions=self.generations, references=self.references)\n",
    "        print(rouge_out)\n",
    "        \n",
    "    def bert_score(self):\n",
    "        bert_out = bert_score.compute(predictions=self.generations, references=self.references,lang='en')\n",
    "        print(\"BertScore f1 mean: \",np.mean(bert_out['f1']))\n",
    "        print(\"BertScore recall mean: \",np.mean(bert_out['recall']))\n",
    "        print(\"BertScore precision mean: \",np.mean(bert_out['precision']))\n",
    "        \n",
    "    def auto_rouge(self):\n",
    "        #computes the mean rouge-1 and rouge-2 between each generation and the restt of the generations\n",
    "        #quantifies mode collapse \n",
    "        self.auto_rouge1s = []\n",
    "        self.auto_rouge2s = []\n",
    "        \n",
    "        for i in range(len(self.generations)):\n",
    "            \n",
    "            \n",
    "            refs = self.generations[:i] + self.generations[i+1:]\n",
    "            pred = [self.generations[i]] * len(refs)\n",
    "          #  print(pred)\n",
    "\n",
    "            rouge_out = rouge.compute(predictions=pred, references=refs)\n",
    "            self.auto_rouge1s.append(rouge_out['rouge1'])\n",
    "            self.auto_rouge2s.append(rouge_out['rouge2'])\n",
    "        print(\"Auto Rouge-1 mean: \",np.mean(self.auto_rouge1s))\n",
    "        print(\"Auto Rouge-2 mean: \",np.mean(self.auto_rouge2s))\n",
    "    \n",
    "    def self_similarity(self):\n",
    "        #computes the mean self-similarity of each generation\n",
    "        #note this is based on TOKENS and not words\n",
    "        #similarity is (# max same tokens) (# tokens) where tokens do not inclue start, pad, end, or newline\n",
    "        bad_tokens = [0,1,2,50118]\n",
    "        self.auto_similarity = []\n",
    "        for i in range(len(self.generated_tokens)):\n",
    "            curr_gen = self.generated_tokens[i].cpu().tolist()\n",
    "            counts = Counter(curr_gen).most_common()\n",
    "            max_tokens = -1\n",
    "            token_count = 0\n",
    "            for token,c in counts:\n",
    "                if token not in bad_tokens:\n",
    "                    if max_tokens == -1:\n",
    "                        max_tokens = c\n",
    "                    token_count += c\n",
    "            self.auto_similarity.append(max_tokens/token_count)\n",
    "        print(\"Self similarity mean: \",np.mean(self.auto_similarity))\n",
    "            \n",
    "        \n",
    "    def test_generation(self):\n",
    "        #tests the model at the standard generation task, returning the huggingface generator loss \n",
    "        print(self.trainer.evaluate())\n",
    "    def print_generations(self):\n",
    "        #prints the first 48 generations and coprresponding titles\n",
    "        dl = DataLoader(self.ds,batch_size=16)\n",
    "        generations = []\n",
    "        titles = []\n",
    "        references = []\n",
    "        \n",
    "        for i,inputs in enumerate(dl):\n",
    "          #  print(inputs)\n",
    "           # inputs = inputs.cuda()\n",
    "            new_poem = ((self.model.eval().generate(inputs['input_ids'].cuda(),attention_mask=inputs[\"attention_mask\"].cuda(),max_length=32, use_cache=True,\n",
    "                decoder_start_token_id = tokenizer.pad_token_id, num_beams= 100,early_stopping = True)))\n",
    "            \n",
    "            generations.append(tokenizer.batch_decode(new_poem,skip_special_tokens=True))\n",
    "            titles.append(tokenizer.batch_decode(inputs['input_ids'],skip_special_tokens=True))\n",
    "            references.append(tokenizer.batch_decode(inputs['label_ids'],skip_special_tokens=True))\n",
    "            if i > 2:\n",
    "                break\n",
    "           # print(generated)\\\n",
    "        generations = [item for sublist in generations for item in sublist] \n",
    "        references = [item for sublist in references for item in sublist] \n",
    "        titles = [item for sublist in titles for item in sublist] \n",
    "        print('generations')\n",
    "        print(generations)\n",
    "        print('titles')\n",
    "        print(titles)\n",
    "        print('references')\n",
    "        print(references)\n",
    "        \n",
    "    def test_syllables_general(self):\n",
    "        #returns the mean and std of sllables of each line in generation\n",
    "        #usefull where model is not conditioned on syllables\n",
    "        self.syllables = {}\n",
    "        self.syllables[0] = []\n",
    "        self.syllables[1] = []\n",
    "        self.syllables[2] = []\n",
    "        \n",
    "        for poem in self.generations:\n",
    "            lines = poem.split('\\n')\n",
    "            SSP = SyllableTokenizer()\n",
    "            for i,l in enumerate(lines):\n",
    "                if i < 3:\n",
    "                    #else too many new lines, bad generation\n",
    "                    tokens = SSP.tokenize(l.strip())\n",
    "                   # print(tokens)\n",
    "                    self.syllables[i].append(len(tokens))\n",
    "        for line_num in range(3):\n",
    "            print('mean syl line ' + str(line_num) + ' : ' + str(np.mean(self.syllables[line_num])))\n",
    "            print('std syl line ' + str(line_num) + ' : ' + str(np.std(self.syllables[line_num])))\n",
    "                    \n",
    "    def test_syllables_specific(self,is_valid):\n",
    "        #returns the mean diff and std between syllables of generations and input desired syllables \n",
    "        #if this is part of the model\n",
    "        if not is_valid:\n",
    "            return\n",
    "        \n",
    "        print(\"Testing Specific Syllables\")\n",
    "        self.syllable_diff = {}\n",
    "        self.syllable_diff[0] = []\n",
    "        self.syllable_diff[1]  = []\n",
    "        self.syllable_diff[2]  = []\n",
    "        #cannot use pregenerated syllables incase some are missing \n",
    "        \n",
    "        for j,poem in enumerate(self.generations):\n",
    "            lines = poem.split('\\n')\n",
    "            SSP = SyllableTokenizer()\n",
    "\n",
    "            title = self.titles[j]\n",
    "\n",
    "            syllables = ast.literal_eval(str(self.titles[j]).split(';')[0].strip())\n",
    "           # print(syllables)\n",
    "            for i,l in enumerate(lines):\n",
    "                if i < 3:\n",
    "                    #else too many new lines, bad generation\n",
    "                    tokens = SSP.tokenize(l.strip())\n",
    "                 #   print(tokens)\n",
    "                    self.syllable_diff[i].append(np.abs(syllables[i] - len(tokens)))\n",
    "        for line_num in range(3):\n",
    "            print('mean difference syl line ' + str(line_num) + ' : ' + str(np.mean(self.syllable_diff[line_num])))\n",
    "            print('std difference syl line ' + str(line_num) + ' : ' + str(np.std(self.syllable_diff[line_num])))\n",
    "                    \n",
    "        \n",
    "        \n",
    "\n",
    "       # rouge_out = metric.compute(predictions=self.generations, references=self.references)\n",
    "        \n",
    "    def test(self,conditioned_syllables=False):\n",
    "        self.test_generation()\n",
    "        self.generate_samples()\n",
    "        self.test_syllables_general()\n",
    "        self.test_syllables_specific(conditioned_syllables)\n",
    "        self.rouge()\n",
    "        self.bert_score()\n",
    "        self.auto_rouge()\n",
    "        self.self_similarity()\n",
    "        self.print_generations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/alexander/.cache/huggingface/hub/models--facebook--bart-base/snapshots/aadd2ab0ae0c8268c7c9693540e9904811f36177/config.json\n",
      "Model config BartConfig {\n",
      "  \"_name_or_path\": \"bart-base\",\n",
      "  \"activation_dropout\": 0.1,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.1,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_attention_heads\": 12,\n",
      "  \"decoder_ffn_dim\": 3072,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 12,\n",
      "  \"encoder_ffn_dim\": 3072,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": false,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 128,\n",
      "      \"min_length\": 12,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_cnn\": {\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 142,\n",
      "      \"min_length\": 56,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_xsum\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 62,\n",
      "      \"min_length\": 11,\n",
      "      \"num_beams\": 6\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/alexander/.cache/huggingface/hub/models--facebook--bart-base/snapshots/aadd2ab0ae0c8268c7c9693540e9904811f36177/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BartForConditionalGeneration.\n",
      "\n",
      "All the weights of BartForConditionalGeneration were initialized from the model checkpoint at facebook/bart-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7202\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='451' max='451' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [451/451 00:34]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 38.80013656616211, 'eval_runtime': 34.3048, 'eval_samples_per_second': 209.942, 'eval_steps_per_second': 13.147}\n",
      "mean syl line 0 : 8.177734375\n",
      "std syl line 0 : 1.7448731664345576\n",
      "mean syl line 1 : nan\n",
      "std syl line 1 : nan\n",
      "mean syl line 2 : nan\n",
      "std syl line 2 : nan\n",
      "Testing Specific Syllables\n",
      "mean difference syl line 0 : 2.48046875\n",
      "std difference syl line 0 : 1.7521218651319426\n",
      "mean difference syl line 1 : nan\n",
      "std difference syl line 1 : nan\n",
      "mean difference syl line 2 : nan\n",
      "std difference syl line 2 : nan\n",
      "{'rouge1': 0.0798149296, 'rouge2': 0.0012463363578189897, 'rougeL': 0.0719680996364451, 'rougeLsum': 0.07726495970184319}\n",
      "BertScore f1 mean:  0.8154970010509714\n",
      "BertScore recall mean:  0.7951456626178697\n",
      "BertScore precision mean:  0.837202305556275\n",
      "Auto Rouge-1 mean:  0.5098428056556263\n",
      "Auto Rouge-2 mean:  0.4020239624103066\n",
      "Self similarity mean:  0.19889322916666669\n",
      "generations\n",
      "[' not the day when i', ' not the day when i', ' they, that I should', ' not the day when i', ' not the day when i', ' they, that I should', ' they, that I should', ' not the day when I', ' they, that I should', ' not the day when i', ' unfortunately the night meal is', ' not the day that was', ' they, that I should', ' not the day when i', ' not the day when i', ' not the day when i', ' not the night that was', ' not the day when I', ' not the day when i', ' unfortunately the night meal is']\n",
      "titles\n",
      "[' [4, 8, 7];em doesnt', ' [6, 9, 5];aller sickness', ' [5, 8, 7];mississippi tornado', ' [23, 25, 5];physician moreover', ' [8, 10, 8];marlon leagues', ' [5, 9, 6];professor hour', ' [5, 10, 4];teamwork warriors', ' [6, 10, 5];connect kills', ' [5, 12, 8];stupidity max', ' [6, 11, 7];suggest ourselves', ' [7, 6, 4];changeful rhythm', ' [2, 2, 2];dream', ' [5, 10, 7];neglecting log', ' [6, 7, 7];early off', ' [6, 9, 6];talks close', ' [6, 9, 8];whack texas', ' [9, 10, 11];trialflames offenders', ' [6, 10, 6];betrayal heartbreak', ' [6, 10, 6];canvas list', ' [7, 10, 8];mallard glimpse']\n",
      "references\n",
      "[' doing it doesnt\\ndo it to em em doing\\nit does it to em', ' im starting a gang\\nto fight the summer sickness\\naller g unit', ' tornado warning\\nfor mississippi county\\neveryone stay safe', ' the two lines may be translated love is the physician of life\\nand next to our lord himself moreover it is the way that goes\\nstraight to heaven', ' of course im facing\\nmarlon mack in both of my\\nmoney leagues this week', ' our professor\\njust let us out an hour\\nearly god is good', ' congratulations\\non the win awesome teamwork\\nlady warriors', ' feeling very dumb\\nas the other half kills it\\nat only connect', ' stupidity at\\nits max youre a dumb bitch if\\nyou do stuff like this', ' i suggest we learn\\nto love ourselves before\\nits made illegal', ' waiting for you\\nthe changeful rhythm\\nof the rain', ' the dream\\nthe dream\\nthe dream', ' finally getting\\nmy life ive been neglecting\\nmyself for too log', ' i get off early\\ntoday but i really dont\\neven want to go', ' i wont ever let\\nsomeone who talks bad about\\nme be close to me', ' texas living is\\nso whack all they do for fun\\nis go out and drink', ' when offenders are indited\\nand with trialflames ignited\\nelsewhere ill attend if cited', ' heartbreak is the worst\\nyou really feel it in your\\nchest the betrayal', ' looking at my long\\nto do list in canvas makes\\nme want to throw up', ' golden waters still\\nautumn leaves fading away\\nglimpse of mallard ducks']\n"
     ]
    }
   ],
   "source": [
    "finetuned_generator_tester = test_generator('/home/alexander/nlg-project/GAN_models_syllables/' + 'generator.pt',eval_ds_syllables)\n",
    "finetuned_generator_tester.test(conditioned_syllables=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/alexander/.cache/huggingface/hub/models--facebook--bart-base/snapshots/aadd2ab0ae0c8268c7c9693540e9904811f36177/config.json\n",
      "Model config BartConfig {\n",
      "  \"_name_or_path\": \"bart-base\",\n",
      "  \"activation_dropout\": 0.1,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.1,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_attention_heads\": 12,\n",
      "  \"decoder_ffn_dim\": 3072,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 12,\n",
      "  \"encoder_ffn_dim\": 3072,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": false,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 128,\n",
      "      \"min_length\": 12,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_cnn\": {\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 142,\n",
      "      \"min_length\": 56,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_xsum\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 62,\n",
      "      \"min_length\": 11,\n",
      "      \"num_beams\": 6\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/alexander/.cache/huggingface/hub/models--facebook--bart-base/snapshots/aadd2ab0ae0c8268c7c9693540e9904811f36177/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BartForConditionalGeneration.\n",
      "\n",
      "All the weights of BartForConditionalGeneration were initialized from the model checkpoint at facebook/bart-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7202\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='451' max='451' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [451/451 00:33]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.9570019245147705, 'eval_runtime': 33.9121, 'eval_samples_per_second': 212.373, 'eval_steps_per_second': 13.299}\n",
      "mean syl line 0 : 4.181640625\n",
      "std syl line 0 : 0.9520818285996269\n",
      "mean syl line 1 : 8.357421875\n",
      "std syl line 1 : 1.3136838387798961\n",
      "mean syl line 2 : 4.902152641878669\n",
      "std syl line 2 : 1.11834693503913\n",
      "{'rouge1': 0.08712119763567386, 'rouge2': 0.00263164241173313, 'rougeL': 0.0771556778237466, 'rougeLsum': 0.08598374520407628}\n",
      "BertScore f1 mean:  0.814120375434868\n",
      "BertScore recall mean:  0.8114235509419814\n",
      "BertScore precision mean:  0.8170057532843202\n",
      "Auto Rouge-1 mean:  0.4058214834926023\n",
      "Auto Rouge-2 mean:  0.24658457844248577\n",
      "Self similarity mean:  0.11791933381288851\n",
      "generations\n",
      "[' tell em i\\nused to suck on the power\\nof my day wow', ' useless\\nravens on the strength of\\nless than a day', ' use your\\nless than a day to study\\ni have neither', ' use your\\nless than every day for\\na study', ' useless\\nravens on the strength of\\nless than a day', ' need to study\\nless than a day so i\\ncan study', ' useless\\nravens on the strength of\\nless than a day', ' use this\\nless than a day to study\\ni have neither', ' use your\\nless than every day for\\na study', ' use this\\nless than a day to catch up\\non my study', ' use your\\nless than every day for\\na study', ' wish i had\\nthe power to study so\\ni could study', ' use your\\nless than every day for\\na study', ' need to study\\nand return the power\\nof my day', ' need to study\\nand return the power\\nof my day', ' need to study\\nand return the power\\nof my day', ' useless\\nravens on the strength of\\nless than a day', ' use your\\nless than a day for shopping\\ni have neither', ' use your\\nless than every day for\\na study', ' useless\\nrapping on the power\\nof my day']\n",
      "titles\n",
      "[' em doesnt', ' aller sickness', ' mississippi tornado', ' physician moreover', ' marlon leagues', ' professor hour', ' teamwork warriors', ' connect kills', ' stupidity max', ' suggest ourselves', ' changeful rhythm', ' dream', ' neglecting log', ' early off', ' talks close', ' whack texas', ' trialflames offenders', ' betrayal heartbreak', ' canvas list', ' mallard glimpse']\n",
      "references\n",
      "[' doing it doesnt\\ndo it to em em doing\\nit does it to em', ' im starting a gang\\nto fight the summer sickness\\naller g unit', ' tornado warning\\nfor mississippi county\\neveryone stay safe', ' the two lines may be translated love is the physician of life\\nand next to our lord himself moreover it is the way that goes\\nstraight to heaven', ' of course im facing\\nmarlon mack in both of my\\nmoney leagues this week', ' our professor\\njust let us out an hour\\nearly god is good', ' congratulations\\non the win awesome teamwork\\nlady warriors', ' feeling very dumb\\nas the other half kills it\\nat only connect', ' stupidity at\\nits max youre a dumb bitch if\\nyou do stuff like this', ' i suggest we learn\\nto love ourselves before\\nits made illegal', ' waiting for you\\nthe changeful rhythm\\nof the rain', ' the dream\\nthe dream\\nthe dream', ' finally getting\\nmy life ive been neglecting\\nmyself for too log', ' i get off early\\ntoday but i really dont\\neven want to go', ' i wont ever let\\nsomeone who talks bad about\\nme be close to me', ' texas living is\\nso whack all they do for fun\\nis go out and drink', ' when offenders are indited\\nand with trialflames ignited\\nelsewhere ill attend if cited', ' heartbreak is the worst\\nyou really feel it in your\\nchest the betrayal', ' looking at my long\\nto do list in canvas makes\\nme want to throw up', ' golden waters still\\nautumn leaves fading away\\nglimpse of mallard ducks']\n"
     ]
    }
   ],
   "source": [
    "finetuned_generator_tester = test_generator('/home/alexander/nlg-project/GAN_models_finetuned/' + 'four_it_ft_generator_bad.pt',eval_ds_plain)\n",
    "finetuned_generator_tester.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_generator_tester = test_generator('/home/alexander/nlg-project/GAN_models_finetuned/' + '3_it_ft_generator.pt',eval_ds_plain)\n",
    "finetuned_generator_tester.test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_generator_tester = test_generator('/home/alexander/nlg-project/GAN_models_finetuned/' + '2_half_it_ft_generator.pt',eval_ds_plain)\n",
    "finetuned_generator_tester.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/alexander/.cache/huggingface/hub/models--facebook--bart-base/snapshots/aadd2ab0ae0c8268c7c9693540e9904811f36177/config.json\n",
      "Model config BartConfig {\n",
      "  \"_name_or_path\": \"bart-base\",\n",
      "  \"activation_dropout\": 0.1,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.1,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_attention_heads\": 12,\n",
      "  \"decoder_ffn_dim\": 3072,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 12,\n",
      "  \"encoder_ffn_dim\": 3072,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": false,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 128,\n",
      "      \"min_length\": 12,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_cnn\": {\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 142,\n",
      "      \"min_length\": 56,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_xsum\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 62,\n",
      "      \"min_length\": 11,\n",
      "      \"num_beams\": 6\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/alexander/.cache/huggingface/hub/models--facebook--bart-base/snapshots/aadd2ab0ae0c8268c7c9693540e9904811f36177/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BartForConditionalGeneration.\n",
      "\n",
      "All the weights of BartForConditionalGeneration were initialized from the model checkpoint at facebook/bart-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7202\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='451' max='451' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [451/451 00:36]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 15.133199691772461, 'eval_runtime': 36.8634, 'eval_samples_per_second': 195.37, 'eval_steps_per_second': 12.234}\n",
      "mean syl line 0 : 10.001953125\n",
      "std syl line 0 : 0.044150994357255134\n",
      "mean syl line 1 : 19.693359375\n",
      "std syl line 1 : 1.1582393544080642\n",
      "mean syl line 2 : 23.044921875\n",
      "std syl line 2 : 2.8311764604394556\n",
      "{'rouge1': 0.06531888558462579, 'rouge2': 0.002192512456698312, 'rougeL': 0.0586958234167825, 'rougeLsum': 0.06398191021926698}\n",
      "BertScore f1 mean:  0.8023670351831242\n",
      "BertScore recall mean:  0.8045374696375802\n",
      "BertScore precision mean:  0.8003168099094182\n",
      "Auto Rouge-1 mean:  0.9387648015320235\n",
      "Auto Rouge-2 mean:  0.8974922897241098\n",
      "Self similarity mean:  0.14943353810541304\n",
      "generations\n",
      "[\"'s the romantic of a whore\\nthis summer will have you if you have\\nthis financial system whenever you have a reasonable\\nof wealth whenever you\", \"'s the romantic of a whore\\nthis summer will have you if you have\\nthis financial system whenever I have a\\nof societies whenever you have\", \"'s the romantic of a whore\\nthis summer will have you if you have\\nthis financial system whenever you have a\\nof societies whenever they have\", \"'s the romantic of a whore\\nthis summer will have you if you have\\nthis financial system whenever you have a reasonable\\nof wealth whenever you\", \"'s the romantic of a whore\\nthis summer will have you if you have\\nthis financial system whenever you have a reasonable\\nof wealth whenever you\", \"'s the romantic of a whore\\nthis summer will have you if you have\\nthis financial system whenever you have a reasonable\\nof wealth whenever you\", \"'s the romantic of a whore\\nthis summer will have you if you have\\nthis financial system whenever you have a reasonable\\nof wealth whenever you\", \"'s the romantic of a whore\\nthis summer will have you if you have\\nthis financial system whenever you have a reasonable\\nof wealth whenever you\", \"'s the romantic of a whore\\nthis summer will have you if you have\\nthis financial system whenever you have a reasonable\\nof wealth whenever you\", \"'s the romantic of a whore\\nthis summer will have you if you have\\nthis financial system whenever you have a reasonable\\nof wealth whenever you\", \"'s the romantic of a whore\\nthis summer will have you if you have\\nthis financial system whenever you have a reasonable\\nof wealth whenever you\", \"'s the romantic of a whore\\nthis summer will have you if you have\\nthis financial system whenever you have a reasonable\\nof wealth whenever you\", \"'s the romantic of a whore\\nthis summer will have you if you have\\nthis financial system whenever you have a reasonable\\nof wealth whenever you\", \"'s the romantic of a whore\\nthis summer whenever you have a\\nthis financial system of societies that would have you if I had to settle for\", \"'s the romantic of a whore\\nthis summer will have you if you have\\nthis financial system whenever you have a reasonable\\nof wealth whenever you\", \"'s the romantic of a whore\\nthis summer will have you if you have\\nthis financial system whenever you have a reasonable\\nof wealth whenever you\", \"'s the romantic of a whore\\nthis summer will have you if\\nthis financial year whenever you have a\\nmore confident that the banks have a\", \"'s the romantic of a whore\\nthis summer will have you if you have\\nthis financial system whenever you have a reasonable\\nof wealth whenever you\", \"'s the romantic of a whore\\nthis summer will have you if you have\\nthis financial system whenever you have a reasonable\\nof wealth whenever you\", \"'s the romantic of a whore\\nthis summer will have you if you have\\nthis financial system whenever you have a reasonable\\nof wealth whenever you\"]\n",
      "titles\n",
      "[' em doesnt', ' aller sickness', ' mississippi tornado', ' physician moreover', ' marlon leagues', ' professor hour', ' teamwork warriors', ' connect kills', ' stupidity max', ' suggest ourselves', ' changeful rhythm', ' dream', ' neglecting log', ' early off', ' talks close', ' whack texas', ' trialflames offenders', ' betrayal heartbreak', ' canvas list', ' mallard glimpse']\n",
      "references\n",
      "[' doing it doesnt\\ndo it to em em doing\\nit does it to em', ' im starting a gang\\nto fight the summer sickness\\naller g unit', ' tornado warning\\nfor mississippi county\\neveryone stay safe', ' the two lines may be translated love is the physician of life\\nand next to our lord himself moreover it is the way that goes\\nstraight to heaven', ' of course im facing\\nmarlon mack in both of my\\nmoney leagues this week', ' our professor\\njust let us out an hour\\nearly god is good', ' congratulations\\non the win awesome teamwork\\nlady warriors', ' feeling very dumb\\nas the other half kills it\\nat only connect', ' stupidity at\\nits max youre a dumb bitch if\\nyou do stuff like this', ' i suggest we learn\\nto love ourselves before\\nits made illegal', ' waiting for you\\nthe changeful rhythm\\nof the rain', ' the dream\\nthe dream\\nthe dream', ' finally getting\\nmy life ive been neglecting\\nmyself for too log', ' i get off early\\ntoday but i really dont\\neven want to go', ' i wont ever let\\nsomeone who talks bad about\\nme be close to me', ' texas living is\\nso whack all they do for fun\\nis go out and drink', ' when offenders are indited\\nand with trialflames ignited\\nelsewhere ill attend if cited', ' heartbreak is the worst\\nyou really feel it in your\\nchest the betrayal', ' looking at my long\\nto do list in canvas makes\\nme want to throw up', ' golden waters still\\nautumn leaves fading away\\nglimpse of mallard ducks']\n"
     ]
    }
   ],
   "source": [
    "finetuned_generator_tester = test_generator('/home/alexander/nlg-project/GAN_models/' + '6_it_nft_generator.pt',eval_ds_plain)\n",
    "finetuned_generator_tester.test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/alexander/.cache/huggingface/hub/models--facebook--bart-base/snapshots/aadd2ab0ae0c8268c7c9693540e9904811f36177/config.json\n",
      "Model config BartConfig {\n",
      "  \"_name_or_path\": \"bart-base\",\n",
      "  \"activation_dropout\": 0.1,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.1,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_attention_heads\": 12,\n",
      "  \"decoder_ffn_dim\": 3072,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 12,\n",
      "  \"encoder_ffn_dim\": 3072,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": false,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 128,\n",
      "      \"min_length\": 12,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_cnn\": {\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 142,\n",
      "      \"min_length\": 56,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_xsum\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 62,\n",
      "      \"min_length\": 11,\n",
      "      \"num_beams\": 6\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/alexander/.cache/huggingface/hub/models--facebook--bart-base/snapshots/aadd2ab0ae0c8268c7c9693540e9904811f36177/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BartForConditionalGeneration.\n",
      "\n",
      "All the weights of BartForConditionalGeneration were initialized from the model checkpoint at facebook/bart-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7202\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='451' max='451' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [451/451 00:37]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 16.44188117980957, 'eval_runtime': 37.6406, 'eval_samples_per_second': 191.336, 'eval_steps_per_second': 11.982}\n",
      "{'rouge1': 0.04775557712498508, 'rouge2': 0.00016276041666666666, 'rougeL': 0.042772379928534596, 'rougeLsum': 0.04742911599380316}\n",
      "BertScore f1 mean:  0.781419841805473\n",
      "BertScore recall mean:  0.7957276809029281\n",
      "BertScore precision mean:  0.7677300509531051\n"
     ]
    }
   ],
   "source": [
    "finetuned_generator_tester = test_generator('/home/alexander/nlg-project/GAN_models/' + '4_it_nft_generator.pt',eval_ds_plain)\n",
    "finetuned_generator_tester.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/alexander/.cache/huggingface/hub/models--facebook--bart-base/snapshots/aadd2ab0ae0c8268c7c9693540e9904811f36177/config.json\n",
      "Model config BartConfig {\n",
      "  \"_name_or_path\": \"bart-base\",\n",
      "  \"activation_dropout\": 0.1,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.1,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_attention_heads\": 12,\n",
      "  \"decoder_ffn_dim\": 3072,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 12,\n",
      "  \"encoder_ffn_dim\": 3072,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": false,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 128,\n",
      "      \"min_length\": 12,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_cnn\": {\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 142,\n",
      "      \"min_length\": 56,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_xsum\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 62,\n",
      "      \"min_length\": 11,\n",
      "      \"num_beams\": 6\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/alexander/.cache/huggingface/hub/models--facebook--bart-base/snapshots/aadd2ab0ae0c8268c7c9693540e9904811f36177/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BartForConditionalGeneration.\n",
      "\n",
      "All the weights of BartForConditionalGeneration were initialized from the model checkpoint at facebook/bart-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7202\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='451' max='451' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [451/451 00:37]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 25.407241821289062, 'eval_runtime': 37.0857, 'eval_samples_per_second': 194.199, 'eval_steps_per_second': 12.161}\n",
      "mean syl line 0 : 12.0\n",
      "std syl line 0 : 0.0\n",
      "mean syl line 1 : nan\n",
      "std syl line 1 : nan\n",
      "mean syl line 2 : nan\n",
      "std syl line 2 : nan\n",
      "{'rouge1': 0.0003800197963800905, 'rouge2': 0.0, 'rougeL': 0.0003800197963800905, 'rougeLsum': 0.0003800197963800905}\n",
      "BertScore f1 mean:  0.754906335962005\n",
      "BertScore recall mean:  0.7771180191775784\n",
      "BertScore precision mean:  0.7341110940324143\n",
      "Auto Rouge-1 mean:  1.0\n",
      "Auto Rouge-2 mean:  1.0\n",
      "Self similarity mean:  0.5999999999999999\n",
      "generations\n",
      "['T markets markets markets market', 'T markets markets markets market', 'T markets markets markets market', 'T markets markets markets market', 'T markets markets markets market', 'T markets markets markets market', 'T markets markets markets market', 'T markets markets markets market', 'T markets markets markets market', 'T markets markets markets market', 'T markets markets markets market', 'T markets markets markets market', 'T markets markets markets market', 'T markets markets markets market', 'T markets markets markets market', 'T markets markets markets market', 'T markets markets markets market', 'T markets markets markets market', 'T markets markets markets market', 'T markets markets markets market']\n",
      "titles\n",
      "[' em doesnt', ' aller sickness', ' mississippi tornado', ' physician moreover', ' marlon leagues', ' professor hour', ' teamwork warriors', ' connect kills', ' stupidity max', ' suggest ourselves', ' changeful rhythm', ' dream', ' neglecting log', ' early off', ' talks close', ' whack texas', ' trialflames offenders', ' betrayal heartbreak', ' canvas list', ' mallard glimpse']\n",
      "references\n",
      "[' doing it doesnt\\ndo it to em em doing\\nit does it to em', ' im starting a gang\\nto fight the summer sickness\\naller g unit', ' tornado warning\\nfor mississippi county\\neveryone stay safe', ' the two lines may be translated love is the physician of life\\nand next to our lord himself moreover it is the way that goes\\nstraight to heaven', ' of course im facing\\nmarlon mack in both of my\\nmoney leagues this week', ' our professor\\njust let us out an hour\\nearly god is good', ' congratulations\\non the win awesome teamwork\\nlady warriors', ' feeling very dumb\\nas the other half kills it\\nat only connect', ' stupidity at\\nits max youre a dumb bitch if\\nyou do stuff like this', ' i suggest we learn\\nto love ourselves before\\nits made illegal', ' waiting for you\\nthe changeful rhythm\\nof the rain', ' the dream\\nthe dream\\nthe dream', ' finally getting\\nmy life ive been neglecting\\nmyself for too log', ' i get off early\\ntoday but i really dont\\neven want to go', ' i wont ever let\\nsomeone who talks bad about\\nme be close to me', ' texas living is\\nso whack all they do for fun\\nis go out and drink', ' when offenders are indited\\nand with trialflames ignited\\nelsewhere ill attend if cited', ' heartbreak is the worst\\nyou really feel it in your\\nchest the betrayal', ' looking at my long\\nto do list in canvas makes\\nme want to throw up', ' golden waters still\\nautumn leaves fading away\\nglimpse of mallard ducks']\n"
     ]
    }
   ],
   "source": [
    "finetuned_generator_tester = test_generator('/home/alexander/nlg-project/GAN_models/' + 'final_gen_cond_title_generator.pt',eval_ds_plain)\n",
    "finetuned_generator_tester.test()\n",
    "#I think this was about 4 epochs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file /home/alexander/nlg-project/BART-base-syllable-loss/checkpoint-34000/config.json\n",
      "Model config BartConfig {\n",
      "  \"_name_or_path\": \"facebook/bart-base\",\n",
      "  \"activation_dropout\": 0.1,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.1,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_attention_heads\": 12,\n",
      "  \"decoder_ffn_dim\": 3072,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 12,\n",
      "  \"encoder_ffn_dim\": 3072,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": false,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 128,\n",
      "      \"min_length\": 12,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_cnn\": {\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 142,\n",
      "      \"min_length\": 56,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_xsum\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 62,\n",
      "      \"min_length\": 11,\n",
      "      \"num_beams\": 6\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file /home/alexander/nlg-project/BART-base-syllable-loss/checkpoint-34000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BartForConditionalGeneration.\n",
      "\n",
      "All the weights of BartForConditionalGeneration were initialized from the model checkpoint at /home/alexander/nlg-project/BART-base-syllable-loss/checkpoint-34000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7202\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='451' max='451' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [451/451 00:34]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7733350992202759, 'eval_runtime': 34.8668, 'eval_samples_per_second': 206.558, 'eval_steps_per_second': 12.935}\n",
      "mean syl line 0 : 6.8046875\n",
      "std syl line 0 : 2.5095399035169277\n",
      "mean syl line 1 : 9.896484375\n",
      "std syl line 1 : 3.1478891960138715\n",
      "mean syl line 2 : 6.954635108481263\n",
      "std syl line 2 : 2.156571813589745\n",
      "Testing Specific Syllables\n",
      "mean difference syl line 0 : 0.419921875\n",
      "std difference syl line 0 : 0.619848766955686\n",
      "mean difference syl line 1 : 0.798828125\n",
      "std difference syl line 1 : 0.8388507028720811\n",
      "mean difference syl line 2 : 0.7554240631163708\n",
      "std difference syl line 2 : 1.2192348813539247\n",
      "{'rouge1': 0.2787391048022977, 'rouge2': 0.0553047204006155, 'rougeL': 0.2197894131748271, 'rougeLsum': 0.2693011237880694}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file config.json from cache at /home/alexander/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-large\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading file vocab.json from cache at /home/alexander/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/vocab.json\n",
      "loading file merges.txt from cache at /home/alexander/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/merges.txt\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at None\n",
      "loading configuration file config.json from cache at /home/alexander/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-large\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /home/alexander/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-large\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/alexander/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/pytorch_model.bin\n",
      "All the weights of RobertaModel were initialized from the model checkpoint at roberta-large.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertScore f1 mean:  0.8413175583118573\n",
      "BertScore recall mean:  0.841056794510223\n",
      "BertScore precision mean:  0.8416823233710602\n",
      "Auto Rouge-1 mean:  0.10184102972441866\n",
      "Auto Rouge-2 mean:  0.013868704962074145\n",
      "Self similarity mean:  0.11734460779423994\n",
      "generations\n",
      "[' im actually\\nso happy for em and it\\ndoesnt make me sad', ' am i the only\\none who cannot handle the\\nsickness of aller', ' anyone know of\\na tornado in mississippi\\nasking for a friend', ' moreover you should be writing like a psychologist in love\\nwith a lover in love with a lover who loves you as much as you love\\na', ' marlon james is the\\nbest player in the league and\\ni love him so much', ' my professor is\\ngoing to sleep for the first\\ntime in an hour', ' warriors need a\\nteamwork day before they go\\nto the nba', ' every time i\\nconnect with my dad he kills\\nme every night', ' stupidity is\\none of the worst things you can\\ndo for yourself max', ' i suggest the best\\nway to start your day is to\\nlove ourselves first', ' changing the rhythm\\nof the river\\nsummers end', ' dream\\ndream\\ndream', ' i really wanna\\nlog off twitter for a lil\\nwhile im neglecting', ' i really want to\\ngo to sleep early but i\\nneed to get off work', ' i just wanna be\\nclose to the person who talks\\nto me all the way', ' texas is a whack\\ncountry and i dont know what\\nto do with my life', ' these are the people of the\\ntrialflames who are the worst\\nand the worst offenders of all men', ' heartbreak is the worst\\nfeeling ive ever felt in\\nmy entire life', ' am i the only\\none who doesnt know how to\\nlisten to canvas', ' consumer seeking\\nmobile home insurance in\\nmallard just a glimpse', ' yeah i really dont\\nknow what to do with my life\\nim coming over', ' the story of my\\nlife is so funny and so\\ncute and i love you', ' im definitely\\nnot gonna be sent to school\\nin the next few days', ' henderson is the\\nbest player in the league at\\nall odds right now lol', ' i pray tomorrow\\nis one of the best midterms\\nof my life so far', ' ive never been so\\nhooked up on a wii game\\nin my entire life', ' i really wanna\\nlearn how to speak french but i\\ndont speak fluent', ' im actually\\nreally excited for the\\ncombine dates this year', ' you should be writing\\nlike an ephesus in love\\nwith your bitterest friend', ' my boyfriend is in\\nthe best timing ever and\\ni love him so much', ' jessica fro is\\none of the best movies out\\nof all time hands down', ' theres a special place in hell\\nin hell for people who make you feel your\\npotent when youre not on issue', ' thanks for the follow\\nburley i wish you a very\\nhappy new year sag', ' people who claim they\\nhave asylum are the worst\\npeople in the world', ' you should be writing\\nlike an original in\\nlove with a mix', ' i would rather die\\npoorly than live in a house\\nfull of rotten eggs', ' i cant remember\\nthe last time i actually\\nhad to go to sleep', ' ive never been more\\nimpressed with a book than i\\nam with books right now', ' there is a blurring line between the\\nchords of your heart and the lines of your lips and the\\nlines of your mouth and the', ' anyways thank you\\nfor all the toxic people\\ni have in my life', ' a construction crane\\ntouches the constellation\\nsummers end', ' im driving to la\\nfor the first time ever and\\nim so excited', ' i just realized\\ni got stolen for the first\\ntime in a long time', ' am i the only\\none that doesnt get in a\\ntrouble with your teacher', ' is it really that\\nhard to idolize a\\nperson with no hustle', ' there is no such thing as a bad day in\\nstyria it is just a matter of time before you start\\n mapping your life out of', ' people who dont know\\nhow to live their lives are the\\nones who get the hype', ' anyone wanna\\nbuy harry potter tickets\\nso i can borrow them', ' cold morning\\nmy dog finds a hole\\nin a puddled rock', ' to be honest with\\nmyself i dont have any\\nbrains in my body', ' the worst thing about\\nchemistry is that you have\\nno sense of humor', ' melo should not have\\nbeen called a foul in the first\\nhalf of the season', ' anyone wanna\\nbuy me crafty christmas gifts\\nasking for a friend', ' toya belated\\nbirthday to one of the best\\npeople in the world', ' theres alot going\\non in my life and i dont\\nknow what to do with', ' the only thing i\\nlike about controlling my\\nlife is my handle', ' never forgave a\\nperson who made you feel like\\nyoure in jail again', ' ive never been more\\nexhausted than i am right\\nnow i need a sigh', ' you should be writing\\nlike a bruce dukes in love\\nwith a bunch of braves', ' you should be writing\\nlike an identity\\nin love with an exchange', ' i have established\\na relationship with the\\ngreatest runner up', ' curtain of smoke\\nbillows in the\\nmiddle of the night', ' there is no room in heaven for fabling\\nthere is only room in the world for rain\\nto sprinkle the rain', ' gazillion germs\\nscent of rain\\non the sidewalk']\n",
      "titles\n",
      "[' [4, 8, 7];em doesnt', ' [6, 9, 5];aller sickness', ' [5, 8, 7];mississippi tornado', ' [23, 25, 5];physician moreover', ' [8, 10, 8];marlon leagues', ' [5, 9, 6];professor hour', ' [5, 10, 4];teamwork warriors', ' [6, 10, 5];connect kills', ' [5, 12, 8];stupidity max', ' [6, 11, 7];suggest ourselves', ' [7, 6, 4];changeful rhythm', ' [2, 2, 2];dream', ' [5, 10, 7];neglecting log', ' [6, 7, 7];early off', ' [6, 9, 6];talks close', ' [6, 9, 8];whack texas', ' [9, 10, 11];trialflames offenders', ' [6, 10, 6];betrayal heartbreak', ' [6, 10, 6];canvas list', ' [7, 10, 8];mallard glimpse', ' [6, 12, 6];yeah coming', ' [6, 9, 8];story cute', ' [6, 10, 8];sent definitely', ' [6, 9, 8];henderson odds', ' [6, 10, 7];midterms pray', ' [8, 10, 7];wii hooked', ' [5, 11, 6];fluent french', ' [4, 9, 9];combine dates', ' [10, 11, 8];ephesus bitterest', ' [5, 9, 7];timing boyfriend', ' [5, 11, 8];fro jessica', ' [12, 17, 14];potent issue', ' [7, 9, 8];burley sag', ' [6, 10, 6];asylum claim', ' [9, 10, 6];original mix', ' [7, 11, 6];poorly rotten', ' [7, 8, 7];actually remember', ' [8, 9, 8];impressed books', ' [14, 20, 13];blurring chords', ' [7, 10, 6];toxic anyways', ' [7, 8, 3];constellation trucks', ' [7, 9, 6];driving la', ' [6, 12, 7];stolen second', ' [6, 9, 9];trouble teacher', ' [6, 7, 7];idolize hustle', ' [12, 21, 18];styria mapping', ' [6, 13, 7];hype lives', ' [5, 8, 8];borrow potter', ' [4, 7, 5];puddled finds', ' [6, 10, 6];brains honest', ' [6, 11, 9];chemistry humor', ' [9, 10, 6];melo foul', ' [5, 9, 6];crafty presents', ' [5, 9, 6];toya belated', ' [6, 9, 7];alot theres', ' [5, 9, 6];controlling handle', ' [7, 12, 8];forgave jail', ' [9, 8, 5];exhausted sigh', ' [9, 10, 7];dukes braves', ' [7, 8, 9];identities exchange', ' [6, 9, 6];established runner', ' [6, 5, 5];billows curtain', ' [16, 15, 6];fabling sprinkle', ' [6, 4, 5];gazillion germs']\n",
      "references\n",
      "[' doing it doesnt\\ndo it to em em doing\\nit does it to em', ' im starting a gang\\nto fight the summer sickness\\naller g unit', ' tornado warning\\nfor mississippi county\\neveryone stay safe', ' the two lines may be translated love is the physician of life\\nand next to our lord himself moreover it is the way that goes\\nstraight to heaven', ' of course im facing\\nmarlon mack in both of my\\nmoney leagues this week', ' our professor\\njust let us out an hour\\nearly god is good', ' congratulations\\non the win awesome teamwork\\nlady warriors', ' feeling very dumb\\nas the other half kills it\\nat only connect', ' stupidity at\\nits max youre a dumb bitch if\\nyou do stuff like this', ' i suggest we learn\\nto love ourselves before\\nits made illegal', ' waiting for you\\nthe changeful rhythm\\nof the rain', ' the dream\\nthe dream\\nthe dream', ' finally getting\\nmy life ive been neglecting\\nmyself for too log', ' i get off early\\ntoday but i really dont\\neven want to go', ' i wont ever let\\nsomeone who talks bad about\\nme be close to me', ' texas living is\\nso whack all they do for fun\\nis go out and drink', ' when offenders are indited\\nand with trialflames ignited\\nelsewhere ill attend if cited', ' heartbreak is the worst\\nyou really feel it in your\\nchest the betrayal', ' looking at my long\\nto do list in canvas makes\\nme want to throw up', ' golden waters still\\nautumn leaves fading away\\nglimpse of mallard ducks', ' having my way with\\nthe money yeah that shit keep\\ncoming and coming', ' my story as been\\nso cute i really had the\\nbest time this weekend', ' i definitely\\njust sent this to my mom and\\ntold her to make it', ' odds on henderson\\nto lift the champions league\\nand world cup trophy', ' after midterms and\\nall i can do is hope and\\npray cause heaven knows', ' ive been wanting to\\nplay tennis for a while so\\ni hooked up the wii', ' if anyone has\\nfrench homework ill do it for\\ni speak fluent french', ' any idea\\nwhen the combine windows and\\ndates will be released', ' but the bitterest disgrace\\nis to see forever the face\\nof the monk of ephesus', ' my boyfriend always\\nwanna be making love but\\nat the wrong timing', ' baby jessica\\nand this fro is giving me\\nso much life on the', ' they issue forth and we who never knew\\ntill then how potent and how real they were\\ntake them and wonder and so bless the hour', ' sag you remember\\nthat word burley going to\\nland you in prison', ' theyre trying to claim\\nasylum theres no reason\\nto fucking do this', ' are you still going\\nto release do it for me\\noriginal mix', ' feel fucking rotten\\nsurvived the first week in work\\nbut im so poorly', ' dont remember the\\nlast time i actually\\ngot ready for school', ' im not impressed by\\nmoney im impressed by how\\nyou look and read books', ' from my bed i can hear him and the round\\nnotes flutter and tap about the room and hit against each\\nother blurring to unexpected chords', ' anyways life is\\ntoo beautiful to be stressed\\nby toxic people', ' just over the fence\\na constellation of stars\\nand trucks', ' driving through la in\\nthe rain is enough to make\\nsomeone go crazy', ' this is the second\\ngame night ive been to and have\\nhad my shit stolen', ' i got in trouble\\nfor asking my teacher to\\nnot dumb down his class', ' i dont idolize\\njay z the person i just\\nadmire his hustle', ' lesson of the day syria and styria\\nfor syria read his conquering banner shook from syria\\nand for styria look at this harp of blood mapping', ' yo being in love\\nwith your best friend really lives\\nup to the hype wow', ' anyone got a\\nharry potter outfit i\\ncan borrow next month', ' puddled field\\nthe cat finds sunlight\\non a fence post', ' lets ask our hearts\\nrather than our brains thats\\nbeing more honest', ' love the chemistry\\nknowledge and sense of humor\\nthe three of you share', ' that was great defense\\nby melo but that was not\\na foul on melo', ' gonna be crafty\\nfor the rest of the day and\\nmake birthday presents', ' happy belated\\nbirthday toya yall looking\\ntoo cute as always', ' baby theres alot\\nof things in my heart i want\\nto tell you about', ' controlling only\\nwhat i can control letting\\ngod handle the rest', ' glad he forgave him\\nbut that shit is gone eat at\\ndude while he in jail', ' can yall fucking choose\\nwhere harry is already\\nsigh im exhausted', ' dukes failed to mention\\nthat the opening day game\\nwas against the braves', ' if were going to\\nexchange identities ill\\nbe needing your clothes', ' he definitely\\nestablished himself as a\\nrunner no question', ' a curtain billows\\nbefore the rain\\nscent of roses', ' which poets prone to lie have paved with gold\\nwhich poets sprinkle oer with sands of gold\\nwhich fabling poets', ' a gazillion germs\\nin a sneeze\\nyet more stars']\n"
     ]
    }
   ],
   "source": [
    "base_mle_tester = test_finetuned('/home/alexander/nlg-project/BART-base-syllable-loss/' + 'checkpoint-34000',eval_ds_syllables)\n",
    "#finetuned_generator_tester.test_generation()\n",
    "base_mle_tester.test(conditioned_syllables=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finetuned_generator_tester.references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file /home/alexander/nlg-project/BART-base-syllable-naive/checkpoint-32000/config.json\n",
      "Model config BartConfig {\n",
      "  \"_name_or_path\": \"facebook/bart-base\",\n",
      "  \"activation_dropout\": 0.1,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.1,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_attention_heads\": 12,\n",
      "  \"decoder_ffn_dim\": 3072,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 12,\n",
      "  \"encoder_ffn_dim\": 3072,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": false,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 128,\n",
      "      \"min_length\": 12,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_cnn\": {\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 142,\n",
      "      \"min_length\": 56,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_xsum\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 62,\n",
      "      \"min_length\": 11,\n",
      "      \"num_beams\": 6\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file /home/alexander/nlg-project/BART-base-syllable-naive/checkpoint-32000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BartForConditionalGeneration.\n",
      "\n",
      "All the weights of BartForConditionalGeneration were initialized from the model checkpoint at /home/alexander/nlg-project/BART-base-syllable-naive/checkpoint-32000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7202\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='451' max='451' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [451/451 00:34]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7790548801422119, 'eval_runtime': 34.1542, 'eval_samples_per_second': 210.867, 'eval_steps_per_second': 13.205}\n",
      "mean syl line 0 : 6.69140625\n",
      "std syl line 0 : 2.4008633650128735\n",
      "mean syl line 1 : 9.931640625\n",
      "std syl line 1 : 3.2423610411010073\n",
      "mean syl line 2 : 6.81335952848723\n",
      "std syl line 2 : 2.129071463126769\n",
      "Testing Specific Syllables\n",
      "mean difference syl line 0 : 0.408203125\n",
      "std difference syl line 0 : 0.6054529987870523\n",
      "mean difference syl line 1 : 0.791015625\n",
      "std difference syl line 1 : 0.7913579506177084\n",
      "mean difference syl line 2 : 0.8781925343811395\n",
      "std difference syl line 2 : 1.71004655156229\n",
      "{'rouge1': 0.28364452790163464, 'rouge2': 0.05480439553769917, 'rougeL': 0.22205493560258635, 'rougeLsum': 0.27343089481602645}\n",
      "BertScore f1 mean:  0.8416935628047213\n",
      "BertScore recall mean:  0.8417783087352291\n",
      "BertScore precision mean:  0.8417256660759449\n",
      "Auto Rouge-1 mean:  0.10691523579973278\n",
      "Auto Rouge-2 mean:  0.013590834264298877\n",
      "Self similarity mean:  0.11588095877926323\n",
      "generations\n",
      "[' i really doesnt\\nknow what i gotta do to\\nmake em happy lol', ' am i the only\\none who gets the sickness of\\nbeing an aller', ' the tornado in\\nmississippi is the worst\\nthing ive ever seen', ' i am a doctor not a lawyer but aphysician moreover\\ni am a professor not a professor in the university of texas\\nand a', ' marlon lewis one\\nof the best players in the\\nlakers this season', ' my professor is\\ngoing to sleep for the first\\ntime in an hour', ' warriors going\\nthrough some of the best work and\\nteamwork ever', ' every time i\\nconnect with someone it kills\\nme every time', ' the stupidity\\nof max has got to be one\\nof the worst things to', ' does anyone else\\nsuggest that we should change the\\nname of ourselves', ' changing the rhythm\\nof the river\\nautumn dusk', ' dream\\ndream\\ndream', ' debating if i\\nshould log off or go back to\\nneglecting myself', ' im going to sleep\\nearly tomorrow and i\\ndont know what to do', ' am i the only\\none who talks about people\\nclose to my family', ' am i the only\\none who thinks texas is a\\nwhack place to live in', ' one of the trialflames is\\none of the worst offenders\\ni have ever seen in my life', ' heartbreak after a\\nbetrayal is one of the\\nbest feelings ever', ' am i the only\\none who doesnt like the black\\npanther canvas list', ' consumer seeking\\nmallard insurance just to\\nget a glimpse of you', ' yeah i didnt see\\nyou coming but i know youre\\ngoing to see me', ' every time i\\nsee this story i think of\\nhow cute it is lol', ' definitely one\\nof the best things ive ever\\nsent to someone else', ' the odds are henderson\\nis going to have the best\\ngame of his career', ' this is going to\\nbe one of the worst midterms\\nive ever seen pray', ' hooked up with some\\nwii games for the first time in\\na long time today', ' i havent ever\\nbeen more fluent in french than\\ni am in english', ' i didnt expect\\nanyone to combine the\\ndate and time of year', ' anephesus is one of the\\nbitterest things ive ever seen\\nin my entire life', ' i really wanna\\nbe friends with my boyfriend but\\nhis timing is bad', ' congratulations\\nfro jessica hope youre\\nhaving a great time', ' there is no such thing as an issue\\nthere is no potential in this world as there is\\nno potential in the rest of the world', ' burley thanks for the\\nfollow i look forward to\\nseeing you in sag', ' is it illegal\\nto claim asylum in the\\nmiddle of the night', ' ive never been more\\nhappy with the mix than the\\noriginal one', ' i am poorly short\\nand stout this is my house and\\nthis is my rotten', ' i cant remember\\nthe last time i actually\\nwent to work today', ' ive never been so\\nimpressed with all the books i\\nhave ever read yet', ' there are so many lines and so many\\nchords and so few lines and there are no lines between\\nthe lines and the lines blurring', ' anyways theres a\\nlot of toxic people out\\nhere in the world', ' between the yellow\\ntruck and the yellow flag\\nconstellation', ' i have never been\\nmore excited about driving\\nto la in my life', ' i havent stolen\\nanything from you for the\\nsecond time today', ' getting in trouble\\nwith a teacher is the worst\\nthing ive ever done', ' i want to hustle\\nbut i also want to\\nidolize my life', ' this is the saddest thing ive ever\\nseen in styria and it reminds me of the days when i used to\\nuse google maps', ' why do people hype\\nup their lives when they have no\\none to talk to', ' does anyone know\\nhow to borrow a harry\\npotter book from me', ' the puddled\\ndog finds his way\\nto the bathroom', ' to be honest i\\ndont know what to do with my\\nbrains anymore lol', ' chemistry humor\\nis one of the best things ive\\never seen in life', ' ive never been more\\nhappy with melo than he\\nwas with a foul', ' merry christmas and\\nhappy mothers day to all\\nthe crafty presents', ' toya belated\\nbirthday for the first time in\\na very long time', ' theres alot of shit\\ngoing on but theres always\\nsomething going on', ' the only thing i\\ncan handle in my life is\\ncontrolling myself', ' never forgave the\\nperson that gave you the right\\nto go to jail lol', ' ive never been more\\nexhausted in a week than\\ni am today sigh', ' you should be writing\\nlike a duck in love with a\\nband of braves and dukes', ' you should be writing\\nlike an identity\\nin love with an exchange', ' justin timberlake\\nestablished as the best\\nrunner in the league', ' a curtain of smoke\\nbillows around\\nthe funeral', ' sometimes you just have to fabling it\\nand sometimes you have to let it go\\nand sprinkle it in', ' germs in a gazillion\\ndifferent kinds\\nof germs']\n",
      "titles\n",
      "[' [4, 8, 7];em doesnt', ' [6, 9, 5];aller sickness', ' [5, 8, 7];mississippi tornado', ' [23, 25, 5];physician moreover', ' [8, 10, 8];marlon leagues', ' [5, 9, 6];professor hour', ' [5, 10, 4];teamwork warriors', ' [6, 10, 5];connect kills', ' [5, 12, 8];stupidity max', ' [6, 11, 7];suggest ourselves', ' [7, 6, 4];changeful rhythm', ' [2, 2, 2];dream', ' [5, 10, 7];neglecting log', ' [6, 7, 7];early off', ' [6, 9, 6];talks close', ' [6, 9, 8];whack texas', ' [9, 10, 11];trialflames offenders', ' [6, 10, 6];betrayal heartbreak', ' [6, 10, 6];canvas list', ' [7, 10, 8];mallard glimpse', ' [6, 12, 6];yeah coming', ' [6, 9, 8];story cute', ' [6, 10, 8];sent definitely', ' [6, 9, 8];henderson odds', ' [6, 10, 7];midterms pray', ' [8, 10, 7];wii hooked', ' [5, 11, 6];fluent french', ' [4, 9, 9];combine dates', ' [10, 11, 8];ephesus bitterest', ' [5, 9, 7];timing boyfriend', ' [5, 11, 8];fro jessica', ' [12, 17, 14];potent issue', ' [7, 9, 8];burley sag', ' [6, 10, 6];asylum claim', ' [9, 10, 6];original mix', ' [7, 11, 6];poorly rotten', ' [7, 8, 7];actually remember', ' [8, 9, 8];impressed books', ' [14, 20, 13];blurring chords', ' [7, 10, 6];toxic anyways', ' [7, 8, 3];constellation trucks', ' [7, 9, 6];driving la', ' [6, 12, 7];stolen second', ' [6, 9, 9];trouble teacher', ' [6, 7, 7];idolize hustle', ' [12, 21, 18];styria mapping', ' [6, 13, 7];hype lives', ' [5, 8, 8];borrow potter', ' [4, 7, 5];puddled finds', ' [6, 10, 6];brains honest', ' [6, 11, 9];chemistry humor', ' [9, 10, 6];melo foul', ' [5, 9, 6];crafty presents', ' [5, 9, 6];toya belated', ' [6, 9, 7];alot theres', ' [5, 9, 6];controlling handle', ' [7, 12, 8];forgave jail', ' [9, 8, 5];exhausted sigh', ' [9, 10, 7];dukes braves', ' [7, 8, 9];identities exchange', ' [6, 9, 6];established runner', ' [6, 5, 5];billows curtain', ' [16, 15, 6];fabling sprinkle', ' [6, 4, 5];gazillion germs']\n",
      "references\n",
      "[' doing it doesnt\\ndo it to em em doing\\nit does it to em', ' im starting a gang\\nto fight the summer sickness\\naller g unit', ' tornado warning\\nfor mississippi county\\neveryone stay safe', ' the two lines may be translated love is the physician of life\\nand next to our lord himself moreover it is the way that goes\\nstraight to heaven', ' of course im facing\\nmarlon mack in both of my\\nmoney leagues this week', ' our professor\\njust let us out an hour\\nearly god is good', ' congratulations\\non the win awesome teamwork\\nlady warriors', ' feeling very dumb\\nas the other half kills it\\nat only connect', ' stupidity at\\nits max youre a dumb bitch if\\nyou do stuff like this', ' i suggest we learn\\nto love ourselves before\\nits made illegal', ' waiting for you\\nthe changeful rhythm\\nof the rain', ' the dream\\nthe dream\\nthe dream', ' finally getting\\nmy life ive been neglecting\\nmyself for too log', ' i get off early\\ntoday but i really dont\\neven want to go', ' i wont ever let\\nsomeone who talks bad about\\nme be close to me', ' texas living is\\nso whack all they do for fun\\nis go out and drink', ' when offenders are indited\\nand with trialflames ignited\\nelsewhere ill attend if cited', ' heartbreak is the worst\\nyou really feel it in your\\nchest the betrayal', ' looking at my long\\nto do list in canvas makes\\nme want to throw up', ' golden waters still\\nautumn leaves fading away\\nglimpse of mallard ducks', ' having my way with\\nthe money yeah that shit keep\\ncoming and coming', ' my story as been\\nso cute i really had the\\nbest time this weekend', ' i definitely\\njust sent this to my mom and\\ntold her to make it', ' odds on henderson\\nto lift the champions league\\nand world cup trophy', ' after midterms and\\nall i can do is hope and\\npray cause heaven knows', ' ive been wanting to\\nplay tennis for a while so\\ni hooked up the wii', ' if anyone has\\nfrench homework ill do it for\\ni speak fluent french', ' any idea\\nwhen the combine windows and\\ndates will be released', ' but the bitterest disgrace\\nis to see forever the face\\nof the monk of ephesus', ' my boyfriend always\\nwanna be making love but\\nat the wrong timing', ' baby jessica\\nand this fro is giving me\\nso much life on the', ' they issue forth and we who never knew\\ntill then how potent and how real they were\\ntake them and wonder and so bless the hour', ' sag you remember\\nthat word burley going to\\nland you in prison', ' theyre trying to claim\\nasylum theres no reason\\nto fucking do this', ' are you still going\\nto release do it for me\\noriginal mix', ' feel fucking rotten\\nsurvived the first week in work\\nbut im so poorly', ' dont remember the\\nlast time i actually\\ngot ready for school', ' im not impressed by\\nmoney im impressed by how\\nyou look and read books', ' from my bed i can hear him and the round\\nnotes flutter and tap about the room and hit against each\\nother blurring to unexpected chords', ' anyways life is\\ntoo beautiful to be stressed\\nby toxic people', ' just over the fence\\na constellation of stars\\nand trucks', ' driving through la in\\nthe rain is enough to make\\nsomeone go crazy', ' this is the second\\ngame night ive been to and have\\nhad my shit stolen', ' i got in trouble\\nfor asking my teacher to\\nnot dumb down his class', ' i dont idolize\\njay z the person i just\\nadmire his hustle', ' lesson of the day syria and styria\\nfor syria read his conquering banner shook from syria\\nand for styria look at this harp of blood mapping', ' yo being in love\\nwith your best friend really lives\\nup to the hype wow', ' anyone got a\\nharry potter outfit i\\ncan borrow next month', ' puddled field\\nthe cat finds sunlight\\non a fence post', ' lets ask our hearts\\nrather than our brains thats\\nbeing more honest', ' love the chemistry\\nknowledge and sense of humor\\nthe three of you share', ' that was great defense\\nby melo but that was not\\na foul on melo', ' gonna be crafty\\nfor the rest of the day and\\nmake birthday presents', ' happy belated\\nbirthday toya yall looking\\ntoo cute as always', ' baby theres alot\\nof things in my heart i want\\nto tell you about', ' controlling only\\nwhat i can control letting\\ngod handle the rest', ' glad he forgave him\\nbut that shit is gone eat at\\ndude while he in jail', ' can yall fucking choose\\nwhere harry is already\\nsigh im exhausted', ' dukes failed to mention\\nthat the opening day game\\nwas against the braves', ' if were going to\\nexchange identities ill\\nbe needing your clothes', ' he definitely\\nestablished himself as a\\nrunner no question', ' a curtain billows\\nbefore the rain\\nscent of roses', ' which poets prone to lie have paved with gold\\nwhich poets sprinkle oer with sands of gold\\nwhich fabling poets', ' a gazillion germs\\nin a sneeze\\nyet more stars']\n"
     ]
    }
   ],
   "source": [
    "finetuned_generator_tester = test_finetuned('/home/alexander/nlg-project/BART-base-syllable-naive/' + 'checkpoint-32000',eval_ds_syllables)\n",
    "finetuned_generator_tester.test(conditioned_syllables=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file /home/alexander/nlg-project/BART-base/checkpoint-25000/config.json\n",
      "Model config BartConfig {\n",
      "  \"_name_or_path\": \"facebook/bart-base\",\n",
      "  \"activation_dropout\": 0.1,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.1,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_attention_heads\": 12,\n",
      "  \"decoder_ffn_dim\": 3072,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 12,\n",
      "  \"encoder_ffn_dim\": 3072,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": false,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 128,\n",
      "      \"min_length\": 12,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_cnn\": {\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 142,\n",
      "      \"min_length\": 56,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_xsum\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 62,\n",
      "      \"min_length\": 11,\n",
      "      \"num_beams\": 6\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file /home/alexander/nlg-project/BART-base/checkpoint-25000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BartForConditionalGeneration.\n",
      "\n",
      "All the weights of BartForConditionalGeneration were initialized from the model checkpoint at /home/alexander/nlg-project/BART-base/checkpoint-25000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7202\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='451' max='451' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [451/451 00:31]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8116941452026367, 'eval_runtime': 31.4397, 'eval_samples_per_second': 229.074, 'eval_steps_per_second': 14.345}\n",
      "mean syl line 0 : 6.48046875\n",
      "std syl line 0 : 1.7023677717442367\n",
      "mean syl line 1 : 9.73046875\n",
      "std syl line 1 : 2.321269136759768\n",
      "mean syl line 2 : 6.564453125\n",
      "std syl line 2 : 1.605576857293893\n",
      "{'rouge1': 0.28379626945691616, 'rouge2': 0.05750800428608828, 'rougeL': 0.21681880911650797, 'rougeLsum': 0.27448841387104017}\n",
      "BertScore f1 mean:  0.8406280169729143\n",
      "BertScore recall mean:  0.8398150866851211\n",
      "BertScore precision mean:  0.8415693667484447\n",
      "Auto Rouge-1 mean:  0.13393355905846266\n",
      "Auto Rouge-2 mean:  0.03923022589649612\n",
      "Self similarity mean:  0.115445509352878\n",
      "generations\n",
      "[' just because someone\\ndoesnt love you doesnt mean\\nyou dont love em back', ' you should be writing\\nlike an aller dreaming in\\nlove with a sickness', ' this tornado is\\nmaking me want to go to\\nmississippi now', ' you should be writing\\nlike a physician in love\\nwith a physician', ' marlon lewis is\\none of the best players in\\nthe baseball leagues', ' my professor is\\ngoing to be at work in\\nan hour or so', ' thank you warriors\\nfor your hard work and hard work\\nand teamwork', ' sometimes i feel like\\nit kills me when i dont know\\nhow to connect', ' you should be writing\\nlike an idiot in love\\nwith a max stupidity', ' i suggest that we\\nlove each other as much as\\nwe love ourselves', ' you should be writing\\nlike a changeful rhythm\\nin love with a song', ' i had a dream last\\nnight that i was going to\\nhave a dream today', ' you should be writing\\nlike a log in love with a\\nbook neglecting it', ' its so early in\\nthe morning and i dont know\\nhow to get off work', ' sometimes i feel like\\nim the only one who talks\\nto people close friends', ' texas is whack\\nas hell and i dont know how\\nto feel about it', ' you should be writing\\nlike an offenders in love\\nwith a trialflames', ' betrayal is the\\nworst heartbreak ive ever felt\\nin my entire life', ' you should be writing\\nlike a canvas in love with\\na list of words', ' a glimpse\\nof a mallard\\nsummer dusk', ' yeah im coming home\\ntomorrow and i dont know\\nwhat to do with life', ' this story is so\\ncute i dont even know what\\nto do with my life', ' am i the only\\none who definitely\\nwants to be sent home', ' you should be writing\\nlike a henderson fan in\\nlove with your odds', ' i pray every\\ntime i go to the gym i\\nmiss midterms so much', ' i havent hooked\\nup with a wii player\\nin almost a week', ' fluent in french\\nis one of the best feelings\\ni have ever felt', ' am i the only\\none who doesnt combine my\\nbirthday dates with dates', ' ephesus\\nis one of the bitterest\\nthings ive ever seen', ' my boyfriend is such\\na good person and i love\\nhis timing so much', ' thanks jessica fro\\nfor the follow i m very\\nhappy about it', ' you should be writing\\nlike an issue in love with\\na potent issue', ' thanks for the follow\\nburley i hope you have a\\nvery fine sag day', ' you should be writing\\nlike an asylum fleeing\\nfrom a claim to be', ' am i the only\\none who doesnt like the mix\\noriginal mix', ' i feel poorly\\nand rotten and i dont know\\nwhat to do with it', ' i cant remember\\nthe last time i actually\\ndid this to myself', ' ive never been more\\nimpressed with my writing than\\ni am with my books', ' you should be writing\\nlike a blurring line\\nin love with chords', ' anyways im so\\ntoxic and i dont know what\\nto do with myself', ' winter stars\\na constellation of trucks\\nacross the road', ' im driving to la\\nand i dont know what i want\\nto do with my life', ' i have stolen my\\nsecond phone and i dont know\\nwhat to do with it', ' am i the only\\none that gets in trouble with\\nmy english teacher', ' you should be writing\\nlike an idolize a\\nhustle in love with', ' you should be writing\\nlike a styria in love\\nwith a mapping app', ' some people are just\\nso hype they dont even know\\nhow to live their lives', ' does anyone know\\nwhere i can borrow a harp\\nfrom harry potter', ' puddled leaves\\nthe dog finds its way\\nto the window', ' sometimes i wish i\\nhad brains so i could be more\\nhonest with myself', ' you should be writing\\nlike a chemistry in love\\nwith a sense of humor', ' melo is the most\\nfucking foul player ive ever\\never seen', ' all i wanna do\\nis buy christmas presents and\\nmake crafty christmas', ' happy belated\\nbirthday toya hope you have\\na wonderful day', ' theres alot going\\non in my life right now and\\ni dont even know', ' ive been controlling\\nmyself for so long i cant\\nhandle myself', ' i forgave myself\\ntoday and now i feel like\\nim in jail again', ' ive never felt so\\nexhausted in my life\\nlike this before sigh', ' thank you dukes braves for\\nthe follow i m very\\nhappy about it', ' am i the only\\none who doesnt exchange their\\naffiliations', ' you should be writing\\nlike an established runner\\nin love with a friend', ' curtain curtain\\nthe sound of rain\\nbillows', ' fabling thank you for\\nthe follow i hope you have\\na fabling sprinkle', ' you should be writing\\nlike a gazillion germs\\nin love with a song']\n",
      "titles\n",
      "[' em doesnt', ' aller sickness', ' mississippi tornado', ' physician moreover', ' marlon leagues', ' professor hour', ' teamwork warriors', ' connect kills', ' stupidity max', ' suggest ourselves', ' changeful rhythm', ' dream', ' neglecting log', ' early off', ' talks close', ' whack texas', ' trialflames offenders', ' betrayal heartbreak', ' canvas list', ' mallard glimpse', ' yeah coming', ' story cute', ' sent definitely', ' henderson odds', ' midterms pray', ' wii hooked', ' fluent french', ' combine dates', ' ephesus bitterest', ' timing boyfriend', ' fro jessica', ' potent issue', ' burley sag', ' asylum claim', ' original mix', ' poorly rotten', ' actually remember', ' impressed books', ' blurring chords', ' toxic anyways', ' constellation trucks', ' driving la', ' stolen second', ' trouble teacher', ' idolize hustle', ' styria mapping', ' hype lives', ' borrow potter', ' puddled finds', ' brains honest', ' chemistry humor', ' melo foul', ' crafty presents', ' toya belated', ' alot theres', ' controlling handle', ' forgave jail', ' exhausted sigh', ' dukes braves', ' identities exchange', ' established runner', ' billows curtain', ' fabling sprinkle', ' gazillion germs']\n",
      "references\n",
      "[' doing it doesnt\\ndo it to em em doing\\nit does it to em', ' im starting a gang\\nto fight the summer sickness\\naller g unit', ' tornado warning\\nfor mississippi county\\neveryone stay safe', ' the two lines may be translated love is the physician of life\\nand next to our lord himself moreover it is the way that goes\\nstraight to heaven', ' of course im facing\\nmarlon mack in both of my\\nmoney leagues this week', ' our professor\\njust let us out an hour\\nearly god is good', ' congratulations\\non the win awesome teamwork\\nlady warriors', ' feeling very dumb\\nas the other half kills it\\nat only connect', ' stupidity at\\nits max youre a dumb bitch if\\nyou do stuff like this', ' i suggest we learn\\nto love ourselves before\\nits made illegal', ' waiting for you\\nthe changeful rhythm\\nof the rain', ' the dream\\nthe dream\\nthe dream', ' finally getting\\nmy life ive been neglecting\\nmyself for too log', ' i get off early\\ntoday but i really dont\\neven want to go', ' i wont ever let\\nsomeone who talks bad about\\nme be close to me', ' texas living is\\nso whack all they do for fun\\nis go out and drink', ' when offenders are indited\\nand with trialflames ignited\\nelsewhere ill attend if cited', ' heartbreak is the worst\\nyou really feel it in your\\nchest the betrayal', ' looking at my long\\nto do list in canvas makes\\nme want to throw up', ' golden waters still\\nautumn leaves fading away\\nglimpse of mallard ducks', ' having my way with\\nthe money yeah that shit keep\\ncoming and coming', ' my story as been\\nso cute i really had the\\nbest time this weekend', ' i definitely\\njust sent this to my mom and\\ntold her to make it', ' odds on henderson\\nto lift the champions league\\nand world cup trophy', ' after midterms and\\nall i can do is hope and\\npray cause heaven knows', ' ive been wanting to\\nplay tennis for a while so\\ni hooked up the wii', ' if anyone has\\nfrench homework ill do it for\\ni speak fluent french', ' any idea\\nwhen the combine windows and\\ndates will be released', ' but the bitterest disgrace\\nis to see forever the face\\nof the monk of ephesus', ' my boyfriend always\\nwanna be making love but\\nat the wrong timing', ' baby jessica\\nand this fro is giving me\\nso much life on the', ' they issue forth and we who never knew\\ntill then how potent and how real they were\\ntake them and wonder and so bless the hour', ' sag you remember\\nthat word burley going to\\nland you in prison', ' theyre trying to claim\\nasylum theres no reason\\nto fucking do this', ' are you still going\\nto release do it for me\\noriginal mix', ' feel fucking rotten\\nsurvived the first week in work\\nbut im so poorly', ' dont remember the\\nlast time i actually\\ngot ready for school', ' im not impressed by\\nmoney im impressed by how\\nyou look and read books', ' from my bed i can hear him and the round\\nnotes flutter and tap about the room and hit against each\\nother blurring to unexpected chords', ' anyways life is\\ntoo beautiful to be stressed\\nby toxic people', ' just over the fence\\na constellation of stars\\nand trucks', ' driving through la in\\nthe rain is enough to make\\nsomeone go crazy', ' this is the second\\ngame night ive been to and have\\nhad my shit stolen', ' i got in trouble\\nfor asking my teacher to\\nnot dumb down his class', ' i dont idolize\\njay z the person i just\\nadmire his hustle', ' lesson of the day syria and styria\\nfor syria read his conquering banner shook from syria\\nand for styria look at this harp of blood mapping', ' yo being in love\\nwith your best friend really lives\\nup to the hype wow', ' anyone got a\\nharry potter outfit i\\ncan borrow next month', ' puddled field\\nthe cat finds sunlight\\non a fence post', ' lets ask our hearts\\nrather than our brains thats\\nbeing more honest', ' love the chemistry\\nknowledge and sense of humor\\nthe three of you share', ' that was great defense\\nby melo but that was not\\na foul on melo', ' gonna be crafty\\nfor the rest of the day and\\nmake birthday presents', ' happy belated\\nbirthday toya yall looking\\ntoo cute as always', ' baby theres alot\\nof things in my heart i want\\nto tell you about', ' controlling only\\nwhat i can control letting\\ngod handle the rest', ' glad he forgave him\\nbut that shit is gone eat at\\ndude while he in jail', ' can yall fucking choose\\nwhere harry is already\\nsigh im exhausted', ' dukes failed to mention\\nthat the opening day game\\nwas against the braves', ' if were going to\\nexchange identities ill\\nbe needing your clothes', ' he definitely\\nestablished himself as a\\nrunner no question', ' a curtain billows\\nbefore the rain\\nscent of roses', ' which poets prone to lie have paved with gold\\nwhich poets sprinkle oer with sands of gold\\nwhich fabling poets', ' a gazillion germs\\nin a sneeze\\nyet more stars']\n"
     ]
    }
   ],
   "source": [
    "finetuned_generator_tester = test_finetuned('/home/alexander/nlg-project/BART-base/' + 'checkpoint-25000',eval_ds_plain)\n",
    "finetuned_generator_tester.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
