{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-333f30692099>:7: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"rouge\")\n"
     ]
    }
   ],
   "source": [
    "from transformers import BartTokenizer, BartModel,BartForConditionalGeneration\n",
    "import pandas as pd\n",
    "from datasets import load_metric\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import  Seq2SeqTrainingArguments, Seq2SeqTrainer,DataCollatorForSeq2Seq\n",
    "metric = load_metric(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import encode_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Model \n",
    "# use checkpoint at 32000 iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-base')\n",
    "model = BartForConditionalGeneration.from_pretrained('facebook/bart-base')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retreive data\n",
    "Right now just using kaggle corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/alexander/nlg-project/data'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data_dir = '/home/alexander/nlg-project/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Unnamed: 0                                               Poem  \\\n",
      "0             7542  the mockingbird has testified\\nto springs exis...   \n",
      "1           116211  ive actually\\nheld a black card in my hand\\nan...   \n",
      "2            31718  although alas the\\ncoat on his back is a coat\\...   \n",
      "3            97130  these memories dont\\nmean anything to you and\\...   \n",
      "4            97063  a mad old lady\\nwhooped her tits out at me in\\...   \n",
      "...            ...                                                ...   \n",
      "136820      133722  the manager hugged\\nme like u back see i am\\na...   \n",
      "136821       50010  an entire plot\\nbased around klaus tripping an...   \n",
      "136822       96222  why cant paulina\\nhave social media i\\nmiss he...   \n",
      "136823        5797  i feel used he said\\nand old as usual\\ni belie...   \n",
      "136824       51245  tip if youre upset\\ntake a walk or jog it off\\...   \n",
      "\n",
      "                   Title  \n",
      "0       testified almond  \n",
      "1              held card  \n",
      "2        lining although  \n",
      "3          memories mean  \n",
      "4            whooped pub  \n",
      "...                  ...  \n",
      "136820    hugged manager  \n",
      "136821    klaus tripping  \n",
      "136822     paulina maddy  \n",
      "136823      height usual  \n",
      "136824           jog tip  \n",
      "\n",
      "[136825 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "test_df = pd.read_csv(processed_data_dir + 'test_data.csv')\n",
    "train_df = pd.read_csv(processed_data_dir + 'train_data.csv')\n",
    "print(train_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_df = pd.read_csv(processed_data_dir + 'kaggle_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>line_0</th>\n",
       "      <th>line_1</th>\n",
       "      <th>line_2</th>\n",
       "      <th>source</th>\n",
       "      <th>valid</th>\n",
       "      <th>line_0_scount</th>\n",
       "      <th>line_1_scount</th>\n",
       "      <th>line_2_scount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>fishing boats</td>\n",
       "      <td>colors of</td>\n",
       "      <td>the rainbow</td>\n",
       "      <td>tempslibres</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>ash wednesday</td>\n",
       "      <td>trying to remember</td>\n",
       "      <td>my dream</td>\n",
       "      <td>tempslibres</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>snowy morn</td>\n",
       "      <td>pouring another cup</td>\n",
       "      <td>of black coffee</td>\n",
       "      <td>tempslibres</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>shortest day</td>\n",
       "      <td>flames dance</td>\n",
       "      <td>in the oven</td>\n",
       "      <td>tempslibres</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>haze</td>\n",
       "      <td>half the horse hidden</td>\n",
       "      <td>behind the house</td>\n",
       "      <td>tempslibres</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144022</th>\n",
       "      <td>144118</td>\n",
       "      <td>im not asking did</td>\n",
       "      <td>you say it nor clarify</td>\n",
       "      <td>what you said neither</td>\n",
       "      <td>twaiku</td>\n",
       "      <td>True</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144023</th>\n",
       "      <td>144119</td>\n",
       "      <td>you are truly a</td>\n",
       "      <td>moron or a liar im</td>\n",
       "      <td>inclined to think both</td>\n",
       "      <td>twaiku</td>\n",
       "      <td>True</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144024</th>\n",
       "      <td>144120</td>\n",
       "      <td>aint no selfie on</td>\n",
       "      <td>this earth thats gonna make me</td>\n",
       "      <td>like theresa may</td>\n",
       "      <td>twaiku</td>\n",
       "      <td>True</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144025</th>\n",
       "      <td>144121</td>\n",
       "      <td>is doing a great</td>\n",
       "      <td>job turning independents</td>\n",
       "      <td>into democrats</td>\n",
       "      <td>twaiku</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144026</th>\n",
       "      <td>144122</td>\n",
       "      <td>wanted to send a</td>\n",
       "      <td>quick follow up on if the</td>\n",
       "      <td>blood is loud talk soon</td>\n",
       "      <td>twaiku</td>\n",
       "      <td>True</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>144027 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0             line_0                          line_1  \\\n",
       "0                0      fishing boats                       colors of   \n",
       "1                1      ash wednesday              trying to remember   \n",
       "2                2         snowy morn             pouring another cup   \n",
       "3                3       shortest day                    flames dance   \n",
       "4                4               haze           half the horse hidden   \n",
       "...            ...                ...                             ...   \n",
       "144022      144118  im not asking did          you say it nor clarify   \n",
       "144023      144119    you are truly a              moron or a liar im   \n",
       "144024      144120  aint no selfie on  this earth thats gonna make me   \n",
       "144025      144121   is doing a great        job turning independents   \n",
       "144026      144122   wanted to send a       quick follow up on if the   \n",
       "\n",
       "                         line_2       source  valid  line_0_scount  \\\n",
       "0                   the rainbow  tempslibres   True              4   \n",
       "1                      my dream  tempslibres   True              5   \n",
       "2               of black coffee  tempslibres   True              3   \n",
       "3                   in the oven  tempslibres   True              4   \n",
       "4              behind the house  tempslibres   True              2   \n",
       "...                         ...          ...    ...            ...   \n",
       "144022    what you said neither       twaiku   True              7   \n",
       "144023   inclined to think both       twaiku   True              7   \n",
       "144024         like theresa may       twaiku   True              6   \n",
       "144025           into democrats       twaiku   True              5   \n",
       "144026  blood is loud talk soon       twaiku   True              6   \n",
       "\n",
       "        line_1_scount  line_2_scount  \n",
       "0                   3              3  \n",
       "1                   6              2  \n",
       "2                   7              6  \n",
       "3                   5              5  \n",
       "4                   7              6  \n",
       "...               ...            ...  \n",
       "144022             10              8  \n",
       "144023              6              8  \n",
       "144024             11              6  \n",
       "144025              8              5  \n",
       "144026              9              8  \n",
       "\n",
       "[144027 rows x 9 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whole_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def syllable_vec(row):\n",
    "    return [row['line_0_scount'],row['line_1_scount'],row['line_2_scount']]\n",
    "\n",
    "def change_title(row):\n",
    "    #adds syllable info to title\n",
    "    #print(row['Title'])\n",
    "    return str(row['syllables']) + ';' + str(row['Title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_syllables_title(part_df, match_df):\n",
    "    merged = pd.merge(part_df, match_df,how='left', left_on=['Unnamed: 0'], right_index=True)\n",
    "    merged['syllables'] = merged.apply(syllable_vec,axis=1)\n",
    "    merged.drop(columns=['Unnamed: 0_x', 'Unnamed: 0_y','line_0','line_1','line_2','source','valid','line_0_scount',\n",
    "                        'line_1_scount','line_2_scount'],inplace=True)\n",
    "    merged['Title'] = merged.apply(change_title,axis=1)\n",
    "    \n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = add_syllables_title(train_df,whole_df)\n",
    "test_df = add_syllables_title(test_df,whole_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Poem</th>\n",
       "      <th>Title</th>\n",
       "      <th>syllables</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the mockingbird has testified\\nto springs exis...</td>\n",
       "      <td>[10, 9, 11];testified almond</td>\n",
       "      <td>[10, 9, 11]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ive actually\\nheld a black card in my hand\\nan...</td>\n",
       "      <td>[5, 9, 8];held card</td>\n",
       "      <td>[5, 9, 8]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>although alas the\\ncoat on his back is a coat\\...</td>\n",
       "      <td>[6, 9, 5];lining although</td>\n",
       "      <td>[6, 9, 5]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>these memories dont\\nmean anything to you and\\...</td>\n",
       "      <td>[7, 10, 7];memories mean</td>\n",
       "      <td>[7, 10, 7]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a mad old lady\\nwhooped her tits out at me in\\...</td>\n",
       "      <td>[6, 12, 5];whooped pub</td>\n",
       "      <td>[6, 12, 5]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136820</th>\n",
       "      <td>the manager hugged\\nme like u back see i am\\na...</td>\n",
       "      <td>[7, 9, 7];hugged manager</td>\n",
       "      <td>[7, 9, 7]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136821</th>\n",
       "      <td>an entire plot\\nbased around klaus tripping an...</td>\n",
       "      <td>[5, 10, 7];klaus tripping</td>\n",
       "      <td>[5, 10, 7]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136822</th>\n",
       "      <td>why cant paulina\\nhave social media i\\nmiss he...</td>\n",
       "      <td>[6, 8, 7];paulina maddy</td>\n",
       "      <td>[6, 8, 7]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136823</th>\n",
       "      <td>i feel used he said\\nand old as usual\\ni belie...</td>\n",
       "      <td>[7, 5, 16];height usual</td>\n",
       "      <td>[7, 5, 16]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136824</th>\n",
       "      <td>tip if youre upset\\ntake a walk or jog it off\\...</td>\n",
       "      <td>[8, 9, 8];jog tip</td>\n",
       "      <td>[8, 9, 8]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>136825 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     Poem  \\\n",
       "0       the mockingbird has testified\\nto springs exis...   \n",
       "1       ive actually\\nheld a black card in my hand\\nan...   \n",
       "2       although alas the\\ncoat on his back is a coat\\...   \n",
       "3       these memories dont\\nmean anything to you and\\...   \n",
       "4       a mad old lady\\nwhooped her tits out at me in\\...   \n",
       "...                                                   ...   \n",
       "136820  the manager hugged\\nme like u back see i am\\na...   \n",
       "136821  an entire plot\\nbased around klaus tripping an...   \n",
       "136822  why cant paulina\\nhave social media i\\nmiss he...   \n",
       "136823  i feel used he said\\nand old as usual\\ni belie...   \n",
       "136824  tip if youre upset\\ntake a walk or jog it off\\...   \n",
       "\n",
       "                               Title    syllables  \n",
       "0       [10, 9, 11];testified almond  [10, 9, 11]  \n",
       "1                [5, 9, 8];held card    [5, 9, 8]  \n",
       "2          [6, 9, 5];lining although    [6, 9, 5]  \n",
       "3           [7, 10, 7];memories mean   [7, 10, 7]  \n",
       "4             [6, 12, 5];whooped pub   [6, 12, 5]  \n",
       "...                              ...          ...  \n",
       "136820      [7, 9, 7];hugged manager    [7, 9, 7]  \n",
       "136821     [5, 10, 7];klaus tripping   [5, 10, 7]  \n",
       "136822       [6, 8, 7];paulina maddy    [6, 8, 7]  \n",
       "136823       [7, 5, 16];height usual   [7, 5, 16]  \n",
       "136824             [8, 9, 8];jog tip    [8, 9, 8]  \n",
       "\n",
       "[136825 rows x 3 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model = encode_sentences(tokenizer,train_df)\n",
    "test_model = encode_sentences(tokenizer,test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoemDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.labels = df['labels']\n",
    "        self.mask = df['attention_mask']\n",
    "        self.input = df['input_ids']\n",
    "        #self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        input_ids = self.input[idx]\n",
    "        attention_masks = self.mask[idx]\n",
    "        target_ids = self.labels[idx]\n",
    "        batch = {\n",
    "          \"input_ids\": input_ids,\n",
    "          \"decoder_attention_mask\": torch.tensor([1] * 64),\n",
    "          \"attention_mask\": attention_masks,\n",
    "          \"label_ids\": target_ids,\n",
    "        }\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = PoemDataset(train_model)\n",
    "eval_ds = PoemDataset(test_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16 \n",
    "\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    \"BART-base-syllable-naive\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    #learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "   # weight_decay=0.01,\n",
    "   # save_total_limit=3,\n",
    "   # num_train_epochs=1,\n",
    "   # predict_with_generate=True,\n",
    "    \n",
    "    learning_rate=1e-4,\n",
    "    weight_decay=0.01,\n",
    "    adam_beta1=0.9,\n",
    "    adam_beta2=0.999,\n",
    "    adam_epsilon=1e-6,\n",
    "    max_grad_norm=1.0,\n",
    "    num_train_epochs=10,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    warmup_ratio=0.1,\n",
    "   # label_names=\"labels\",\n",
    "    #fp16=True,\n",
    "  #  use_auth_token=False\n",
    "    #push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_collator = Collator_poem_gen(tokenizer=tokenizer, max_length=512)\n",
    "#train_ds = PoemDataset(model_input)\n",
    "#eval_ds = PoemDataset(model_input)\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "   # data_collator=data_collator,\n",
    "    train_dataset=train_ds,\n",
    "    \n",
    "    eval_dataset=eval_ds,\n",
    "    #use_auth_token=False,\n",
    "    \n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -c \"from huggingface_hub.hf_api import HfFolder; HfFolder.save_token('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')\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexander/anaconda3/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 136825\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 85520\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='44011' max='85520' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [44011/85520 7:46:46 < 7:20:15, 1.57 it/s, Epoch 5.15/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.451300</td>\n",
       "      <td>0.422517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.414300</td>\n",
       "      <td>0.400288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.382900</td>\n",
       "      <td>0.389930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.344900</td>\n",
       "      <td>0.388026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.315000</td>\n",
       "      <td>0.391123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.279700</td>\n",
       "      <td>0.403206</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to BART-base-syllable-naive/checkpoint-500\n",
      "Configuration saved in BART-base-syllable-naive/checkpoint-500/config.json\n",
      "Model weights saved in BART-base-syllable-naive/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base-syllable-naive/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in BART-base-syllable-naive/checkpoint-500/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base-syllable-naive/checkpoint-1000\n",
      "Configuration saved in BART-base-syllable-naive/checkpoint-1000/config.json\n",
      "Model weights saved in BART-base-syllable-naive/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base-syllable-naive/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in BART-base-syllable-naive/checkpoint-1000/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base-syllable-naive/checkpoint-1500\n",
      "Configuration saved in BART-base-syllable-naive/checkpoint-1500/config.json\n",
      "Model weights saved in BART-base-syllable-naive/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base-syllable-naive/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in BART-base-syllable-naive/checkpoint-1500/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base-syllable-naive/checkpoint-2000\n",
      "Configuration saved in BART-base-syllable-naive/checkpoint-2000/config.json\n",
      "Model weights saved in BART-base-syllable-naive/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base-syllable-naive/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in BART-base-syllable-naive/checkpoint-2000/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base-syllable-naive/checkpoint-2500\n",
      "Configuration saved in BART-base-syllable-naive/checkpoint-2500/config.json\n",
      "Model weights saved in BART-base-syllable-naive/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base-syllable-naive/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in BART-base-syllable-naive/checkpoint-2500/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base-syllable-naive/checkpoint-3000\n",
      "Configuration saved in BART-base-syllable-naive/checkpoint-3000/config.json\n",
      "Model weights saved in BART-base-syllable-naive/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base-syllable-naive/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in BART-base-syllable-naive/checkpoint-3000/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base-syllable-naive/checkpoint-3500\n",
      "Configuration saved in BART-base-syllable-naive/checkpoint-3500/config.json\n",
      "Model weights saved in BART-base-syllable-naive/checkpoint-3500/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base-syllable-naive/checkpoint-3500/tokenizer_config.json\n",
      "Special tokens file saved in BART-base-syllable-naive/checkpoint-3500/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base-syllable-naive/checkpoint-4000\n",
      "Configuration saved in BART-base-syllable-naive/checkpoint-4000/config.json\n",
      "Model weights saved in BART-base-syllable-naive/checkpoint-4000/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base-syllable-naive/checkpoint-4000/tokenizer_config.json\n",
      "Special tokens file saved in BART-base-syllable-naive/checkpoint-4000/special_tokens_map.json\n",
      "Model weights saved in BART-base-syllable-naive/checkpoint-4500/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base-syllable-naive/checkpoint-4500/tokenizer_config.json\n",
      "Special tokens file saved in BART-base-syllable-naive/checkpoint-4500/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base-syllable-naive/checkpoint-5000\n",
      "Configuration saved in BART-base-syllable-naive/checkpoint-5000/config.json\n",
      "Model weights saved in BART-base-syllable-naive/checkpoint-5000/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base-syllable-naive/checkpoint-5000/tokenizer_config.json\n",
      "Special tokens file saved in BART-base-syllable-naive/checkpoint-5000/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base-syllable-naive/checkpoint-5500\n",
      "Configuration saved in BART-base-syllable-naive/checkpoint-5500/config.json\n",
      "Model weights saved in BART-base-syllable-naive/checkpoint-5500/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base-syllable-naive/checkpoint-5500/tokenizer_config.json\n",
      "Special tokens file saved in BART-base-syllable-naive/checkpoint-5500/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base-syllable-naive/checkpoint-6000\n",
      "Configuration saved in BART-base-syllable-naive/checkpoint-6000/config.json\n",
      "Model weights saved in BART-base-syllable-naive/checkpoint-6000/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base-syllable-naive/checkpoint-6000/tokenizer_config.json\n",
      "Special tokens file saved in BART-base-syllable-naive/checkpoint-6000/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base-syllable-naive/checkpoint-6500\n",
      "Configuration saved in BART-base-syllable-naive/checkpoint-6500/config.json\n",
      "Model weights saved in BART-base-syllable-naive/checkpoint-6500/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base-syllable-naive/checkpoint-6500/tokenizer_config.json\n",
      "Special tokens file saved in BART-base-syllable-naive/checkpoint-6500/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base-syllable-naive/checkpoint-7000\n",
      "Configuration saved in BART-base-syllable-naive/checkpoint-7000/config.json\n",
      "Model weights saved in BART-base-syllable-naive/checkpoint-7000/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base-syllable-naive/checkpoint-7000/tokenizer_config.json\n",
      "Special tokens file saved in BART-base-syllable-naive/checkpoint-7000/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base-syllable-naive/checkpoint-7500\n",
      "Configuration saved in BART-base-syllable-naive/checkpoint-7500/config.json\n",
      "Model weights saved in BART-base-syllable-naive/checkpoint-7500/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base-syllable-naive/checkpoint-7500/tokenizer_config.json\n",
      "Special tokens file saved in BART-base-syllable-naive/checkpoint-7500/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base-syllable-naive/checkpoint-8000\n",
      "Configuration saved in BART-base-syllable-naive/checkpoint-8000/config.json\n",
      "Model weights saved in BART-base-syllable-naive/checkpoint-8000/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base-syllable-naive/checkpoint-8000/tokenizer_config.json\n",
      "Special tokens file saved in BART-base-syllable-naive/checkpoint-8000/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base-syllable-naive/checkpoint-8500\n",
      "Configuration saved in BART-base-syllable-naive/checkpoint-8500/config.json\n",
      "Model weights saved in BART-base-syllable-naive/checkpoint-8500/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base-syllable-naive/checkpoint-8500/tokenizer_config.json\n",
      "Special tokens file saved in BART-base-syllable-naive/checkpoint-8500/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7202\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to BART-base-syllable-naive/checkpoint-9000\n",
      "Configuration saved in BART-base-syllable-naive/checkpoint-9000/config.json\n",
      "Model weights saved in BART-base-syllable-naive/checkpoint-9000/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base-syllable-naive/checkpoint-9000/tokenizer_config.json\n",
      "Special tokens file saved in BART-base-syllable-naive/checkpoint-9000/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base-syllable-naive/checkpoint-9500\n",
      "Configuration saved in BART-base-syllable-naive/checkpoint-9500/config.json\n",
      "Model weights saved in BART-base-syllable-naive/checkpoint-9500/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base-syllable-naive/checkpoint-9500/tokenizer_config.json\n",
      "Special tokens file saved in BART-base-syllable-naive/checkpoint-9500/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base-syllable-naive/checkpoint-10000\n",
      "Configuration saved in BART-base-syllable-naive/checkpoint-10000/config.json\n",
      "Model weights saved in BART-base-syllable-naive/checkpoint-10000/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base-syllable-naive/checkpoint-10000/tokenizer_config.json\n",
      "Special tokens file saved in BART-base-syllable-naive/checkpoint-10000/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to BART-base-syllable-naive/checkpoint-10500\n",
      "Configuration saved in BART-base-syllable-naive/checkpoint-10500/config.json\n",
      "Model weights saved in BART-base-syllable-naive/checkpoint-10500/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base-syllable-naive/checkpoint-10500/tokenizer_config.json\n",
      "Special tokens file saved in BART-base-syllable-naive/checkpoint-10500/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base-syllable-naive/checkpoint-11000\n",
      "Configuration saved in BART-base-syllable-naive/checkpoint-11000/config.json\n",
      "Model weights saved in BART-base-syllable-naive/checkpoint-11000/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base-syllable-naive/checkpoint-11000/tokenizer_config.json\n",
      "Special tokens file saved in BART-base-syllable-naive/checkpoint-11000/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base-syllable-naive/checkpoint-12000\n",
      "Configuration saved in BART-base-syllable-naive/checkpoint-12000/config.json\n",
      "Model weights saved in BART-base-syllable-naive/checkpoint-12000/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base-syllable-naive/checkpoint-12000/tokenizer_config.json\n",
      "Special tokens file saved in BART-base-syllable-naive/checkpoint-12000/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base-syllable-naive/checkpoint-12500\n",
      "Configuration saved in BART-base-syllable-naive/checkpoint-12500/config.json\n",
      "Model weights saved in BART-base-syllable-naive/checkpoint-12500/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base-syllable-naive/checkpoint-12500/tokenizer_config.json\n",
      "Special tokens file saved in BART-base-syllable-naive/checkpoint-12500/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base-syllable-naive/checkpoint-13000\n",
      "Configuration saved in BART-base-syllable-naive/checkpoint-13000/config.json\n",
      "Model weights saved in BART-base-syllable-naive/checkpoint-13000/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base-syllable-naive/checkpoint-13000/tokenizer_config.json\n",
      "Special tokens file saved in BART-base-syllable-naive/checkpoint-13000/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base-syllable-naive/checkpoint-13500\n",
      "Configuration saved in BART-base-syllable-naive/checkpoint-13500/config.json\n",
      "Model weights saved in BART-base-syllable-naive/checkpoint-13500/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base-syllable-naive/checkpoint-13500/tokenizer_config.json\n",
      "Special tokens file saved in BART-base-syllable-naive/checkpoint-13500/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base-syllable-naive/checkpoint-14000\n",
      "Configuration saved in BART-base-syllable-naive/checkpoint-14000/config.json\n",
      "Model weights saved in BART-base-syllable-naive/checkpoint-14000/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base-syllable-naive/checkpoint-14000/tokenizer_config.json\n",
      "Special tokens file saved in BART-base-syllable-naive/checkpoint-14000/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base-syllable-naive/checkpoint-14500\n",
      "Configuration saved in BART-base-syllable-naive/checkpoint-14500/config.json\n",
      "Model weights saved in BART-base-syllable-naive/checkpoint-14500/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base-syllable-naive/checkpoint-14500/tokenizer_config.json\n",
      "Special tokens file saved in BART-base-syllable-naive/checkpoint-14500/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base-syllable-naive/checkpoint-15000\n",
      "Configuration saved in BART-base-syllable-naive/checkpoint-15000/config.json\n",
      "Model weights saved in BART-base-syllable-naive/checkpoint-15000/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base-syllable-naive/checkpoint-15000/tokenizer_config.json\n",
      "Special tokens file saved in BART-base-syllable-naive/checkpoint-15000/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base-syllable-naive/checkpoint-15500\n",
      "Configuration saved in BART-base-syllable-naive/checkpoint-15500/config.json\n",
      "Model weights saved in BART-base-syllable-naive/checkpoint-15500/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base-syllable-naive/checkpoint-15500/tokenizer_config.json\n",
      "Special tokens file saved in BART-base-syllable-naive/checkpoint-15500/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base-syllable-naive/checkpoint-16000\n",
      "Configuration saved in BART-base-syllable-naive/checkpoint-16000/config.json\n",
      "Model weights saved in BART-base-syllable-naive/checkpoint-16000/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base-syllable-naive/checkpoint-16000/tokenizer_config.json\n",
      "Special tokens file saved in BART-base-syllable-naive/checkpoint-16000/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base-syllable-naive/checkpoint-16500\n",
      "Configuration saved in BART-base-syllable-naive/checkpoint-16500/config.json\n",
      "Model weights saved in BART-base-syllable-naive/checkpoint-16500/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base-syllable-naive/checkpoint-16500/tokenizer_config.json\n",
      "Special tokens file saved in BART-base-syllable-naive/checkpoint-16500/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base-syllable-naive/checkpoint-17000\n",
      "Configuration saved in BART-base-syllable-naive/checkpoint-17000/config.json\n",
      "Model weights saved in BART-base-syllable-naive/checkpoint-17000/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base-syllable-naive/checkpoint-17000/tokenizer_config.json\n",
      "Special tokens file saved in BART-base-syllable-naive/checkpoint-17000/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7202\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to BART-base-syllable-naive/checkpoint-17500\n",
      "Configuration saved in BART-base-syllable-naive/checkpoint-17500/config.json\n",
      "Model weights saved in BART-base-syllable-naive/checkpoint-17500/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base-syllable-naive/checkpoint-17500/tokenizer_config.json\n",
      "Special tokens file saved in BART-base-syllable-naive/checkpoint-17500/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base-syllable-naive/checkpoint-18000\n",
      "Configuration saved in BART-base-syllable-naive/checkpoint-18000/config.json\n",
      "Model weights saved in BART-base-syllable-naive/checkpoint-18000/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base-syllable-naive/checkpoint-18000/tokenizer_config.json\n",
      "Special tokens file saved in BART-base-syllable-naive/checkpoint-18000/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base-syllable-naive/checkpoint-18500\n",
      "Configuration saved in BART-base-syllable-naive/checkpoint-18500/config.json\n",
      "Model weights saved in BART-base-syllable-naive/checkpoint-18500/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base-syllable-naive/checkpoint-18500/tokenizer_config.json\n",
      "Special tokens file saved in BART-base-syllable-naive/checkpoint-18500/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base-syllable-naive/checkpoint-19000\n",
      "Configuration saved in BART-base-syllable-naive/checkpoint-19000/config.json\n",
      "Model weights saved in BART-base-syllable-naive/checkpoint-19000/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base-syllable-naive/checkpoint-19000/tokenizer_config.json\n",
      "Special tokens file saved in BART-base-syllable-naive/checkpoint-19000/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base-syllable-naive/checkpoint-19500\n",
      "Configuration saved in BART-base-syllable-naive/checkpoint-19500/config.json\n",
      "Model weights saved in BART-base-syllable-naive/checkpoint-19500/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base-syllable-naive/checkpoint-19500/tokenizer_config.json\n",
      "Special tokens file saved in BART-base-syllable-naive/checkpoint-19500/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base-syllable-naive/checkpoint-20000\n",
      "Configuration saved in BART-base-syllable-naive/checkpoint-20000/config.json\n",
      "Model weights saved in BART-base-syllable-naive/checkpoint-20000/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base-syllable-naive/checkpoint-20000/tokenizer_config.json\n",
      "Special tokens file saved in BART-base-syllable-naive/checkpoint-20000/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base-syllable-naive/checkpoint-20500\n",
      "Configuration saved in BART-base-syllable-naive/checkpoint-20500/config.json\n",
      "Model weights saved in BART-base-syllable-naive/checkpoint-20500/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer config file saved in BART-base-syllable-naive/checkpoint-20500/tokenizer_config.json\n",
      "Special tokens file saved in BART-base-syllable-naive/checkpoint-20500/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base-syllable-naive/checkpoint-21000\n",
      "Configuration saved in BART-base-syllable-naive/checkpoint-21000/config.json\n",
      "Model weights saved in BART-base-syllable-naive/checkpoint-21000/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base-syllable-naive/checkpoint-21000/tokenizer_config.json\n",
      "Special tokens file saved in BART-base-syllable-naive/checkpoint-21000/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base-syllable-naive/checkpoint-21500\n",
      "Configuration saved in BART-base-syllable-naive/checkpoint-21500/config.json\n",
      "Model weights saved in BART-base-syllable-naive/checkpoint-21500/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base-syllable-naive/checkpoint-21500/tokenizer_config.json\n",
      "Special tokens file saved in BART-base-syllable-naive/checkpoint-21500/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base-syllable-naive/checkpoint-22000\n",
      "Configuration saved in BART-base-syllable-naive/checkpoint-22000/config.json\n",
      "Model weights saved in BART-base-syllable-naive/checkpoint-22000/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base-syllable-naive/checkpoint-22000/tokenizer_config.json\n",
      "Special tokens file saved in BART-base-syllable-naive/checkpoint-22000/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base-syllable-naive/checkpoint-22500\n",
      "Configuration saved in BART-base-syllable-naive/checkpoint-22500/config.json\n",
      "Model weights saved in BART-base-syllable-naive/checkpoint-22500/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base-syllable-naive/checkpoint-22500/tokenizer_config.json\n",
      "Special tokens file saved in BART-base-syllable-naive/checkpoint-22500/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base-syllable-naive/checkpoint-23000\n",
      "Configuration saved in BART-base-syllable-naive/checkpoint-23000/config.json\n",
      "Model weights saved in BART-base-syllable-naive/checkpoint-23000/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base-syllable-naive/checkpoint-23000/tokenizer_config.json\n",
      "Special tokens file saved in BART-base-syllable-naive/checkpoint-23000/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base-syllable-naive/checkpoint-23500\n",
      "Configuration saved in BART-base-syllable-naive/checkpoint-23500/config.json\n",
      "Model weights saved in BART-base-syllable-naive/checkpoint-23500/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base-syllable-naive/checkpoint-23500/tokenizer_config.json\n",
      "Special tokens file saved in BART-base-syllable-naive/checkpoint-23500/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base-syllable-naive/checkpoint-24000\n",
      "Configuration saved in BART-base-syllable-naive/checkpoint-24000/config.json\n",
      "Model weights saved in BART-base-syllable-naive/checkpoint-24000/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base-syllable-naive/checkpoint-24000/tokenizer_config.json\n",
      "Special tokens file saved in BART-base-syllable-naive/checkpoint-24000/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base-syllable-naive/checkpoint-24500\n",
      "Configuration saved in BART-base-syllable-naive/checkpoint-24500/config.json\n",
      "Model weights saved in BART-base-syllable-naive/checkpoint-24500/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base-syllable-naive/checkpoint-24500/tokenizer_config.json\n",
      "Special tokens file saved in BART-base-syllable-naive/checkpoint-24500/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base-syllable-naive/checkpoint-25000\n",
      "Configuration saved in BART-base-syllable-naive/checkpoint-25000/config.json\n",
      "Model weights saved in BART-base-syllable-naive/checkpoint-25000/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base-syllable-naive/checkpoint-25000/tokenizer_config.json\n",
      "Special tokens file saved in BART-base-syllable-naive/checkpoint-25000/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base-syllable-naive/checkpoint-25500\n",
      "Configuration saved in BART-base-syllable-naive/checkpoint-25500/config.json\n",
      "Model weights saved in BART-base-syllable-naive/checkpoint-25500/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base-syllable-naive/checkpoint-25500/tokenizer_config.json\n",
      "Special tokens file saved in BART-base-syllable-naive/checkpoint-25500/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7202\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to BART-base-syllable-naive/checkpoint-26000\n",
      "Configuration saved in BART-base-syllable-naive/checkpoint-26000/config.json\n",
      "Model weights saved in BART-base-syllable-naive/checkpoint-26000/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base-syllable-naive/checkpoint-26000/tokenizer_config.json\n",
      "Special tokens file saved in BART-base-syllable-naive/checkpoint-26000/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base-syllable-naive/checkpoint-26500\n",
      "Configuration saved in BART-base-syllable-naive/checkpoint-26500/config.json\n",
      "Model weights saved in BART-base-syllable-naive/checkpoint-26500/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base-syllable-naive/checkpoint-26500/tokenizer_config.json\n",
      "Special tokens file saved in BART-base-syllable-naive/checkpoint-26500/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base-syllable-naive/checkpoint-27000\n",
      "Configuration saved in BART-base-syllable-naive/checkpoint-27000/config.json\n",
      "Model weights saved in BART-base-syllable-naive/checkpoint-27000/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base-syllable-naive/checkpoint-27000/tokenizer_config.json\n",
      "Special tokens file saved in BART-base-syllable-naive/checkpoint-27000/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base-syllable-naive/checkpoint-27500\n",
      "Configuration saved in BART-base-syllable-naive/checkpoint-27500/config.json\n",
      "Model weights saved in BART-base-syllable-naive/checkpoint-27500/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base-syllable-naive/checkpoint-27500/tokenizer_config.json\n",
      "Special tokens file saved in BART-base-syllable-naive/checkpoint-27500/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base-syllable-naive/checkpoint-28000\n",
      "Configuration saved in BART-base-syllable-naive/checkpoint-28000/config.json\n",
      "Model weights saved in BART-base-syllable-naive/checkpoint-28000/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base-syllable-naive/checkpoint-28000/tokenizer_config.json\n",
      "Special tokens file saved in BART-base-syllable-naive/checkpoint-28000/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base-syllable-naive/checkpoint-28500\n",
      "Configuration saved in BART-base-syllable-naive/checkpoint-28500/config.json\n",
      "Model weights saved in BART-base-syllable-naive/checkpoint-28500/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base-syllable-naive/checkpoint-28500/tokenizer_config.json\n",
      "Special tokens file saved in BART-base-syllable-naive/checkpoint-28500/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base-syllable-naive/checkpoint-29000\n",
      "Configuration saved in BART-base-syllable-naive/checkpoint-29000/config.json\n",
      "Model weights saved in BART-base-syllable-naive/checkpoint-29000/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base-syllable-naive/checkpoint-29000/tokenizer_config.json\n",
      "Special tokens file saved in BART-base-syllable-naive/checkpoint-29000/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base-syllable-naive/checkpoint-29500\n",
      "Configuration saved in BART-base-syllable-naive/checkpoint-29500/config.json\n",
      "Model weights saved in BART-base-syllable-naive/checkpoint-29500/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base-syllable-naive/checkpoint-29500/tokenizer_config.json\n",
      "Special tokens file saved in BART-base-syllable-naive/checkpoint-29500/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base-syllable-naive/checkpoint-30000\n",
      "Configuration saved in BART-base-syllable-naive/checkpoint-30000/config.json\n",
      "Model weights saved in BART-base-syllable-naive/checkpoint-30000/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base-syllable-naive/checkpoint-30000/tokenizer_config.json\n",
      "Special tokens file saved in BART-base-syllable-naive/checkpoint-30000/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to BART-base-syllable-naive/checkpoint-30500\n",
      "Configuration saved in BART-base-syllable-naive/checkpoint-30500/config.json\n",
      "Model weights saved in BART-base-syllable-naive/checkpoint-30500/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base-syllable-naive/checkpoint-30500/tokenizer_config.json\n",
      "Special tokens file saved in BART-base-syllable-naive/checkpoint-30500/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base-syllable-naive/checkpoint-31000\n",
      "Configuration saved in BART-base-syllable-naive/checkpoint-31000/config.json\n",
      "Model weights saved in BART-base-syllable-naive/checkpoint-31000/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base-syllable-naive/checkpoint-31000/tokenizer_config.json\n",
      "Special tokens file saved in BART-base-syllable-naive/checkpoint-31000/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base-syllable-naive/checkpoint-31500\n",
      "Configuration saved in BART-base-syllable-naive/checkpoint-31500/config.json\n",
      "Model weights saved in BART-base-syllable-naive/checkpoint-31500/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base-syllable-naive/checkpoint-31500/tokenizer_config.json\n",
      "Special tokens file saved in BART-base-syllable-naive/checkpoint-31500/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base-syllable-naive/checkpoint-32000\n",
      "Configuration saved in BART-base-syllable-naive/checkpoint-32000/config.json\n",
      "Model weights saved in BART-base-syllable-naive/checkpoint-32000/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base-syllable-naive/checkpoint-32000/tokenizer_config.json\n",
      "Special tokens file saved in BART-base-syllable-naive/checkpoint-32000/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base-syllable-naive/checkpoint-32500\n",
      "Configuration saved in BART-base-syllable-naive/checkpoint-32500/config.json\n",
      "Model weights saved in BART-base-syllable-naive/checkpoint-32500/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base-syllable-naive/checkpoint-32500/tokenizer_config.json\n",
      "Special tokens file saved in BART-base-syllable-naive/checkpoint-32500/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base-syllable-naive/checkpoint-33000\n",
      "Configuration saved in BART-base-syllable-naive/checkpoint-33000/config.json\n",
      "Model weights saved in BART-base-syllable-naive/checkpoint-33000/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base-syllable-naive/checkpoint-33000/tokenizer_config.json\n",
      "Special tokens file saved in BART-base-syllable-naive/checkpoint-33000/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base-syllable-naive/checkpoint-33500\n",
      "Configuration saved in BART-base-syllable-naive/checkpoint-33500/config.json\n",
      "Model weights saved in BART-base-syllable-naive/checkpoint-33500/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base-syllable-naive/checkpoint-33500/tokenizer_config.json\n",
      "Special tokens file saved in BART-base-syllable-naive/checkpoint-33500/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base-syllable-naive/checkpoint-34000\n",
      "Configuration saved in BART-base-syllable-naive/checkpoint-34000/config.json\n",
      "Model weights saved in BART-base-syllable-naive/checkpoint-34000/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base-syllable-naive/checkpoint-34000/tokenizer_config.json\n",
      "Special tokens file saved in BART-base-syllable-naive/checkpoint-34000/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7202\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to BART-base-syllable-naive/checkpoint-34500\n",
      "Configuration saved in BART-base-syllable-naive/checkpoint-34500/config.json\n",
      "Model weights saved in BART-base-syllable-naive/checkpoint-34500/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base-syllable-naive/checkpoint-34500/tokenizer_config.json\n",
      "Special tokens file saved in BART-base-syllable-naive/checkpoint-34500/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base-syllable-naive/checkpoint-35000\n",
      "Configuration saved in BART-base-syllable-naive/checkpoint-35000/config.json\n",
      "Model weights saved in BART-base-syllable-naive/checkpoint-35000/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base-syllable-naive/checkpoint-35000/tokenizer_config.json\n",
      "Special tokens file saved in BART-base-syllable-naive/checkpoint-35000/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base-syllable-naive/checkpoint-35500\n",
      "Configuration saved in BART-base-syllable-naive/checkpoint-35500/config.json\n",
      "Model weights saved in BART-base-syllable-naive/checkpoint-35500/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base-syllable-naive/checkpoint-35500/tokenizer_config.json\n",
      "Special tokens file saved in BART-base-syllable-naive/checkpoint-35500/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base-syllable-naive/checkpoint-36000\n",
      "Configuration saved in BART-base-syllable-naive/checkpoint-36000/config.json\n",
      "Model weights saved in BART-base-syllable-naive/checkpoint-36000/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base-syllable-naive/checkpoint-36000/tokenizer_config.json\n",
      "Special tokens file saved in BART-base-syllable-naive/checkpoint-36000/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base-syllable-naive/checkpoint-36500\n",
      "Configuration saved in BART-base-syllable-naive/checkpoint-36500/config.json\n",
      "Model weights saved in BART-base-syllable-naive/checkpoint-36500/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base-syllable-naive/checkpoint-36500/tokenizer_config.json\n",
      "Special tokens file saved in BART-base-syllable-naive/checkpoint-36500/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base-syllable-naive/checkpoint-37000\n",
      "Configuration saved in BART-base-syllable-naive/checkpoint-37000/config.json\n",
      "Model weights saved in BART-base-syllable-naive/checkpoint-37000/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base-syllable-naive/checkpoint-37000/tokenizer_config.json\n",
      "Special tokens file saved in BART-base-syllable-naive/checkpoint-37000/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base-syllable-naive/checkpoint-37500\n",
      "Configuration saved in BART-base-syllable-naive/checkpoint-37500/config.json\n",
      "Model weights saved in BART-base-syllable-naive/checkpoint-37500/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base-syllable-naive/checkpoint-37500/tokenizer_config.json\n",
      "Special tokens file saved in BART-base-syllable-naive/checkpoint-37500/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base-syllable-naive/checkpoint-38000\n",
      "Configuration saved in BART-base-syllable-naive/checkpoint-38000/config.json\n",
      "Model weights saved in BART-base-syllable-naive/checkpoint-38000/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base-syllable-naive/checkpoint-38000/tokenizer_config.json\n",
      "Special tokens file saved in BART-base-syllable-naive/checkpoint-38000/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base-syllable-naive/checkpoint-38500\n",
      "Configuration saved in BART-base-syllable-naive/checkpoint-38500/config.json\n",
      "Model weights saved in BART-base-syllable-naive/checkpoint-38500/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base-syllable-naive/checkpoint-38500/tokenizer_config.json\n",
      "Special tokens file saved in BART-base-syllable-naive/checkpoint-38500/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base-syllable-naive/checkpoint-39000\n",
      "Configuration saved in BART-base-syllable-naive/checkpoint-39000/config.json\n",
      "Model weights saved in BART-base-syllable-naive/checkpoint-39000/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base-syllable-naive/checkpoint-39000/tokenizer_config.json\n",
      "Special tokens file saved in BART-base-syllable-naive/checkpoint-39000/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base-syllable-naive/checkpoint-39500\n",
      "Configuration saved in BART-base-syllable-naive/checkpoint-39500/config.json\n",
      "Model weights saved in BART-base-syllable-naive/checkpoint-39500/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base-syllable-naive/checkpoint-39500/tokenizer_config.json\n",
      "Special tokens file saved in BART-base-syllable-naive/checkpoint-39500/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base-syllable-naive/checkpoint-40000\n",
      "Configuration saved in BART-base-syllable-naive/checkpoint-40000/config.json\n",
      "Model weights saved in BART-base-syllable-naive/checkpoint-40000/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer config file saved in BART-base-syllable-naive/checkpoint-40000/tokenizer_config.json\n",
      "Special tokens file saved in BART-base-syllable-naive/checkpoint-40000/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base-syllable-naive/checkpoint-40500\n",
      "Configuration saved in BART-base-syllable-naive/checkpoint-40500/config.json\n",
      "Model weights saved in BART-base-syllable-naive/checkpoint-40500/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base-syllable-naive/checkpoint-40500/tokenizer_config.json\n",
      "Special tokens file saved in BART-base-syllable-naive/checkpoint-40500/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base-syllable-naive/checkpoint-41000\n",
      "Configuration saved in BART-base-syllable-naive/checkpoint-41000/config.json\n",
      "Model weights saved in BART-base-syllable-naive/checkpoint-41000/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base-syllable-naive/checkpoint-41000/tokenizer_config.json\n",
      "Special tokens file saved in BART-base-syllable-naive/checkpoint-41000/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base-syllable-naive/checkpoint-41500\n",
      "Configuration saved in BART-base-syllable-naive/checkpoint-41500/config.json\n",
      "Model weights saved in BART-base-syllable-naive/checkpoint-41500/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base-syllable-naive/checkpoint-41500/tokenizer_config.json\n",
      "Special tokens file saved in BART-base-syllable-naive/checkpoint-41500/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base-syllable-naive/checkpoint-42000\n",
      "Configuration saved in BART-base-syllable-naive/checkpoint-42000/config.json\n",
      "Model weights saved in BART-base-syllable-naive/checkpoint-42000/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base-syllable-naive/checkpoint-42000/tokenizer_config.json\n",
      "Special tokens file saved in BART-base-syllable-naive/checkpoint-42000/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base-syllable-naive/checkpoint-42500\n",
      "Configuration saved in BART-base-syllable-naive/checkpoint-42500/config.json\n",
      "Model weights saved in BART-base-syllable-naive/checkpoint-42500/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base-syllable-naive/checkpoint-42500/tokenizer_config.json\n",
      "Special tokens file saved in BART-base-syllable-naive/checkpoint-42500/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7202\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to BART-base-syllable-naive/checkpoint-43000\n",
      "Configuration saved in BART-base-syllable-naive/checkpoint-43000/config.json\n",
      "Model weights saved in BART-base-syllable-naive/checkpoint-43000/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base-syllable-naive/checkpoint-43000/tokenizer_config.json\n",
      "Special tokens file saved in BART-base-syllable-naive/checkpoint-43000/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base-syllable-naive/checkpoint-43500\n",
      "Configuration saved in BART-base-syllable-naive/checkpoint-43500/config.json\n",
      "Model weights saved in BART-base-syllable-naive/checkpoint-43500/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base-syllable-naive/checkpoint-43500/tokenizer_config.json\n",
      "Special tokens file saved in BART-base-syllable-naive/checkpoint-43500/special_tokens_map.json\n",
      "Saving model checkpoint to BART-base-syllable-naive/checkpoint-44000\n",
      "Configuration saved in BART-base-syllable-naive/checkpoint-44000/config.json\n",
      "Model weights saved in BART-base-syllable-naive/checkpoint-44000/pytorch_model.bin\n",
      "tokenizer config file saved in BART-base-syllable-naive/checkpoint-44000/tokenizer_config.json\n",
      "Special tokens file saved in BART-base-syllable-naive/checkpoint-44000/special_tokens_map.json\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-3435b262f1ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1498\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inner_training_loop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_find_batch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1499\u001b[0m         )\n\u001b[0;32m-> 1500\u001b[0;31m         return inner_training_loop(\n\u001b[0m\u001b[1;32m   1501\u001b[0m             \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1502\u001b[0m             \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1740\u001b[0m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1742\u001b[0;31m                     \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1743\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1744\u001b[0m                 if (\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2502\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepspeed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2503\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2504\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2505\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2506\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \"\"\"\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    126\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 7202\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.40320608019828796}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Seq2SeqTrainer' object has no attribute 'plot'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-7f90a5714312>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'Seq2SeqTrainer' object has no attribute 'plot'"
     ]
    }
   ],
   "source": [
    "trainer.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
