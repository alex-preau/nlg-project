{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Title conditioned GAN training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-7dd2d3f9d1d8>:12: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"rouge\")\n"
     ]
    }
   ],
   "source": [
    "from transformers import BartTokenizer, BartModel,BartForConditionalGeneration\n",
    "from torch.distributions import Categorical\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from undecorated import undecorated\n",
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from transformers import  Seq2SeqTrainingArguments, Seq2SeqTrainer,DataCollatorForSeq2Seq\n",
    "metric = load_metric(\"rouge\")\n",
    "import torch.optim as optim\n",
    "import torchvision.utils as vutils\n",
    "import types\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.distributions import Categorical\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gan_utils import get_end_mask,get_valid_mask,Discriminator_Titles,discriminator_train_title, generate_random_input,reinforce_loss,Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import PoemDataset,encode_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-base')\n",
    "#generator = BartForConditionalGeneration.from_pretrained('facebook/bart-base')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data_dir =  '/home/alexander/nlg-project/data/'\n",
    "test_df = pd.read_csv(processed_data_dir + 'test_data.csv')\n",
    "test_model = encode_sentences(tokenizer,test_df)\n",
    "\n",
    "train_df = pd.read_csv(processed_data_dir + 'train_data.csv')\n",
    "train_model = encode_sentences(tokenizer,train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#logprob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#_,logprob = simple_decoding(generator,noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = PoemDataset(train_model)\n",
    "eval_ds = PoemDataset(test_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#discriminator = discriminator.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fixed_noise = generate_random_input(bsize,'cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = '/home/alexander/nlg-project/GAN_models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bsize = 12\n",
    "generator =  Generator().cuda()#BartForConditionalGeneration.from_pretrained('facebook/bart-base').cuda()\n",
    "#model = TheModelClass(*args, **kwargs)\n",
    "#generator.load_state_dict(torch.load(PATH + 'generator.pt')['model_state_dict'])\n",
    "discriminator = Discriminator_Titles().cuda()\n",
    "#discriminator.load_state_dict(torch.load(PATH + 'discriminator.pt')['model_state_dict'])\n",
    "criterion = nn.BCEWithLogitsLoss(reduction='none')\n",
    "\n",
    "# Create batch of latent vectors that we will use to visualize\n",
    "#  the progression of the generator\n",
    "fixed_noise = generate_random_input(bsize,'cuda')\n",
    "\n",
    "\n",
    "real_label = 1.\n",
    "ewma_reward = 0.0#torch.tensor(0.0)\n",
    "fake_label = 0.\n",
    "lr = .000001\n",
    "beta1 = .9\n",
    "\n",
    "optimizerD = optim.Adam(discriminator.parameters(), lr=lr*.05, betas=(beta1, 0.99)) \n",
    "optimizerG = optim.Adam(generator.parameters(), lr=lr, betas=(beta1, 0.99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"G_losses_2.txt\", \"w\") as f:\n",
    "    for s in G_losses:\n",
    "        f.write(str(s) +\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop (takes a long long time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training Loop...\n",
      "Label Noise: 0.05\n",
      "Disc LogProbs True\n",
      "tensor([[0.0357, 0.0705, 0.0716, 0.0788, 0.0167, 0.0682, 0.0883, 0.0807, 0.0600,\n",
      "         0.0562, 0.0406, 0.0327, 0.0387, 0.0276, 0.0303, 0.0400, 0.0413, 0.0485,\n",
      "         0.0524, 0.0582, 0.0564]], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "Disc LogProbs False\n",
      "tensor([[0.0210, 0.0056, 0.0224, 0.0413, 0.0320, 0.0180, 0.0309, 0.0324, 0.0553,\n",
      "         0.0000, 0.0000]], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "gen loss\n",
      "tensor([[-1.9235e-07, -5.2422e-02,  2.8746e-04,  1.9727e-02,  4.1461e-02,\n",
      "         -6.6696e-03,  2.7344e-02,  3.0963e-03,  1.4474e-01, -0.0000e+00,\n",
      "         -0.0000e+00]], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "cum rewards\n",
      "tensor([[0.0105, 0.0036, 0.0156, 0.0293, 0.0228, 0.0128, 0.0220, 0.0231, 0.2062,\n",
      "         0.0000, 0.0000]], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "gen logprobs\n",
      "tensor([[-5.3406e-05, -5.0101e+00, -1.9445e-01, -1.2996e+00, -4.7418e+00,\n",
      "         -5.2734e+00, -3.4352e+00, -3.4237e-01, -6.6185e-04, -1.8259e+01,\n",
      "         -1.4954e+01]], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "['<s>PHOTO: KT LAKERA</s><pad><pad>']\n",
      "[0/30][0/11403]\tLoss_D: 16.8546\tLoss_G: 0.0344\tD(x): 0.0394\tD(G(z)): 0.0292 / 0.0344\n",
      "['</s><s>Term fue tree</s><pad><pad>', '</s><s>Romaoard imagined</s><pad>']\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-9d6cd4387b25>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0moutput_real\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss_real\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscriminator_train_title\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiscriminator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreal_gpu\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreal_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreal_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mloss_real\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;31m#del label\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \"\"\"\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    126\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training Loop\n",
    "device = 'cuda'\n",
    "\n",
    "\n",
    "G_losses = []\n",
    "D_losses = []\n",
    "iters = 0\n",
    "num_epochs = 30\n",
    "\n",
    "train_dl = DataLoader(train_ds,batch_size=bsize,num_workers=2)\n",
    "print(\"Starting Training Loop...\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(\"Label Noise:\",0.05*.9**epoch)\n",
    "\n",
    "    for i, data in enumerate(train_dl, 0):\n",
    "       # print(i)\n",
    "        \n",
    "        if len(data['input_ids']) < bsize:\n",
    "            continue\n",
    "       # print(data)\n",
    "        ############################\n",
    "        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "        ###########################\n",
    "        ## Train with all-real batch\n",
    "        \n",
    "\n",
    "        real_inputs = data['input_ids'].to(device)\n",
    "        real_input_attention = data['attention_mask'].to(device)\n",
    "        real_input_test = {'input_ids':real_inputs,'attention_mask':real_input_attention}\n",
    "        \n",
    "        \n",
    "        real_gpu = data['label_ids'].to(device)\n",
    "        attention_gpu = data['decoder_attention_mask'].to(device)\n",
    "        b_size = real_gpu.size(0)\n",
    "        #\n",
    "        label = torch.full((b_size,), 1.0 , dtype=torch.float, device=device) #.95 instead of real to inject noise\n",
    "        label = torch.bernoulli(label)\n",
    "       # print(label)\n",
    "        # Forward pass real batch through D\n",
    "        real_mask = get_valid_mask(real_gpu)\n",
    "\n",
    "        output_real,loss_real = discriminator_train_title(discriminator,real_gpu[:,1:],label,real_inputs,criterion,mask=real_mask)\n",
    "\n",
    "        loss_real.backward()\n",
    "\n",
    "        # Calculate loss on all-real batch\n",
    "\n",
    "        D_x = output_real.detach().mean().item()\n",
    "\n",
    "        # Generate batch of latent vectors\n",
    "        noise = generate_random_input(bsize,device)\n",
    "\n",
    "        if random.random() > .5:\n",
    "            titles = noise['input_ids']\n",
    "            fake,fake_logprobs = generator(noise)\n",
    "        else:\n",
    "            titles = real_input_test['input_ids']\n",
    "            #sometimes the discirminator learns words in real input, this way some will be based on these same words\n",
    "            fake,fake_logprobs = generator(real_input_test)\n",
    "            #real_input_test\n",
    "  \n",
    "        \n",
    "        fake_mask = get_valid_mask(fake)\n",
    "        end_mask = get_end_mask(fake)\n",
    "  \n",
    "\n",
    "        label.fill_(0.0) \n",
    "        label = torch.bernoulli(label)\n",
    "       # print(label)\n",
    "        # Classify all fake batch with D\n",
    "        output_fake,loss_fake = discriminator_train_title(discriminator,fake,label,titles,criterion,mask=fake_mask)\n",
    "\n",
    "        loss_fake.backward()\n",
    "\n",
    "        D_G_z1 = output_fake.detach().cpu().mean().item()\n",
    "        # Compute error of D as sum over the fake and the real batches\n",
    "        errD = (torch.mean(loss_real.detach()) + torch.mean(loss_fake.detach())).cpu()\n",
    "\n",
    "        optimizerD.step()\n",
    "        \n",
    "\n",
    "        ############################\n",
    "        # (2) Update G network: maximize log(D(G(z)))\n",
    "        ###########################\n",
    "        \n",
    "        #generator.zero_grad()\n",
    "\n",
    "        lossG, cumulative_rewards, ewma_reward = reinforce_loss(output_fake,fake_logprobs,.3,.08,epoch,fake_mask,end_mask,ewma_reward)\n",
    "        if i % 256 == 0:\n",
    "            print('Disc LogProbs True')\n",
    "            print(output_real[0:1])\n",
    "            print('Disc LogProbs False')\n",
    "            print(output_fake[0:1])\n",
    "            print('gen loss')\n",
    "            print(lossG[0:1])\n",
    "            print('cum rewards')\n",
    "            print(cumulative_rewards[0:1])\n",
    "            print('gen logprobs')\n",
    "            print(fake_logprobs[0:1])\n",
    "            print(tokenizer.batch_decode(fake[0:1].cpu()))\n",
    "\n",
    "        #print(lossG)\n",
    "        lossG = torch.mean(lossG)\n",
    "\n",
    "        # Calculate gradients for G\n",
    "        lossG.backward()\n",
    "        D_G_z2 = lossG.detach().cpu().mean().item()\n",
    "        # Update G\n",
    "        \n",
    "        optimizerG.step()\n",
    "        generator.zero_grad()\n",
    "        discriminator.zero_grad()\n",
    "        #a = 1/0\n",
    "        # Output training stats\n",
    "        if i % 128 == 0:\n",
    "            print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n",
    "                  % (epoch, num_epochs, i, len(train_dl),\n",
    "                     errD.item(), lossG.item(), D_x, D_G_z1, D_G_z2))\n",
    "          #  print('ewma_reward:',ewma_reward)\n",
    "\n",
    "        # Save Losses for plotting later\n",
    "        G_losses.append(lossG.detach().item())\n",
    "        D_losses.append(errD.detach().item())\n",
    "        \n",
    "        del lossG\n",
    "\n",
    "        # Check how the generator is doing by saving G's output on fixed_noise\n",
    "        if (iters % 500 == 0) or ((epoch == num_epochs-1) and (i == len(dataloader)-1)):\n",
    "            with torch.no_grad():\n",
    "                fake = tokenizer.batch_decode(generator.model.generate(fixed_noise['input_ids'],attention_mask=fixed_noise[\"attention_mask\"],max_length=32,num_beams= 50,early_stopping = True))\n",
    "                print(fake[0:2])\n",
    "        if (iters % 1000 == 0 and iters != 0):\n",
    "            print('saving geneartor')\n",
    "            torch.save({\n",
    "            'epoch': 15,\n",
    "            'model_state_dict': generator.state_dict(),\n",
    "            'optimizer_state_dict': optimizerG.state_dict(),\n",
    "            'loss': 0.3355,\n",
    "            }, PATH + 'generator.pt')\n",
    "            torch.save({\n",
    "            'epoch': 15,\n",
    "            'model_state_dict': discriminator.state_dict(),\n",
    "            'optimizer_state_dict': optimizerD.state_dict(),\n",
    "            'loss': 0.3355,\n",
    "            }, PATH + 'discriminator.pt')\n",
    "\n",
    "\n",
    "        iters += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# del optimizerG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "            'epoch': 15,\n",
    "            'model_state_dict': generator.state_dict(),\n",
    "            'optimizer_state_dict': optimizerG.state_dict(),\n",
    "            'loss': 0.3355,\n",
    "            }, PATH + '4_it_nft_generator.pt')\n",
    "torch.save({\n",
    "            'epoch': 15,\n",
    "            'model_state_dict': discriminator.state_dict(),\n",
    "            'optimizer_state_dict': optimizerD.state_dict(),\n",
    "            'loss': 0.3355,\n",
    "            }, PATH + '4_it_nft_discriminator.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(\"cats\", return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.batch_decode(generator.eval().cpu().generate(inputs['input_ids'],attention_mask=inputs[\"attention_mask\"],max_length=128, use_cache=True,\n",
    "        decoder_start_token_id = tokenizer.pad_token_id,\n",
    "        num_beams= 50,\n",
    "        early_stopping = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16 \n",
    "\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    \"BART-base\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    #learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "   # weight_decay=0.01,\n",
    "   # save_total_limit=3,\n",
    "   # num_train_epochs=1,\n",
    "   # predict_with_generate=True,\n",
    "    \n",
    "    learning_rate=1e-4,\n",
    "    weight_decay=0.01,\n",
    "    adam_beta1=0.9,\n",
    "    adam_beta2=0.999,\n",
    "    adam_epsilon=1e-6,\n",
    "    max_grad_norm=1.0,\n",
    "    num_train_epochs=10,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    warmup_ratio=0.1,\n",
    "   # label_names=\"labels\",\n",
    "    #fp16=True,\n",
    "  #  use_auth_token=False\n",
    "    #push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_ds = PoemDataset(model_input)\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=BartForConditionalGeneration.from_pretrained('BART-base/checkpoint-25000'),\n",
    "    args=args,\n",
    "   # data_collator=data_collator,\n",
    "    train_dataset=train_ds,\n",
    "    \n",
    "    eval_dataset=eval_ds,\n",
    "    #use_auth_token=False,\n",
    "    \n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir GAN_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "            'epoch': 15,\n",
    "            'model_state_dict': generator.state_dict(),\n",
    "            'optimizer_state_dict': optimizerG.state_dict(),\n",
    "            'loss': 0.3355,\n",
    "            }, PATH + 'generator.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "            'epoch': 15,\n",
    "            'model_state_dict': discriminator.state_dict(),\n",
    "            'optimizer_state_dict': optimizerD.state_dict(),\n",
    "            'loss': 0.3355,\n",
    "            }, PATH + 'discriminator.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd GAN_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
